{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Working torch version. By pressing enter you start to record a gesture of arbitrary length, stop recording by pressing enter again, \n",
    "# repeat for desired number of gestures. The recorded gestures are then compiled to a pytorch dataset used to train the model bellow\n",
    "# to classify the gestures.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import cv2\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import signal\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Initialize MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize variables\n",
    "capturing = False\n",
    "gestures = {}\n",
    "pose_check = {}\n",
    "poserow_check = {}\n",
    "\n",
    "# Function to handle interrupt signal\n",
    "def signal_handler_csv(sig, frame):\n",
    "    global gestures\n",
    "    print('You pressed Ctrl+C or stopped the script!')\n",
    "    export_gestures_to_csv(gestures)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    sys.exit(0)\n",
    "\n",
    "def signal_handler_torch(sig, frame):\n",
    "    global gestures\n",
    "    print('You pressed Ctrl+C or stopped the script!')\n",
    "    store_as_torch_tensors(gestures)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    sys.exit(0)\n",
    "\n",
    "def store_as_torch_tensors(gestures):\n",
    "    for name, gesture in gestures.items():\n",
    "        numpy_array = np.array(gesture, dtype=np.float32)  # Ensure the NumPy array is of type float32\n",
    "        torch_tensor = torch.from_numpy(numpy_array)\n",
    "        gestures[name] = torch_tensor\n",
    "\n",
    "# Set the signal handler\n",
    "signal.signal(signal.SIGINT, signal_handler_torch)\n",
    "\n",
    "# Function to export gestures to CSV\n",
    "def export_gestures_to_csv(gestures):\n",
    "    with open('gestures_data.csv', mode='w', newline='') as f:\n",
    "        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for gesture_name, gesture_data in gestures.items():\n",
    "            for pose in gesture_data:\n",
    "                row = [gesture_name] + pose\n",
    "                csv_writer.writerow(row)\n",
    "    print(\"Gesture data exported to 'gestures_data.csv'.\")\n",
    "\n",
    "\n",
    "    #return torch_tensor\n",
    "\n",
    "# Function to capture poses\n",
    "def capture_poses():\n",
    "    global capturing, gestures, cap\n",
    "    # Initialize Video Capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_width = int(cap.get(3))\n",
    "    current_pose = []\n",
    "    current_gesture = []\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Failed to grab frame\")\n",
    "                break\n",
    "\n",
    "            # Recolor Feed\n",
    "            image = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make Detections\n",
    "            results = holistic.process(image)\n",
    "\n",
    "            # Recolor image back to BGR for rendering\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "            # Capture pose and save data\n",
    "            if capturing:\n",
    "                try:\n",
    "                    pose = results.pose_landmarks.landmark\n",
    "                    #pose_check[0] = pose\n",
    "                    pose_row = np.array([[landmark.x, landmark.y, landmark.z] for landmark in pose])#.flatten())\n",
    "                    poserow_check[0] = pose_row\n",
    "                    current_gesture.append(pose_row)\n",
    "                except AttributeError:\n",
    "                    pass  # Handle the case where no landmarks are detected\n",
    "\n",
    "                cv2.putText(image, \"Capturing... \", (frame_width // 2 - 20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            else:\n",
    "                cv2.putText(image, \"Press 'Enter' to start/stop capturing\", (frame_width // 2 - 20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('Movement, Music & Machines', image)\n",
    "\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('\\r'):  # Enter key is pressed\n",
    "                if capturing:\n",
    "                    capturing = False\n",
    "                    #ts = time.time() - t\n",
    "                    gesture_name = f\"gesture_{len(gestures) + 1}\"\n",
    "                    gestures[gesture_name] = current_gesture #, [ts]]\n",
    "                    current_gesture = []\n",
    "                    print(f\"Capture stopped. Gesture {gesture_name} saved.\")\n",
    "                else:\n",
    "                    capturing = True\n",
    "                    #t = time.time()\n",
    "                    print(\"Capture started.\")\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return gestures\n",
    "\n",
    "# Start capturing poses\n",
    "gestures = capture_poses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from spiralnet import instantiate_model as instantiate_spiralnet \n",
    "from torch.nn import init\n",
    "\n",
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, gestures):\n",
    "        self.gestures = gestures\n",
    "        self.labels = [i for i in range(len(gestures))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gestures)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gesture_name = f\"gesture_{idx + 1}\"\n",
    "        gesture = self.gestures[gesture_name]\n",
    "        label = self.labels[idx]\n",
    "        return gesture, label\n",
    "    \n",
    "\n",
    "class SpiralnetClassifierGRU(nn.Module):\n",
    "    def __init__(self, nr_of_classes, embedding_dim=32, nr_spiralnet_layers=4, nr_rnn_layers=2):\n",
    "        super(SpiralnetClassifierGRU, self).__init__()\n",
    "        self.nr_of_gesture_classes = nr_of_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.spiralnet = instantiate_spiralnet(nr_layers=nr_spiralnet_layers, output_dim=self.embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.embedding_dim, nr_rnn_layers, bidirectional=False, batch_first=False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(self.embedding_dim, self.nr_of_gesture_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        for param in self.gru.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spiralnet(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.gelu(x[-1])\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "nr_of_classes = len(list(gestures.keys()))\n",
    "model = SpiralnetClassifierGRU(nr_of_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataset, criterion, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(len(dataset)):\n",
    "            gesture, label = dataset[i]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(gesture)  # Assuming gesture is of correct shape\n",
    "            loss = criterion(outputs.unsqueeze(0), torch.tensor([label], dtype=torch.long))  # Corrected target dtype\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #if i % 10 == 9:  # Print every 10 gestures\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "# Dataset\n",
    "gesture_dataset = GestureDataset(gestures)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, gesture_dataset, criterion, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
