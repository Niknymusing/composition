{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import datetime\n",
    "import os\n",
    "from multiscale_mine import Multiscale_MINE\n",
    "from multiscale_mine import Multiscale_MINE_test\n",
    "from multiscale_mine import GRUCell\n",
    "from multiscale_mine import RNNCell\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn \n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Francois Delalande, Glenn Gould study, read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating mutual information between audio and movement across musically relevant features, \n",
    "we get an unsupervised learning criterion to train mappings between movement and sound. \n",
    "Below is my (very non-standard) implementation of the MINE method (Bengio et al. https://arxiv.org/pdf/1801.04062.pdf) for estimating mutual information between\n",
    "random variables using neural networks across multiple time- and frequency- resolutions. Also a few recording and data processing utilities to apply it directly on audio and mocap data recorded from the computers input devices. This is intended as a POC to explore in our coming workshopswith music and dance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current version instantiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: patch re-linking failed\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "PolyMeshT::add_face: complex edge\n",
      "/Users/nikny/poseapps/poseapps/spiralnet.py:1084: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1699601932289/work/torch/csrc/utils/tensor_new.cpp:276.)\n",
      "  torch.LongTensor([spmat.tocoo().row,\n",
      "/Users/nikny/poseapps/poseapps/spiralnet.py:1083: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1699601932289/work/torch/csrc/utils/tensor_new.cpp:618.)\n",
      "  return torch.sparse.FloatTensor(\n"
     ]
    }
   ],
   "source": [
    "# instantiate the neural network (my implementation code in multiscale_mine.py file, which wraps the spiralnet\n",
    "# mocap encoder and various signal processing on the audio signal) used to estimate mutual information between music audio \n",
    "# and movements, creating a joint embedding of movement and audio to compute mutual information scores across a binning of \n",
    "# the time-frequency plane given by selected (nr_time_scales, nr_frequency_bands).\n",
    "\n",
    "mine_network = Multiscale_MINE(GRUCell,nr_time_scales = 9, \n",
    "                               nr_frequency_bands = 13, \n",
    "                               embedding_dim = 50, #gives dim 100 for the concattenated encoding of audio and mocap input\n",
    "                               input_dim = 1470, # selected input size for audio sample rate 44100 since blazenet runs at aproximately 30fps\n",
    "                               nr_layers_per_timescale = 4, # nr of layers for the RNNs encoding the respective timescales\n",
    "                               nr_spiralnet_layers = 16, \n",
    "                               delay_size = 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32285677"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in mine_network.parameters() if p.requires_grad]) # count number of model parameters with current model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing utilities to obtain joint and marginal samples and construct the training data set for the MINE estimation\n",
    "\n",
    "def get_marginal_samples(current_session_dir, audio_data_tensors, audioposes_data_dir='audioposes_data'):\n",
    "    all_sessions = [d for d in os.listdir(audioposes_data_dir) if os.path.isdir(os.path.join(audioposes_data_dir, d))]\n",
    "    other_sessions = [s for s in all_sessions if s != current_session_dir]\n",
    "\n",
    "    marginal_poses = None\n",
    "    while marginal_poses is None or len(marginal_poses) < len(audio_data_tensors):\n",
    "        selected_session = random.choice(other_sessions)\n",
    "        poses_path = os.path.join(audioposes_data_dir, selected_session, 'poses_tensors.pt')\n",
    "\n",
    "        if os.path.exists(poses_path):\n",
    "            new_poses = torch.load(poses_path)\n",
    "            if marginal_poses is None:\n",
    "                marginal_poses = new_poses\n",
    "            else:\n",
    "                marginal_poses = torch.cat((marginal_poses, new_poses), dim=0)\n",
    "\n",
    "    return marginal_poses\n",
    "\n",
    "def generate_joint_and_marginal_samples(audio_data_tensors, poses_tensors, pose_selected_values, poses_marginal_samples, batch_size=32):\n",
    "    joint_sample_pairs, marginal_sample_pairs = {}, {}\n",
    "    \n",
    "    for i in range(len(pose_selected_values)):\n",
    "        if 0 < pose_selected_values[i] <= len(poses_tensors):\n",
    "            pose_ind = pose_selected_values[i] - 1    # converting to 0-indexed\n",
    "            joint_sample_pairs[i] = [audio_data_tensors[i], poses_tensors[pose_ind]]\n",
    "    \n",
    "    joint_keys = list(joint_sample_pairs.keys())\n",
    "    for i in range(len(joint_keys)):\n",
    "        idx = joint_keys[i] \n",
    "        marginal_sample_pairs[idx] = [joint_sample_pairs[idx][0], poses_marginal_samples[i]]\n",
    "\n",
    "    joint_batches, marginal_batches = [], []\n",
    "    joint_batch, marginal_batch = [], []\n",
    "\n",
    "    for idx in joint_keys:\n",
    "        joint_batch.append(joint_sample_pairs[idx])\n",
    "        marginal_batch.append(marginal_sample_pairs[idx])\n",
    "\n",
    "        if len(joint_batch) == batch_size:\n",
    "            joint_tensor = [torch.stack(samples) for samples in zip(*joint_batch)]\n",
    "            marginal_tensor = [torch.stack(samples) for samples in zip(*marginal_batch)]\n",
    "            joint_batches.append(joint_tensor)\n",
    "            marginal_batches.append(marginal_tensor)\n",
    "            joint_batch, marginal_batch = [], []\n",
    "\n",
    "    if joint_batch:\n",
    "        joint_tensor = [torch.stack(samples) for samples in zip(*joint_batch)]\n",
    "        marginal_tensor = [torch.stack(samples) for samples in zip(*marginal_batch)]\n",
    "        joint_batches.append(joint_tensor)\n",
    "        marginal_batches.append(marginal_tensor)\n",
    "\n",
    "    return joint_batches[:-1], marginal_batches[:-1] # remove the last batch since it has an irregular number of data messing with model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1700158517.617406       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 83), renderer: Apple M2 Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording stopped.\n",
      "Generated a dataset with 12 nr of batches, with batch size 32\n",
      "Audio data tensors saved to audioposes_data/session_20231116_191533_nr_poses=345/audio_data_tensors.pt\n",
      "Dictionary values tensors saved to audioposes_data/session_20231116_191533_nr_poses=345/dict_values_tensors.pt\n",
      "Poses tensors saved to audioposes_data/session_20231116_191533_nr_poses=345/poses_tensors.pt\n"
     ]
    }
   ],
   "source": [
    "# This cell records realtime audio and mocap from the computer mic and camera and upon stopping the cell compiles the training dataset.\n",
    " \n",
    "#Audio Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 512\n",
    "DEVICE_INDEX = 1   # adjust this integer according to your computers available audio input devices\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Define the dictionary\n",
    "pose_counter = {0: 0}\n",
    "\n",
    "# List to save the audio buffers and dictionary values\n",
    "saved_audio_data = []\n",
    "\n",
    "# List to save the poses\n",
    "saved_poses = []\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Define the callback function for audio\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16)#float32)\n",
    "    saved_audio_data.append([audio_data.tolist(), pose_counter[0]])\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#print(\"Recording... Press 'q' in the webcam window to stop.\") # if uncommenting cv2.imshow('Pose Tracking', frame)\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose results\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Save the pose landmarks\n",
    "            pose_landmarks = results.pose_landmarks.landmark\n",
    "            pose_data = [[landmark.x, landmark.y, landmark.z] for landmark in pose_landmarks]\n",
    "            saved_poses.append(pose_data)\n",
    "            \n",
    "            # Increment the dictionary value\n",
    "            pose_counter[0] += 1\n",
    "        \n",
    "        # Display the frame\n",
    "        #cv2.imshow('Pose Tracking', frame)\n",
    "        \n",
    "        cv2.waitKey(1) \n",
    "       \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Recording stopped.\")\n",
    "\n",
    "finally:\n",
    "    # Release the webcam\n",
    "    cap.release()\n",
    "    \n",
    "    # Close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    # Close the OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    audio_data = [audio_data for audio_data, _ in saved_audio_data]\n",
    "    audio_data_flat = np.concatenate(audio_data)\n",
    "    \n",
    "    # Reshape count values and select the most frequent (larger in case of tie) for each buffer\n",
    "    count_values = [np.full_like(audio_data, count_value) for audio_data, count_value in saved_audio_data]\n",
    "    count_values_flat = np.concatenate(count_values)\n",
    "    \n",
    "    # since we record audio at sample rate 44100 and poses at 30fps and want to try time-sync audio and movements, \n",
    "    # reshape the data to audio buffers of size 1470 = 44100/30\n",
    "\n",
    "    leftover_samples = len(audio_data_flat) % 1470\n",
    "    if leftover_samples != 0:\n",
    "        audio_data_flat = audio_data_flat[:-leftover_samples]\n",
    "        count_values_flat = count_values_flat[:-leftover_samples]\n",
    "\n",
    "    audio_data_reshaped = audio_data_flat.reshape(-1, 1470)\n",
    "    count_values_reshaped = count_values_flat.reshape(-1, 1470)\n",
    "    \n",
    "    selected_count_values = []\n",
    "    for buffer in count_values_reshaped:\n",
    "        # Select the count value with the most occurrences (larger in case of tie)\n",
    "        unique_values, counts = np.unique(buffer, return_counts=True)\n",
    "        selected_count = unique_values[np.argmax(counts)]\n",
    "        selected_count_values.append(selected_count)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    audio_data_tensors = torch.tensor(audio_data_reshaped, dtype=torch.float32) #dtype=torch.int16)\n",
    "    selected_count_values_tensor = torch.tensor(selected_count_values, dtype=torch.int32)\n",
    "    poses_tensors = torch.tensor(saved_poses, dtype=torch.float32)\n",
    "\n",
    "        # Save the tensors to disk with the date and time in the filename\n",
    "\n",
    "    parent_dir = \"audioposes_data\"\n",
    "\n",
    "    # Create a session directory inside the parent directory\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_dir = f'session_{formatted_time}_nr_poses='+str(len(poses_tensors))\n",
    "    session_dir_path = os.path.join(parent_dir, session_dir)\n",
    "    os.makedirs(session_dir_path, exist_ok=True)\n",
    "\n",
    "    # Save the tensors to disk in the session directory\n",
    "    audio_data_file_path = os.path.join(session_dir_path, 'audio_data_tensors.pt')\n",
    "    dict_values_file_path = os.path.join(session_dir_path, 'dict_values_tensors.pt')\n",
    "    poses_file_path = os.path.join(session_dir_path, 'poses_tensors.pt')\n",
    "    \n",
    "    torch.save(audio_data_tensors, audio_data_file_path)\n",
    "    torch.save(selected_count_values_tensor, dict_values_file_path)\n",
    "    torch.save(poses_tensors, poses_file_path)\n",
    "\n",
    "    audioposes_data_dir='audioposes_data'\n",
    "    all_sessions = [d for d in os.listdir(audioposes_data_dir) if os.path.isdir(os.path.join(audioposes_data_dir, d))]\n",
    "    other_sessions = [s for s in all_sessions if s != session_dir]\n",
    "    \n",
    "    if not other_sessions:\n",
    "        print(\"No previous session recorded to use for marginal samples. Record another session to obtain marginal samples for the mine computation.\")\n",
    "    \n",
    "    else:\n",
    "        marginal_poses = get_marginal_samples(session_dir, audio_data_tensors)\n",
    "        joint_batches, marginal_batches = generate_joint_and_marginal_samples(audio_data_tensors, poses_tensors, selected_count_values_tensor.numpy(), marginal_poses, batch_size=32)\n",
    "        \n",
    "        print('Generated a dataset with '+str(len(joint_batches))+' nr of batches, with batch size 32')\n",
    "        print(f\"Audio data tensors saved to {audio_data_file_path}\")\n",
    "        print(f\"Dictionary values tensors saved to {dict_values_file_path}\")\n",
    "        print(f\"Poses tensors saved to {poses_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to save the current data as backup\n",
    "#tensor_list = [joint_batches, marginal_batches]\n",
    "#torch.save(tensor_list, 'tensor_list'+current_time+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing training data example\n",
    "#loaded_tensor_list = torch.load('tensor_list.pth')\n",
    "#joint_batches, marginal_batches = loaded_tensor_list[0], loaded_tensor_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_dict = torch.load('data/trained_models/epoch_1.pt')\n",
    "#mine_network.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new training session training_session_20231116_212123, starting training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joint_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# starting the training session:\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m train()\n",
      "\u001b[1;32m/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m running_neg_mine_estimate \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m mine_matrices \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (joint_batch, marginal_batch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(joint_batches, marginal_batches)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# Extract audio, joint poses, and marginal poses from batches\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x_joint, y_joint \u001b[39m=\u001b[39m joint_batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     _, y_marginal \u001b[39m=\u001b[39m marginal_batch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joint_batches' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the mine_network on the data recorded in the previous cell to optimize the mutual information estimate across multiple time and frequency resolutions.\n",
    "# Since RNNs are prone to exploding gradients, we need to chech for nan values often and if found revert to previous model states and varying learning rates.\n",
    "current_mines = {}\n",
    "params_without_gradients = {}\n",
    "def train(new_session=True, session_path=None):\n",
    "    if new_session:\n",
    "        parent_dir = \"training_sessions\"\n",
    "        current_time = datetime.datetime.now()\n",
    "        formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        session_dir = f'training_session_{formatted_time}'\n",
    "        session_path = os.path.join(parent_dir, session_dir)\n",
    "        os.makedirs(session_path, exist_ok=True)\n",
    "        \n",
    "        models_path = os.path.join(session_path, 'trained_models')\n",
    "        os.makedirs(models_path, exist_ok=True)\n",
    "        output_path = os.path.join(session_path, 'mine_matrices')\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        print(f'Created new training session {session_dir}, starting training')\n",
    "    elif session_path:\n",
    "        print(f'Continuing training session {session_path}, starting training')\n",
    "    else:\n",
    "        print('Set new_session = True or enter a path to existing training session')\n",
    "        return\n",
    "    \n",
    "    writer = SummaryWriter(log_dir=session_path)\n",
    "    \n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(mine_network.parameters(), lr=0.0001) \n",
    "    num_epochs = 10000\n",
    "\n",
    "    # To keep track of the best model\n",
    "    best_model_state = None\n",
    "    best_loss = float('inf')\n",
    "    training_scores = []\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_neg_mine_estimate = 0.0\n",
    "        mine_matrices = []\n",
    "\n",
    "        for i, (joint_batch, marginal_batch) in enumerate(zip(joint_batches, marginal_batches)):\n",
    "            # Extract audio, joint poses, and marginal poses from batches\n",
    "            x_joint, y_joint = joint_batch\n",
    "            _, y_marginal = marginal_batch\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass, criterion computation\n",
    "            mine_score, mine_matrix = mine_network(x_joint, y_joint, y_marginal)\n",
    "            if torch.isnan(mine_score).any():\n",
    "                print('Found NaN in mine_score, reverting to state_dict from previous epoch and lowering learning rate by *0.75')\n",
    "                if epoch > 0:\n",
    "                    model_path = os.path.join(models_path, f'epoch_{epoch-1}.pt')\n",
    "                    if os.path.exists(model_path):\n",
    "                        mine_network.load_state_dict(torch.load(model_path))\n",
    "                        optimizer = optim.AdamW(mine_network.parameters(), lr=optimizer.param_groups[0]['lr'] * 0.75)\n",
    "                    else:\n",
    "                        print(f'Model file not found: {model_path}')\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                mine_matrices.append(mine_matrix.detach().cpu().numpy())\n",
    "                current_mines[i]=mine_matrix.detach().cpu().numpy()\n",
    "                training_scores.append(mine_score.item())\n",
    "                criterion = mine_score\n",
    "                print('MI lower bound = ', -criterion.item())\n",
    "                criterion.backward()\n",
    "                optimizer.step()\n",
    "                running_neg_mine_estimate += criterion.item()             \n",
    "                \n",
    "                if i % 100 == 99:\n",
    "                    print(f'[{epoch + 1}, {i + 1}] MI lower bound running mean: {-running_neg_mine_estimate / 100:.3f}')\n",
    "                    running_neg_mine_estimate = 0.0\n",
    "\n",
    "                # Log gradients of all model parameters\n",
    "                params_no_grad = []\n",
    "                for name, param in mine_network.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        writer.add_histogram(f'Gradients/{name}', param.grad, global_step)\n",
    "                    else:\n",
    "                        params_no_grad.append(name) #print(f'Gradient not found for parameter: {name}')\n",
    "                params_without_gradients[global_step] = params_no_grad\n",
    "                # Log optimizer state\n",
    "                for j, param_group in enumerate(optimizer.param_groups):\n",
    "                    writer.add_scalar(f'Learning Rate/param_group_{j}', param_group['lr'], global_step)\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "        torch.save(mine_network.state_dict(), os.path.join(models_path, f'epoch_{epoch}.pt'))\n",
    "        np.save(os.path.join(output_path, f'epoch_{epoch}.npy'), mine_matrices)\n",
    "\n",
    "        if mine_matrices and np.isnan(mine_matrices[-1]).any():\n",
    "            print('Found NaN in mine_matrix, reverting to state_dict from previous epoch and lowering learning rate by *0.75')\n",
    "            if epoch > 0:\n",
    "                model_path = os.path.join(models_path, f'epoch_{epoch-1}.pt')\n",
    "                if os.path.exists(model_path):\n",
    "                    mine_network.load_state_dict(torch.load(model_path))\n",
    "                    optimizer = optim.AdamW(mine_network.parameters(), lr=optimizer.param_groups[0]['lr'] * 0.75)\n",
    "                else:\n",
    "                    print(f'Model file not found: {model_path}')\n",
    "        elif optimizer.param_groups[0]['lr'] < 0.0001:\n",
    "            optimizer.param_groups[0]['lr'] = 0.0001\n",
    "\n",
    "        mine_matrices = []\n",
    "    return mine_matrices, training_scores\n",
    "    print('Finished Training')\n",
    "\n",
    "# starting the training session:\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'JSAnimation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39manimation\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39manimation\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mJSAnimation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mIPython_display\u001b[39;00m \u001b[39mimport\u001b[39;00m display_animation\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/poseapps/poseapps/mine_audioposes_io.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39manimation\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39manimation\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'JSAnimation'"
     ]
    }
   ],
   "source": [
    "# Visualise the mine matrix across a training epoch as a heat map animation. Each matrix element is the mutual information lower bound estimate between \n",
    "# the recorded mocap and audio input signals. \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "num_tensors = len(list(current_mines.keys()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.imshow(current_mines[0], cmap='viridis', aspect='auto', animated=True)\n",
    "\n",
    "def animate(i):\n",
    "    cax.set_array(current_mines[i])\n",
    "    return [cax]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=num_tensors, interval=5, blit=True)\n",
    "\n",
    "# To display the animation in Jupyter\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_without_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/musicmove/mine_audioposes_io.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/musicmove/mine_audioposes_io.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m params_without_gradients\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_without_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "params_without_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant experimentation cells bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(new_session = True, session_path = None):\n",
    "\n",
    "    if new_session == True:\n",
    "        parent_dir = \"training_sessions\"\n",
    "        # Create a session directory inside the parent directory\n",
    "        current_time = datetime.datetime.now()\n",
    "        formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        session_dir = f'training_session_{formatted_time}'\n",
    "        session_path = os.path.join(parent_dir, session_dir)\n",
    "        os.makedirs(session_path, exist_ok=True)\n",
    "        \n",
    "        models_path = os.path.join(session_dir, 'trained_models')\n",
    "        os.makedirs(models_path, exist_ok=True)\n",
    "        output_path = os.path.join(session_dir, 'mine_matrices')\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        print('created new training session '+session_dir_path+', starting training')\n",
    "    elif session_path != None:\n",
    "        session_dir = session_path\n",
    "        print('continueing training session '+session_path+', starting training')\n",
    "    else:\n",
    "        print('set new_session = True or enter a path to existing training session')\n",
    "\n",
    "    #Initialize optimizer\n",
    "    optimizer = optim.AdamW(mine_network.parameters(), lr=0.0001) \n",
    "    #joint_batches, marginal_batches = joint_batches_, marginal_batches_\n",
    "    # Number of training epochs\n",
    "    num_epochs = 10000\n",
    "\n",
    "    # To keep track of the best model\n",
    "    best_model_state = None\n",
    "    best_loss = float('inf')\n",
    "    training_scores = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_neg_mine_estimate = 0.0\n",
    "        mine_matrices = []\n",
    "\n",
    "        for i, (joint_batch, marginal_batch) in enumerate(zip(joint_batches, marginal_batches)):\n",
    "            # Extract audio, joint poses, and marginal poses from batches\n",
    "            x_joint, y_joint = joint_batch\n",
    "            _, y_marginal = marginal_batch\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass, criterion computation\n",
    "            mine_score, mine_matrix = mine_network(x_joint, y_joint, y_marginal)\n",
    "            if np.isnan(mine_score.detach().numpy()).any():\n",
    "                print('found nan in mine_score, reverting to state_dict from previous epoch and lowering learning rate by *0.75')\n",
    "                #if epoch > 0:\n",
    "                mine_network.load_state_dict(torch.load(session_path+f'/trained_models/epoch_{epoch-1}.pt'))\n",
    "                optimizer = optim.AdamW(mine_network.parameters(), lr=0.0001) \n",
    "                optimizer.param_groups[0]['lr'] *= 0.75\n",
    "                #else:\n",
    "                        #print('instantiate a new model and try again')\n",
    "                break\n",
    "            \n",
    "            else:    \n",
    "                \n",
    "                mine_matrices.append(mine_matrix.detach().cpu().numpy())\n",
    "                training_scores.append(mine_score)\n",
    "                criterion = mine_score\n",
    "                #training_scores.append(-criterion.item())\n",
    "                print('MI lower bound =  ', -criterion.item()) #-neg_mine_estimate\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                criterion.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "                running_neg_mine_estimate += criterion.item()             \n",
    "                \n",
    "                if i % 100 == 99:    # Print every 100 mini-batches\n",
    "                    print('[%d, %5d] MI lower bound running mean: %.3f' %\n",
    "                        (epoch + 1, i + 1, -running_neg_mine_estimate / 100))\n",
    "                    running_neg_mine_estimate = 0.0\n",
    "\n",
    "        # Save model and mine_matrices after each epoch\n",
    "        torch.save(mine_network.state_dict(), session_path+f'/trained_models/epoch_{epoch}.pt')\n",
    "        np.save(session_path+f'/mine_matrices/epoch_{epoch}.npy', mine_matrices)\n",
    "\n",
    "        # Check for NaN values in the latest mine_matrix\n",
    "        if np.isnan(mine_matrices[-1]).any():\n",
    "            # If learning rate is higher than 0.0001, reduce it and load previous state_dict\n",
    "            print('found nan in mine_matrix, reverting to state_dict from previous epoch and lowering learning rate by *0.75')\n",
    "            \n",
    "            if epoch > 0:\n",
    "                mine_network.load_state_dict(torch.load(session_path+f'/trained_models/epoch_{epoch-1}.pt'))\n",
    "                optimizer = optim.AdamW(mine_network.parameters(), lr=0.0001) \n",
    "                optimizer.param_groups[0]['lr'] *= 0.75\n",
    "        else:\n",
    "            # If no NaN values and learning rate is less than 0.0001, reset it back to 0.0001\n",
    "            if optimizer.param_groups[0]['lr'] < 0.0001:\n",
    "                optimizer.param_groups[0]['lr'] = 0.0001\n",
    "\n",
    "        # Clear mine_matrices from running memory\n",
    "        mine_matrices = []\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# starting the training session:\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the mine_network instantiated above on the audio and mocap data recorded in the previous cell\n",
    "\n",
    "# need to experiment with the learning rate and scheduling a lot more, but initial experiments show we need to keep relatively small\n",
    "# learning rate to avoid exploding gradients in the RNN layers.\n",
    "\n",
    "optimizer = optim.AdamW(mine_network.parameters(), lr=0.0001) \n",
    "\n",
    "mine_matrices = []\n",
    "# Number of training epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_neg_mine_estimate  = 0.0\n",
    "    for i, (joint_batch, marginal_batch) in enumerate(zip(joint_batches, marginal_batches)):\n",
    "        # Extract audio, joint poses, and marginal poses from batches\n",
    "        x_joint, y_joint = joint_batch\n",
    "        _, y_marginal = marginal_batch\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass, criterion computation\n",
    "        mine_score, mine_matrix = mine_network(x_joint, y_joint, y_marginal)\n",
    "        mine_matrices.append(mine_matrix)\n",
    "        \n",
    "        criterion = mine_score\n",
    "        \n",
    "        print('MI lower bound =  ', -criterion.item()) #-neg_mine_estimate\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        criterion.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(mine_network.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_neg_mine_estimate += criterion.item()\n",
    "        #print('MI lower bound running mean ', -running_neg_mine_estimate )\n",
    "        \n",
    "            \n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print('[%d, %5d] MI lower bound running mean: %.3f' %\n",
    "                  (epoch + 1, i + 1, -running_neg_mine_estimate / 100))\n",
    "            running_neg_mine_estimate = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(joint_batches), len(marginal_batches), len(joint_batches[0]), len(marginal_batches[0]), \n",
    "joint_batches[-1][0].shape, \n",
    "marginal_batches[-1][0].shape,  \n",
    "joint_batches[-1][0].shape, \n",
    "marginal_batches[-1][0].shape,  \n",
    "joint_batches[-1][0].shape, marginal_batches[-1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming output_matrix is the output from the neural network\n",
    "output_matrix = np.random.rand(8, 12)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "cax = plt.matshow(output_matrix, cmap=\"viridis\")\n",
    "plt.colorbar(cax)\n",
    "plt.title(\"Neural Network Output Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Generating a sequence of 1000 matrices (8x12) as an example\n",
    "sequence_of_matrices = np.random.rand(1000, 8, 12)\n",
    "\n",
    "# Set up the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cax = ax.matshow(sequence_of_matrices[0], cmap=\"viridis\")\n",
    "plt.colorbar(cax)\n",
    "plt.title(\"Neural Network Output Heatmap\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "\n",
    "def update(frame):\n",
    "    cax.set_array(sequence_of_matrices[frame].flatten())\n",
    "    return [cax]\n",
    "\n",
    "# Create an animation\n",
    "ani = FuncAnimation(fig, update, frames=range(1000), interval=50, blit=True)\n",
    "\n",
    "# To save the animation, use ani.save('filename.mp4')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Define the audio stream parameters\n",
    "FORMAT = pyaudio.paInt16  # Audio format (16-bit PCM)\n",
    "CHANNELS = 1             # Number of audio channels (1 for mono, 2 for stereo)\n",
    "RATE = 44100             # Sample rate (samples per second)\n",
    "CHUNK = 512              # Number of frames per buffer\n",
    "DEVICE_INDEX = 1         # Index of the audio device to use\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Define the dictionary\n",
    "my_dict = {0: 0}\n",
    "\n",
    "# List to save the audio buffers and dictionary values\n",
    "saved_data = []\n",
    "\n",
    "# Define the callback function\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    # Convert the byte data to numpy array\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "    \n",
    "    # Save the audio buffer and dictionary value to the list\n",
    "    saved_data.append([audio_data.tolist(), my_dict[0]])\n",
    "    \n",
    "    # Return the data and continue\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                stream_callback=callback)\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Keep the script running to record audio in real time\n",
    "print(\"Recording... Press Ctrl+C to stop.\")\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        pass\n",
    "except KeyboardInterrupt:\n",
    "    # Stop the audio stream when Ctrl+C is pressed\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    print(\"Recording stopped.\")\n",
    "\n",
    "# Print the saved data\n",
    "#print(\"Saved data:\", saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "# Audio Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 512\n",
    "DEVICE_INDEX = 1\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Define the dictionary\n",
    "my_dict = {0: 0}\n",
    "\n",
    "# List to save the audio buffers and dictionary values\n",
    "saved_audio_data = []\n",
    "\n",
    "# List to save the poses\n",
    "saved_poses = []\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Define the callback function for audio\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "    saved_audio_data.append([audio_data.tolist(), my_dict[0]])\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#print(\"Recording... Press 'q' in the webcam window to stop.\")\n",
    "try:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "        \n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose results\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Save the pose landmarks\n",
    "            pose_landmarks = results.pose_landmarks.landmark\n",
    "            pose_data = [[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]\n",
    "            saved_poses.append(pose_data)\n",
    "            \n",
    "            # Increment the dictionary value\n",
    "            my_dict[0] += 1\n",
    "        \n",
    "        # Display the frame\n",
    "        #cv2.imshow('Pose Tracking', frame)\n",
    "        \n",
    "        cv2.waitKey(1) \n",
    "       \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Recording stopped.\")\n",
    "\n",
    "finally:\n",
    "    # Release the webcam\n",
    "    cap.release()\n",
    "    \n",
    "    # Close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    \n",
    "    # Close the OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Convert saved audio data and poses to PyTorch tensors\n",
    "    audio_data_tensors = [torch.tensor(audio_data) for audio_data, _ in saved_audio_data]\n",
    "    dict_values_tensors = torch.tensor([dict_value for _, dict_value in saved_audio_data], dtype=torch.int32)\n",
    "    poses_tensors = torch.tensor(saved_poses, dtype=torch.float32)\n",
    "\n",
    "    # Save the tensors to disk with the date and time in the filename\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    audio_data_file_path = f'audio_data_tensors_{formatted_time}.pt'\n",
    "    dict_values_file_path = f'dict_values_tensors_{formatted_time}.pt'\n",
    "    poses_file_path = f'poses_tensors_{formatted_time}.pt'\n",
    "    torch.save(audio_data_tensors, audio_data_file_path)\n",
    "    torch.save(dict_values_tensors, dict_values_file_path)\n",
    "    torch.save(poses_tensors, poses_file_path)\n",
    "\n",
    "    print(\"Tensors saved to disk.\")\n",
    "    print(f\"Audio data tensors saved to {audio_data_file_path}\")\n",
    "    print(f\"Dictionary values tensors saved to {dict_values_file_path}\")\n",
    "    print(f\"Poses tensors saved to {poses_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "# Audio Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 512\n",
    "DEVICE_INDEX = 1\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Define the dictionary\n",
    "my_dict = {0: 0}\n",
    "\n",
    "# List to save the audio buffers and dictionary values\n",
    "saved_audio_data = []\n",
    "\n",
    "# List to save the poses\n",
    "saved_poses = []\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Define the callback function for audio\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "    saved_audio_data.append([audio_data.tolist(), my_dict[0]])\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Recording... Press 'q' in the webcam window to stop.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get pose results\n",
    "    results = pose.process(rgb_frame)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Save the pose landmarks\n",
    "        pose_landmarks = results.pose_landmarks.landmark\n",
    "        pose_data = [[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]\n",
    "        saved_poses.append(pose_data)\n",
    "        \n",
    "        # Increment the dictionary value\n",
    "        my_dict[0] += 1\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Pose Tracking', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the audio stream\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "print(\"Recording stopped.\")\n",
    "\n",
    "audio_data_tensors = [torch.tensor(audio_data) for audio_data, _ in saved_audio_data]\n",
    "dict_values_tensors = torch.tensor([dict_value for _, dict_value in saved_audio_data], dtype=torch.int32)\n",
    "poses_tensors = torch.tensor(saved_poses, dtype=torch.float32)\n",
    "\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the tensors to disk with the date and time in the filename\n",
    "audio_data_file_path = f'audio_data_tensors_{formatted_time}.pt'\n",
    "dict_values_file_path = f'dict_values_tensors_{formatted_time}.pt'\n",
    "poses_file_path = f'poses_tensors_{formatted_time}.pt'\n",
    "\n",
    "torch.save(audio_data_tensors, audio_data_file_path)\n",
    "torch.save(dict_values_tensors, dict_values_file_path)\n",
    "torch.save(poses_tensors, poses_file_path)\n",
    "\n",
    "print(f\"Audio data tensors saved to {audio_data_file_path}\")\n",
    "print(f\"Dictionary values tensors saved to {dict_values_file_path}\")\n",
    "print(f\"Poses tensors saved to {poses_file_path}\")\n",
    "\n",
    "# Print the saved audio data\n",
    "print(\"Saved audio data:\", saved_audio_data)\n",
    "\n",
    "# Print the number of recorded poses\n",
    "print(\"Number of recorded poses:\", len(saved_poses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "\n",
    "# Audio Configuration\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 512\n",
    "DEVICE_INDEX = 1\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Define the dictionary\n",
    "my_dict = {0: 0}\n",
    "\n",
    "# Lists to save the audio buffers, dictionary values, and poses\n",
    "saved_audio_data = []\n",
    "saved_poses = []\n",
    "\n",
    "# Initialize MediaPipe BlazePose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Define the callback function for audio\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    audio_data = np.frombuffer(in_data, dtype=np.int16)\n",
    "    saved_audio_data.append([audio_data.tolist(), my_dict[0]])\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=DEVICE_INDEX,\n",
    "                frames_per_buffer=CHUNK,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Recording... Press 'q' in the webcam window to stop.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get pose results\n",
    "    results = pose.process(rgb_frame)\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        # Save the pose landmarks\n",
    "        pose_landmarks = results.pose_landmarks.landmark\n",
    "        pose_data = [[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose_landmarks]\n",
    "        saved_poses.append(pose_data)\n",
    "        \n",
    "        # Increment the dictionary value\n",
    "        my_dict[0] += 1\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Pose Tracking', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam\n",
    "cap.release()\n",
    "\n",
    "# Close the audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "# Close the OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Recording stopped.\")\n",
    "\n",
    "# Convert saved audio data and poses to PyTorch tensors\n",
    "audio_data_tensors = [torch.tensor(audio_data) for audio_data, _ in saved_audio_data]\n",
    "dict_values_tensors = torch.tensor([dict_value for _, dict_value in saved_audio_data], dtype=torch.int32)\n",
    "poses_tensors = torch.tensor(saved_poses, dtype=torch.float32)\n",
    "\n",
    "print(\"Audio data tensors:\", audio_data_tensors)\n",
    "print(\"Dictionary values tensors:\", dict_values_tensors)\n",
    "print(\"Poses tensors:\", poses_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, nonlinearity='tanh'):\n",
    "        super(LSTMCell, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=batch_first)\n",
    "        \n",
    "        for param in self.lstm.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        x = self.layer_norm(x)\n",
    "        output, hn = self.lstm(x, state)\n",
    "        return output, hn\n",
    "    \n",
    "class GRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, nonlinearity='tanh'):\n",
    "        super(GRUCell, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=batch_first)\n",
    "        \n",
    "        for param in self.gru.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        x = self.layer_norm(x)\n",
    "        output, hn = self.gru(x, state)\n",
    "        return output, hn\n",
    "    \n",
    "\n",
    "class TransformerCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, nonlinearity='tanh'):\n",
    "        super(TransformerCell, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_size, nhead=hidden_size // 30),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        x = self.layer_norm(x)\n",
    "        output = self.transformer(x)\n",
    "        return output, None  # Transformer does not maintain state like RNNs\n",
    "\n",
    "\n",
    "\n",
    "class FCLayersCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, nonlinearity='tanh'):\n",
    "        # ...\n",
    "        super(FCLayersCell, self).__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        x = self.layer_norm(x)\n",
    "        output = self.layers(x)\n",
    "        return output, None  # Fully connected layers do not maintain state like RNNs\n",
    "\n",
    "\n",
    "class WaveNetCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, batch_first=True, nonlinearity='relu'):\n",
    "        super(WaveNetCell, self).__init__()\n",
    "        \n",
    "        # Adding Layer Normalization\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        \n",
    "        # Defining a simple WaveNet-style CNN\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_size, 2*hidden_size, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Applying Xavier Uniform Initialization to CNN weights\n",
    "        for layer in self.cnn:\n",
    "            if isinstance(layer, nn.Conv1d):\n",
    "                init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        x = self.layer_norm(x)\n",
    "        output = self.cnn(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return output, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [FCLayersCell, TransformerCell, GRUCell, LSTMCell, FCLayersCell, TransformerCell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in cells:\n",
    "\n",
    "    mine_network = Multiscale_MINE_test(cell, nr_time_scales = 9, \n",
    "                                nr_frequency_bands = 13, \n",
    "                                embedding_dim = 100, #gives dim 200 for the concattenated encoding of audio and mocap input\n",
    "                                input_dim = 1470, # selected input size for audio sample rate 44100 since blazenet runs at aproximately 30fps\n",
    "                                nr_layers_per_timescale = 4, #nr of layers for the RNNs encoding the respective timescales\n",
    "                                nr_spiralnet_layers = 16, \n",
    "                                delay_size = 3) \n",
    "    \n",
    "    optimizer = optim.AdamW(mine_network.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # Number of training epochs\n",
    "    num_epochs = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (joint_batch, marginal_batch) in enumerate(zip(joint_batches, marginal_batches)):\n",
    "            # Extract audio, joint poses, and marginal poses from batches\n",
    "            x_joint, y_joint = joint_batch\n",
    "            _, y_marginal = marginal_batch\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print('x_joint : ', x_joint)\n",
    "            #print('y_joint : ', y_joint)\n",
    "            #print('y_marginal : ', y_marginal)\n",
    "            # Forward pass\n",
    "            score_joint, score_marginal = mine_network(x_joint, y_joint, y_marginal)\n",
    "            \n",
    "            #print(score_joint)\n",
    "            #print(score_marginal)\n",
    "            # Loss computation\n",
    "            loss = mine_score(score_joint, score_marginal)\n",
    "            print('MI lower bound =  ', loss)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(mine_network.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "            print('loss', -running_loss)\n",
    "            \n",
    "                \n",
    "            if i % 100 == 99:    # Print every 100 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, layer_type):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.input_dim = 1024\n",
    "        self.hidden_dim = 100\n",
    "        self.num_layers = 4\n",
    "        self.batch_size = 32\n",
    "        self.seq_len = 10\n",
    "        \n",
    "        if layer_type == 'RNN':\n",
    "            self.layer = nn.RNN(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        elif layer_type == 'LSTM':\n",
    "            self.layer = nn.LSTM(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        elif layer_type == 'GRU':\n",
    "            self.layer = nn.GRU(input_size=self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        elif layer_type == 'FC':\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(self.input_dim, self.hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            )\n",
    "        elif layer_type == 'Transformer':\n",
    "            self.layer = nn.Transformer(d_model=self.input_dim, num_encoder_layers=self.num_layers, num_decoder_layers=self.num_layers, batch_first=True)\n",
    "        \n",
    "        elif layer_type == 'CNN':\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv1d(self.input_dim, self.hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=3, padding=1),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer type\")\n",
    "\n",
    "        # Initializing weights\n",
    "        for param in self.layer.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.layer_type in ['RNN', 'LSTM', 'GRU']:\n",
    "            output, _ = self.layer(x)\n",
    "        elif self.layer_type == 'Transformer':\n",
    "            output = self.layer(x, x)\n",
    "        else:  # 'FC' or 'CNN'\n",
    "            output = self.layer(x)\n",
    "        return output\n",
    "\n",
    "# Validation function\n",
    "def validate_layer(layer_type):\n",
    "    model = DummyModel(layer_type)\n",
    "    x = torch.randn(model.batch_size, model.seq_len, model.input_dim)\n",
    "    output = model(x)\n",
    "    print(f\"Layer Type: {layer_type}\")\n",
    "    print(f\"Input Shape: {x.shape}\")\n",
    "    print(f\"Output Shape: {output.shape}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# List of layer types to validate\n",
    "layer_types = ['RNN', 'LSTM', 'GRU', 'Transformer', 'FC', 'CNN']\n",
    "\n",
    "# Run validation\n",
    "for layer_type in layer_types:\n",
    "    validate_layer(layer_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
