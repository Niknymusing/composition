{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving on ('127.0.0.1', 4548)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W0sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(x[:, :, (model_lookback \u001b[39m+\u001b[39m i\u001b[39m*\u001b[39mbuffer_length):(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mmodel_lookback \u001b[39m+\u001b[39m i\u001b[39m*\u001b[39mbuffer_length)])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W0sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m z[:, \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m perturbation_tensors[\u001b[39m0\u001b[39m]\u001b[39m# torch.linspace(-2, 2, z.shape[-1])\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(z)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m last_samples \u001b[39m=\u001b[39m y[\u001b[39m-\u001b[39mbuffer_length:]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# Write to PyAudio stream\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa as li \n",
    "import soundfile as sf\n",
    "from typing import Any, Callable, Optional, Union\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "from torch import nn\n",
    "import time\n",
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "wav_path = \"/Users/nikny/musing_instruments/data/stravinski_wav/01 Petroushka (Original 1911 Version), First Scene_ I. The Shrove-tide Fair.wav\"\n",
    "model_path1 = \"/Users/nikny/Downloads/percussion.ts\"\n",
    "model_path2 = \"/Users/nikny/Downloads/nasa.ts\"\n",
    "model_path3 = \"/Users/nikny/Downloads/vintage.ts\" #too heavy\n",
    "perturbation_tensors = {0: torch.ones(47)}\n",
    "\n",
    "\n",
    "def osc_callback0(addr, *args):\n",
    "\n",
    "    perturbation_tensors[0][0] *= args[0]#tensor\n",
    "    print('updated 0')\n",
    "\n",
    "def osc_callback1(addr, *args):\n",
    "\n",
    "    perturbation_tensors[0][1] *= args[0]#tensor\n",
    "    print('updated 1')\n",
    "\n",
    "def osc_callback2(addr, *args):\n",
    "\n",
    "    perturbation_tensors[0][2] *= args[0]#tensor\n",
    "    print('updated 2')\n",
    "\n",
    "def osc_callback3(addr, *args):\n",
    "\n",
    "    perturbation_tensors[0][3] *= args[0]#tensor\n",
    "    print('updated 3')\n",
    "\n",
    "# Setting up the OSC server\n",
    "def start_osc_server(ip, port):\n",
    "    disp = dispatcher.Dispatcher()\n",
    "    disp.map(\"/latent_perturbations0\", osc_callback0)\n",
    "    disp.map(\"/latent_perturbations1\", osc_callback1)\n",
    "    disp.map(\"/latent_perturbations2\", osc_callback2)\n",
    "    disp.map(\"/latent_perturbations3\", osc_callback3)\n",
    "    server = osc_server.ThreadingOSCUDPServer((ip, port), disp)\n",
    "    print(f\"Serving on {server.server_address}\")\n",
    "    server.serve_forever()\n",
    "\n",
    "\n",
    "def convert_wav_to_float32(input_path, output_path):\n",
    "    # Open the wav file\n",
    "    with wave.open(input_path, 'rb') as wav_file:\n",
    "        # Extract audio data and parameters\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        framerate = wav_file.getframerate()\n",
    "        n_frames = wav_file.getnframes()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "\n",
    "        # Convert audio data to numpy array depending on the sample width\n",
    "        if sample_width == 1:  # 8-bit WAV files are unsigned\n",
    "            data = np.frombuffer(audio_data, dtype=np.uint8) - 128\n",
    "        elif sample_width == 2:  # 16-bit WAV files are signed\n",
    "            data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        else:\n",
    "            raise ValueError(\"Only supports 8 or 16 bit audio formats.\")\n",
    "\n",
    "        # Normalize the data to the range between -1.0 and 1.0\n",
    "        max_int_value = float(2 ** (8 * sample_width - 1))\n",
    "        data = data / max_int_value\n",
    "\n",
    "        # Write the data to a new file\n",
    "        sf.write(output_path, data, framerate, 'FLOAT')\n",
    "\n",
    "# Replace 'input_path.wav' and 'output_path.wav' with the actual paths\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')\n",
    "\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')\n",
    "torch.set_grad_enabled(False)\n",
    "model = torch.jit.load(model_path1).eval()\n",
    "x = li.load(\"output_path_float32.wav\")[0]\n",
    "x = torch.from_numpy(x).reshape(1, 1, -1)\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "osc_thread = Thread(target=start_osc_server, args=('127.0.0.1', 4548))\n",
    "osc_thread.start()\n",
    "# Assuming the audio is mono (1 channel) and float32 format, adjust as needed\n",
    "stream = p.open(format=pyaudio.paFloat32,  # float32 format\n",
    "                channels=1,                # Mono audio\n",
    "                rate=44100,                # Sample rate\n",
    "                output=True,               # Output stream\n",
    "                output_device_index=2) \n",
    "\n",
    "buffer_length = 8192\n",
    "model_lookback = 96000\n",
    "output_buffer = []\n",
    "for i in range((x.shape[2] // buffer_length) - 13):\n",
    "    #t = time.time()\n",
    "    z = model.encode(x[:, :, (model_lookback + i*buffer_length):(2*model_lookback + i*buffer_length)])\n",
    "    z[:, 2] += perturbation_tensors[0]# torch.linspace(-2, 2, z.shape[-1])\n",
    "    y = model.decode(z).numpy().reshape(-1)\n",
    "    last_samples = y[-buffer_length:].astype('float32').tobytes()\n",
    "    # Write to PyAudio stream\n",
    "    stream.write(last_samples)\n",
    "    #print(y.shape)\n",
    "    #print(time.time() - t)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "\n",
    "# Terminate PyAudio\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192512,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192512,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cached_conv as cc\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = cc.CachedSequential(\n",
    "            cc.Conv1d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            cc.Conv1d(16, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            cc.Conv1d(16, 16, 3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.decoder = cc.CachedSequential(\n",
    "            cc.ConvTranspose1d(16, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            cc.ConvTranspose1d(16, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            cc.ConvTranspose1d(16, 1, 4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, AdamW\n",
    "cc.use_cached_conv(True)\n",
    "model = AutoEncoder()\n",
    "optimizer = AdamW(model.parameters(), lr = 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path1 = \"/Users/nikny/Downloads/percussion.ts\"\n",
    "torch.set_grad_enabled(True)\n",
    "model = torch.jit.load(model_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 23, in forward\n  def forward(self: __torch__.TraceModel,\n    x: Tensor) -> Tensor:\n    return (self).decode((self).encode(x, ), )\n                          ~~~~~~~~~~~~ <--- HERE\n  def decode(self: __torch__.TraceModel,\n    z: Tensor) -> Tensor:\n  File \"code/__torch__.py\", line 92, in encode\n    x4 = (pqmf).forward(x3, )\n    encoder = self.encoder\n    mean, scale, = (encoder).forward(x4, )\n                    ~~~~~~~~~~~~~~~~ <--- HERE\n    _10 = (self).post_process_distribution(mean, scale, )\n    mean0, std, = _10\n  File \"code/__torch__/rave/model.py\", line 11, in forward\n    x: Tensor) -> List[Tensor]:\n    net = self.net\n    z = (net).forward(x, )\n         ~~~~~~~~~~~~ <--- HERE\n    _0 = torch.floordiv((torch.size(z))[1], 2)\n    return torch.split(z, _0, 1)\n  File \"code/__torch__/cached_conv/convs.py\", line 106, in forward\n    _13 = getattr(self, \"13\")\n    _14 = getattr(self, \"14\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n  File \"code/__torch__/cached_conv/convs/___torch_mangle_1.py\", line 27, in forward\n    x0 = (downsampling_delay).forward(x, )\n    cache = self.cache\n    x1 = (cache).forward(x0, )\n          ~~~~~~~~~~~~~~ <--- HERE\n    weight = self.weight\n    bias = self.bias\n  File \"code/__torch__/cached_conv/convs.py\", line 56, in forward\n      padding0 = self.padding\n      _2 = torch.slice(x3, -1, torch.neg(padding0))\n      _3 = torch.copy_(pad0, _2)\n           ~~~~~~~~~~~ <--- HERE\n      crop = self.crop\n      if crop:\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 197, in forward\n    def forward(self, x):\n        return self.decode(self.encode(x))\n                           ~~~~~~~~~~~ <--- HERE\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 116, in encode\n            x = self.pqmf(x)\n    \n        mean, scale = self.encoder(x)\n                      ~~~~~~~~~~~~ <--- HERE\n        mean, std = self.post_process_distribution(mean, scale)\n    \n  File \"/slow-2/antoine/projects/instances/release/rave/model.py\", line 344, in forward\n    def forward(self, x):\n        z = self.net(x)\n            ~~~~~~~~ <--- HERE\n        return torch.split(z, z.shape[1] // 2, 1)\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 141, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 120, in forward\n    def forward(self, x):\n        x = self.downsampling_delay(x)\n        x = self.cache(x)\n            ~~~~~~~~~~ <--- HERE\n        return nn.functional.conv1d(\n            x,\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 78, in forward\n        if self.padding:\n            x = torch.cat([self.pad, x], -1)\n            self.pad.copy_(x[..., -self.padding:])\n            ~~~~~~~~~~~~~~ <--- HERE\n    \n            if self.crop:\nRuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m DataX, DataY \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(S, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4096\u001b[39m), torch\u001b[39m.\u001b[39mrandn(S, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4096\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m o \u001b[39m=\u001b[39m model(DataX[\u001b[39m3\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(o \u001b[39m-\u001b[39m DataY[\u001b[39m3\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 23, in forward\n  def forward(self: __torch__.TraceModel,\n    x: Tensor) -> Tensor:\n    return (self).decode((self).encode(x, ), )\n                          ~~~~~~~~~~~~ <--- HERE\n  def decode(self: __torch__.TraceModel,\n    z: Tensor) -> Tensor:\n  File \"code/__torch__.py\", line 92, in encode\n    x4 = (pqmf).forward(x3, )\n    encoder = self.encoder\n    mean, scale, = (encoder).forward(x4, )\n                    ~~~~~~~~~~~~~~~~ <--- HERE\n    _10 = (self).post_process_distribution(mean, scale, )\n    mean0, std, = _10\n  File \"code/__torch__/rave/model.py\", line 11, in forward\n    x: Tensor) -> List[Tensor]:\n    net = self.net\n    z = (net).forward(x, )\n         ~~~~~~~~~~~~ <--- HERE\n    _0 = torch.floordiv((torch.size(z))[1], 2)\n    return torch.split(z, _0, 1)\n  File \"code/__torch__/cached_conv/convs.py\", line 106, in forward\n    _13 = getattr(self, \"13\")\n    _14 = getattr(self, \"14\")\n    input0 = (_0).forward(input, )\n              ~~~~~~~~~~~ <--- HERE\n    input1 = (_1).forward(input0, )\n    input2 = (_2).forward(input1, )\n  File \"code/__torch__/cached_conv/convs/___torch_mangle_1.py\", line 27, in forward\n    x0 = (downsampling_delay).forward(x, )\n    cache = self.cache\n    x1 = (cache).forward(x0, )\n          ~~~~~~~~~~~~~~ <--- HERE\n    weight = self.weight\n    bias = self.bias\n  File \"code/__torch__/cached_conv/convs.py\", line 56, in forward\n      padding0 = self.padding\n      _2 = torch.slice(x3, -1, torch.neg(padding0))\n      _3 = torch.copy_(pad0, _2)\n           ~~~~~~~~~~~ <--- HERE\n      crop = self.crop\n      if crop:\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 197, in forward\n    def forward(self, x):\n        return self.decode(self.encode(x))\n                           ~~~~~~~~~~~ <--- HERE\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 116, in encode\n            x = self.pqmf(x)\n    \n        mean, scale = self.encoder(x)\n                      ~~~~~~~~~~~~ <--- HERE\n        mean, std = self.post_process_distribution(mean, scale)\n    \n  File \"/slow-2/antoine/projects/instances/release/rave/model.py\", line 344, in forward\n    def forward(self, x):\n        z = self.net(x)\n            ~~~~~~~~ <--- HERE\n        return torch.split(z, z.shape[1] // 2, 1)\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 141, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 120, in forward\n    def forward(self, x):\n        x = self.downsampling_delay(x)\n        x = self.cache(x)\n            ~~~~~~~~~~ <--- HERE\n        return nn.functional.conv1d(\n            x,\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 78, in forward\n        if self.padding:\n            x = torch.cat([self.pad, x], -1)\n            self.pad.copy_(x[..., -self.padding:])\n            ~~~~~~~~~~~~~~ <--- HERE\n    \n            if self.crop:\nRuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "DataX, DataY = torch.randn(S, 1, 1, 4096), torch.randn(S, 1, 1, 4096)\n",
    "optimizer.zero_grad()\n",
    "o = model(DataX[3])\n",
    "loss = torch.norm(o - DataY[3])\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(optimizer.param_groups[0]['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params'][6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Optimizer.state_dict of AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_avg: tensor([[ 1.2733e-03,  2.4114e-06, -8.1230e-03,  ...,  6.3951e-03,\n",
      "          3.2222e-03,  4.4032e-03],\n",
      "        [ 7.4183e-04,  3.8198e-03,  1.7003e-03,  ...,  1.2080e-03,\n",
      "         -1.1888e-03, -3.4950e-03],\n",
      "        [-5.0784e-04,  4.1229e-04, -3.6956e-03,  ...,  7.6238e-03,\n",
      "         -5.1172e-04, -1.0482e-03],\n",
      "        ...,\n",
      "        [-1.2483e-03, -3.5841e-04,  2.8999e-03,  ..., -9.5514e-03,\n",
      "          9.2199e-04, -4.0033e-03],\n",
      "        [-1.7406e-04,  2.9575e-03, -3.7766e-03,  ...,  6.9287e-03,\n",
      "         -3.8486e-03,  2.3028e-03],\n",
      "        [ 1.0000e-03, -2.0914e-03,  5.0888e-04,  ..., -1.3420e-03,\n",
      "          1.1733e-03, -2.2819e-03]])\n",
      "exp_avg_sq: tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]])\n",
      "exp_avg: tensor([ 6.4382e-03,  4.1421e-03,  8.5815e-04,  2.2679e-03, -2.1460e-03,\n",
      "         5.2707e-03, -3.3603e-03, -5.3088e-04,  1.7401e-03, -4.6176e-03,\n",
      "         2.9199e-03,  1.7248e-03,  7.1479e-03, -3.0444e-03, -1.8437e-03,\n",
      "        -1.0483e-05, -6.2677e-03,  4.9061e-03,  3.4339e-03,  2.0088e-03,\n",
      "         2.5575e-03,  3.7241e-04, -1.8499e-03, -1.3138e-03,  1.0852e-03,\n",
      "        -8.7615e-04, -5.0810e-03,  1.8559e-03, -4.3032e-03, -1.4592e-03,\n",
      "         4.5871e-03, -2.5109e-03,  9.4894e-04, -2.9143e-03, -7.7060e-03,\n",
      "        -7.7141e-04, -8.0627e-03, -1.0429e-03, -1.0622e-03,  2.6981e-03,\n",
      "        -4.0187e-04, -4.2000e-03,  8.9775e-04, -1.9332e-03,  2.4869e-03,\n",
      "        -2.8487e-03,  1.5368e-03, -2.9663e-03,  2.0786e-03, -8.8073e-04,\n",
      "         3.4001e-03,  1.9367e-03,  6.5335e-04, -5.4201e-03, -7.5166e-03,\n",
      "         4.1210e-03, -5.7484e-03, -9.1753e-03,  9.8519e-04,  1.6283e-03,\n",
      "        -7.3180e-04, -2.3477e-03, -9.4691e-04, -7.0269e-03, -3.5879e-03,\n",
      "         1.4423e-03, -1.4745e-03, -4.7462e-03,  1.3336e-04,  1.3604e-03,\n",
      "        -8.3110e-03, -4.0569e-04, -7.7126e-04,  3.0287e-03,  2.9384e-03,\n",
      "         2.0064e-03, -1.0169e-02, -1.0277e-03,  6.2623e-03,  3.4334e-03,\n",
      "        -2.1773e-03,  1.0485e-03,  1.8216e-03,  3.4870e-03, -4.2451e-03,\n",
      "        -1.5652e-03,  5.4681e-03,  1.0923e-03, -3.7951e-05, -2.5415e-03,\n",
      "        -2.2518e-05, -5.6417e-04, -7.2005e-05, -7.6401e-03, -2.5681e-03,\n",
      "         2.7075e-03, -3.1871e-03,  2.3267e-03,  4.7510e-03, -3.2468e-03])\n",
      "exp_avg_sq: tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002])\n",
      "exp_avg: tensor([[-0.0030, -0.0043, -0.0065,  ..., -0.0026,  0.0013,  0.0005],\n",
      "        [-0.0005,  0.0034,  0.0019,  ...,  0.0021,  0.0001, -0.0010],\n",
      "        [-0.0019, -0.0018,  0.0046,  ...,  0.0048, -0.0049,  0.0012],\n",
      "        ...,\n",
      "        [-0.0014,  0.0010, -0.0020,  ..., -0.0027,  0.0026, -0.0068],\n",
      "        [ 0.0006, -0.0030, -0.0030,  ..., -0.0031,  0.0001,  0.0047],\n",
      "        [ 0.0002,  0.0027, -0.0015,  ...,  0.0016,  0.0023, -0.0058]])\n",
      "exp_avg_sq: tensor([[0.0002, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0001, 0.0002, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001]])\n",
      "exp_avg: tensor([-0.0034,  0.0017,  0.0044, -0.0059,  0.0084, -0.0092, -0.0067, -0.0038,\n",
      "         0.0109,  0.0023,  0.0067, -0.0008, -0.0028, -0.0046, -0.0024,  0.0074,\n",
      "        -0.0041,  0.0002,  0.0063, -0.0074, -0.0028,  0.0065,  0.0144, -0.0151,\n",
      "         0.0051,  0.0032,  0.0082, -0.0007, -0.0060, -0.0137,  0.0034, -0.0015,\n",
      "         0.0094, -0.0020,  0.0011,  0.0017,  0.0083,  0.0011, -0.0022,  0.0031,\n",
      "         0.0028, -0.0004, -0.0019, -0.0006, -0.0028,  0.0101,  0.0026,  0.0079,\n",
      "         0.0010, -0.0129,  0.0001,  0.0015, -0.0003,  0.0027,  0.0172, -0.0048,\n",
      "        -0.0068, -0.0047,  0.0039, -0.0056,  0.0041, -0.0026, -0.0023,  0.0007,\n",
      "         0.0013, -0.0176, -0.0082, -0.0048,  0.0053, -0.0042, -0.0136, -0.0013,\n",
      "         0.0057,  0.0049, -0.0226,  0.0022,  0.0027,  0.0043,  0.0061,  0.0023,\n",
      "         0.0065,  0.0026,  0.0104, -0.0066,  0.0062, -0.0026,  0.0059,  0.0050,\n",
      "         0.0032,  0.0134,  0.0079, -0.0068,  0.0111, -0.0037,  0.0049,  0.0019,\n",
      "         0.0120,  0.0041, -0.0079,  0.0070])\n",
      "exp_avg_sq: tensor([0.0007, 0.0006, 0.0006, 0.0006, 0.0006, 0.0006, 0.0005, 0.0005, 0.0006,\n",
      "        0.0006, 0.0006, 0.0006, 0.0005, 0.0006, 0.0006, 0.0004, 0.0005, 0.0005,\n",
      "        0.0007, 0.0007, 0.0006, 0.0004, 0.0007, 0.0006, 0.0006, 0.0005, 0.0006,\n",
      "        0.0005, 0.0006, 0.0006, 0.0007, 0.0006, 0.0006, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0006, 0.0006, 0.0006, 0.0005, 0.0004, 0.0005, 0.0006,\n",
      "        0.0007, 0.0004, 0.0006, 0.0006, 0.0004, 0.0005, 0.0005, 0.0006, 0.0007,\n",
      "        0.0005, 0.0005, 0.0006, 0.0005, 0.0005, 0.0006, 0.0007, 0.0006, 0.0004,\n",
      "        0.0006, 0.0007, 0.0006, 0.0005, 0.0005, 0.0005, 0.0007, 0.0008, 0.0006,\n",
      "        0.0005, 0.0006, 0.0005, 0.0005, 0.0006, 0.0004, 0.0006, 0.0005, 0.0005,\n",
      "        0.0006, 0.0007, 0.0007, 0.0005, 0.0005, 0.0005, 0.0006, 0.0007, 0.0005,\n",
      "        0.0005, 0.0006, 0.0005, 0.0006, 0.0005, 0.0004, 0.0006, 0.0007, 0.0005,\n",
      "        0.0004])\n",
      "exp_avg: tensor([[-6.4811e-03, -1.2774e-04,  6.9855e-03,  ..., -5.7678e-03,\n",
      "         -7.5143e-03, -3.6213e-03],\n",
      "        [ 5.1937e-03,  1.3022e-03,  7.7760e-04,  ..., -2.1833e-03,\n",
      "          3.2612e-03,  4.0178e-04],\n",
      "        [-1.9622e-03,  5.2272e-03,  4.5257e-03,  ..., -3.2170e-03,\n",
      "         -3.7575e-03, -3.5225e-04],\n",
      "        ...,\n",
      "        [ 1.7015e-03,  1.8100e-03, -1.5264e-03,  ..., -2.5897e-04,\n",
      "         -7.1832e-04, -2.6799e-03],\n",
      "        [ 1.6796e-03, -1.7457e-03, -1.7890e-03,  ..., -7.3452e-04,\n",
      "          1.1129e-05, -4.2033e-03],\n",
      "        [ 2.2277e-04,  1.6067e-03, -1.7739e-03,  ...,  2.6567e-03,\n",
      "         -6.6665e-04,  2.7470e-03]])\n",
      "exp_avg_sq: tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0001, 0.0001,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        [0.0002, 0.0001, 0.0002,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        ...,\n",
      "        [0.0002, 0.0001, 0.0001,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0003, 0.0003, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0003, 0.0002]])\n",
      "exp_avg: tensor([-1.3665e-02, -3.8172e-03, -1.7884e-02, -4.0257e-03,  8.8263e-03,\n",
      "        -5.6817e-03,  5.1982e-03,  2.4769e-02, -6.2971e-03,  6.0663e-03,\n",
      "        -6.5378e-03,  3.7461e-04,  1.9224e-03,  8.2663e-03, -5.3772e-03,\n",
      "        -5.5942e-03,  9.8050e-04, -3.0175e-05, -1.9416e-02,  1.9027e-02,\n",
      "        -1.5664e-02, -3.0861e-02,  8.5719e-03, -2.7285e-04, -1.4822e-02,\n",
      "        -1.8157e-02,  2.8881e-02, -1.0664e-02,  1.3237e-02,  2.3311e-02,\n",
      "         1.1678e-02, -3.5663e-03,  6.8320e-03,  1.4999e-02, -8.2903e-03,\n",
      "        -9.0886e-04, -1.8551e-02,  5.3325e-03,  5.9479e-04,  2.3471e-03,\n",
      "         7.7158e-03,  9.8189e-04, -5.5178e-04, -2.1203e-02,  9.8562e-03,\n",
      "        -1.0424e-02,  2.8318e-03,  6.6330e-04,  2.2539e-02,  1.5545e-02,\n",
      "         1.9372e-02,  7.0154e-03,  2.2889e-03, -1.1240e-02,  1.6046e-02,\n",
      "        -2.7384e-03,  2.2834e-03, -1.7735e-04,  2.8114e-03,  1.4949e-02,\n",
      "         1.5807e-02,  2.2305e-02,  3.2037e-03,  5.0229e-03, -5.5890e-03,\n",
      "         1.8144e-02, -8.5193e-03,  2.0392e-02,  3.6740e-03, -1.6436e-03,\n",
      "        -2.6415e-03,  6.0758e-04,  9.7265e-03,  8.3655e-04, -5.7638e-03,\n",
      "         1.7760e-02,  1.4820e-02,  2.8715e-03,  8.6128e-03,  6.2560e-03,\n",
      "         8.1290e-03, -1.3344e-02,  1.6438e-02, -1.5333e-02,  4.1993e-03,\n",
      "         2.2799e-02,  3.8394e-03, -2.5155e-02, -1.0740e-02,  8.4605e-03,\n",
      "         1.2138e-02,  4.2835e-04,  7.4703e-03,  6.2511e-03,  9.2329e-03,\n",
      "         2.8799e-02,  1.5105e-02, -1.6046e-03,  9.1126e-03, -1.1035e-02])\n",
      "exp_avg_sq: tensor([0.0021, 0.0018, 0.0017, 0.0018, 0.0019, 0.0019, 0.0018, 0.0022, 0.0020,\n",
      "        0.0021, 0.0020, 0.0019, 0.0020, 0.0019, 0.0020, 0.0021, 0.0017, 0.0019,\n",
      "        0.0018, 0.0024, 0.0018, 0.0022, 0.0018, 0.0017, 0.0017, 0.0022, 0.0022,\n",
      "        0.0021, 0.0018, 0.0017, 0.0022, 0.0019, 0.0021, 0.0019, 0.0021, 0.0019,\n",
      "        0.0017, 0.0019, 0.0020, 0.0018, 0.0018, 0.0021, 0.0021, 0.0020, 0.0019,\n",
      "        0.0019, 0.0016, 0.0018, 0.0020, 0.0018, 0.0021, 0.0019, 0.0019, 0.0016,\n",
      "        0.0018, 0.0020, 0.0018, 0.0019, 0.0020, 0.0021, 0.0021, 0.0019, 0.0020,\n",
      "        0.0020, 0.0019, 0.0018, 0.0019, 0.0019, 0.0021, 0.0016, 0.0021, 0.0021,\n",
      "        0.0023, 0.0017, 0.0019, 0.0019, 0.0017, 0.0019, 0.0018, 0.0016, 0.0019,\n",
      "        0.0020, 0.0021, 0.0015, 0.0019, 0.0018, 0.0020, 0.0019, 0.0022, 0.0021,\n",
      "        0.0020, 0.0022, 0.0017, 0.0017, 0.0017, 0.0020, 0.0022, 0.0017, 0.0022,\n",
      "        0.0022])\n",
      "exp_avg: tensor([[ 2.2037e-03,  5.0692e-03, -1.4534e-03,  ...,  2.3047e-04,\n",
      "         -3.3664e-03,  3.3138e-03],\n",
      "        [-5.8880e-04, -4.5300e-03, -8.5646e-04,  ..., -1.6397e-03,\n",
      "         -1.8734e-03, -5.9459e-04],\n",
      "        [-3.1828e-03, -6.3248e-04, -9.3015e-04,  ..., -1.8044e-03,\n",
      "         -6.1908e-04,  1.9582e-03],\n",
      "        ...,\n",
      "        [-2.3245e-03,  1.2484e-03, -1.1247e-04,  ...,  4.4493e-04,\n",
      "          1.2925e-03, -9.5497e-05],\n",
      "        [ 4.2969e-03, -2.4375e-03,  4.6009e-03,  ..., -1.6078e-03,\n",
      "          1.2289e-03,  1.2286e-03],\n",
      "        [-2.6396e-03, -1.7705e-03,  2.9788e-03,  ..., -1.6674e-03,\n",
      "         -1.4908e-03, -4.5879e-04]])\n",
      "exp_avg_sq: tensor([[0.0003, 0.0001, 0.0002,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0003, 0.0001, 0.0002,  ..., 0.0002, 0.0002, 0.0001],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0001, 0.0002,  ..., 0.0002, 0.0002, 0.0001]])\n",
      "exp_avg: tensor([ 0.0027,  0.0114,  0.0022, -0.0038, -0.0400, -0.0119, -0.0312,  0.0028,\n",
      "         0.0279,  0.0132,  0.0060,  0.0136, -0.0223,  0.0672,  0.0059, -0.0057,\n",
      "         0.0047,  0.0487,  0.0069, -0.0124, -0.0117, -0.0149, -0.0502,  0.0059,\n",
      "        -0.0067,  0.0084,  0.0261, -0.0005, -0.0352, -0.0257,  0.0115,  0.0148,\n",
      "         0.0021, -0.0217,  0.0656, -0.0113, -0.0157,  0.0121,  0.0271,  0.0130,\n",
      "         0.0427, -0.0006,  0.0371,  0.0024, -0.0243, -0.0074, -0.0373,  0.0255,\n",
      "        -0.0310, -0.0212,  0.0150, -0.0319,  0.0020,  0.0169, -0.0251, -0.0374,\n",
      "        -0.0037,  0.0120, -0.0047, -0.0284, -0.0495,  0.0450, -0.0294, -0.0156,\n",
      "         0.0020,  0.0293,  0.0033, -0.0040,  0.0269,  0.0119, -0.0277,  0.0014,\n",
      "         0.0235, -0.0125, -0.0060,  0.0126, -0.0286, -0.0074, -0.0098,  0.0107,\n",
      "        -0.0543, -0.0213, -0.0420,  0.0049, -0.0062, -0.0193,  0.0234, -0.0098,\n",
      "         0.0238,  0.0104,  0.0118,  0.0129, -0.0283,  0.0369,  0.0009, -0.0579,\n",
      "         0.0103,  0.0022,  0.0175, -0.0146])\n",
      "exp_avg_sq: tensor([0.0061, 0.0063, 0.0065, 0.0066, 0.0070, 0.0061, 0.0062, 0.0063, 0.0063,\n",
      "        0.0062, 0.0066, 0.0062, 0.0065, 0.0064, 0.0066, 0.0065, 0.0074, 0.0066,\n",
      "        0.0064, 0.0063, 0.0062, 0.0063, 0.0063, 0.0064, 0.0060, 0.0063, 0.0066,\n",
      "        0.0061, 0.0067, 0.0060, 0.0062, 0.0064, 0.0065, 0.0064, 0.0064, 0.0063,\n",
      "        0.0061, 0.0062, 0.0066, 0.0064, 0.0062, 0.0060, 0.0065, 0.0060, 0.0065,\n",
      "        0.0064, 0.0064, 0.0067, 0.0066, 0.0064, 0.0065, 0.0061, 0.0062, 0.0067,\n",
      "        0.0057, 0.0062, 0.0065, 0.0068, 0.0065, 0.0067, 0.0061, 0.0061, 0.0060,\n",
      "        0.0059, 0.0065, 0.0063, 0.0061, 0.0061, 0.0059, 0.0056, 0.0065, 0.0064,\n",
      "        0.0059, 0.0062, 0.0062, 0.0061, 0.0063, 0.0067, 0.0064, 0.0061, 0.0065,\n",
      "        0.0064, 0.0060, 0.0064, 0.0063, 0.0059, 0.0069, 0.0061, 0.0063, 0.0064,\n",
      "        0.0064, 0.0062, 0.0062, 0.0064, 0.0064, 0.0072, 0.0060, 0.0060, 0.0064,\n",
      "        0.0061])\n"
     ]
    }
   ],
   "source": [
    "# Assume model is your neural network model and optimizer is initialized\n",
    "#optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# After performing some training steps\n",
    "for param in model.parameters():\n",
    "    state = optimizer.state[param]\n",
    "    exp_avg = state.get('exp_avg')\n",
    "    exp_avg_sq = state.get('exp_avg_sq')\n",
    "    print(\"exp_avg:\", exp_avg)\n",
    "    print(\"exp_avg_sq:\", exp_avg_sq)\n",
    "\n",
    "\n",
    "# during training store: for each paremeter in model.parameters(), store a neural summation of the \n",
    "# optimizer.state[parameter], in a graph-structure with encoded connectivity of the model architecture by a GNN, \n",
    "# during training also store (input_feature, input_data, criterion/task, MI(model_optimizer_graph, reward)), \n",
    "# save this summary as the model's fingerprint. Use the fingerprint summary downstream to optimize finetuning \n",
    "# of model and reward function. Also as a way to explicitly correlate and control the latent dimensions of the encoding \n",
    "# to different learnt features, which are themselves biased by the style characteristics inherited from the dataset/feature-mappings \n",
    "# and reward function evolution learnt through the training and finetuning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(100, 100), nn.Linear(100, 100), nn.Linear(100, 100), nn.Linear(100, 100))\n",
    "optimizer = AdamW(model.parameters(), lr = 0.001)\n",
    "S = 1000\n",
    "DataX, DataY = torch.randn(S, 100), torch.randn(S, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "S = 1000\n",
    "\n",
    "lr_evolution = []\n",
    "for i in range(S):\n",
    "    optimizer.zero_grad()\n",
    "    o = model(DataX[i])\n",
    "    loss = torch.norm(o - DataY[i])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #for group in optimizer.param_groups:\n",
    "    #    for p in group['params']:\n",
    "            #if p.grad is not None:\n",
    "    #                state = optimizer.state[p]\n",
    "                    # Compute effective learning rate for this parameter\n",
    "                    # This is a simplified version, actual computation may vary\n",
    "                    # depending on the exact version of Adam you're using\n",
    "    #                lr = group['lr'] * (state.get('exp_avg_sq').sqrt() + 1e-8)\n",
    "    #                lr_evolution.append(lr.item())\n",
    "\n",
    "# Plotting\n",
    "plt.plot(lr_evolution)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Effective Learning Rate')\n",
    "plt.title('Evolution of Learning Rates in Adam Optimizer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {0: {}},\n",
       " 'param_groups': [{'lr': 0.01,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'eps': 1e-08,\n",
       "   'weight_decay': 0,\n",
       "   'amsgrad': False,\n",
       "   'maximize': False,\n",
       "   'foreach': None,\n",
       "   'capturable': False,\n",
       "   'differentiable': False,\n",
       "   'fused': None,\n",
       "   'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(1, 1, 1024)\n",
    "model(t)\n",
    "model.register_buffer(\"forward_params\", torch.tensor([1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(scripted_model, \"exported_model.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.register_buffer(\"forward_params\", torch.tensor([1, 1, 1, 1]))\n",
    "scripted_model = torch.jit.script(model)\n",
    "torch.jit.save(scripted_model, \"exported_model.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.ones_like(z)\n",
    "z += p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(-2, 2, z.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(47).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "# Define the CircularBuffer class as per the user's description\n",
    "class CircularBuffer:\n",
    "    def __init__(self, size, tensor_shape):\n",
    "        self.size = size\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.tensor_shape = tensor_shape\n",
    "\n",
    "    def add(self, tensor):\n",
    "        if tensor.shape == self.tensor_shape:\n",
    "            self.buffer.append(tensor)\n",
    "        else:\n",
    "            raise ValueError(\"Tensor shape does not match the buffer's tensor shape\")\n",
    "\n",
    "    def get(self, index):\n",
    "        if index < 0 or index >= len(self.buffer):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.buffer[index]\n",
    "\n",
    "# Define constants for the audio stream\n",
    "SAMPLE_RATE = 44100\n",
    "BUFFER_SIZE = 2048\n",
    "\n",
    "# Initialize the circular buffer for input tensors\n",
    "input_tensors = CircularBuffer(size=2, tensor_shape=(1, 4, 1))\n",
    "\n",
    "# Load the model\n",
    "model = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "\n",
    "# Audio callback function\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    x = torch.tensor(np.frombuffer(in_data, dtype=np.float32), dtype=torch.float32)\n",
    "    x = x.reshape(1, 1, -1)  # Reshape to (1, 1, 2048)\n",
    "    \n",
    "    z = model.encode(x)\n",
    "    z += input_tensors.get(0)\n",
    "    y = model.decode(z)\n",
    "    \n",
    "    # Convert y to stereo and float32 for PyAudio output\n",
    "    y_np = y.detach().numpy().astype(np.float32).reshape(-1)\n",
    "    out_data = y_np.tobytes()\n",
    "    \n",
    "    return (out_data, pyaudio.paContinue)\n",
    "\n",
    "# OSC callback function\n",
    "def osc_callback(addr, *args):\n",
    "    # Convert the incoming float values to a torch tensor with the shape (1, 4, 1)\n",
    "    tensor = torch.tensor([[args]], dtype=torch.float32)\n",
    "    # Add the tensor to the circular buffer\n",
    "    input_tensors.add(tensor)\n",
    "\n",
    "# Setting up the OSC server\n",
    "def start_osc_server(ip, port):\n",
    "    disp = dispatcher.Dispatcher()\n",
    "    disp.map(\"/latent_perturbations\", osc_callback)\n",
    "    server = osc_server.ThreadingOSCUDPServer((ip, port), disp)\n",
    "    print(f\"Serving on {server.server_address}\")\n",
    "    server.serve_forever()\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                channels=2,\n",
    "                rate=SAMPLE_RATE,\n",
    "                input=True,\n",
    "                output=True,\n",
    "                frames_per_buffer=BUFFER_SIZE,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the OSC server in a new thread\n",
    "osc_thread = Thread(target=start_osc_server, args=('127.0.0.1', 12345))\n",
    "osc_thread.start()\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Keep the main thread alive\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        # You could do some processing here, or just keep the thread alive\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop and close the stream and server\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    osc_thread.join()\n",
    "\n",
    "print(\"Shutting down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -1.9130, -1.8261, -1.7391, -1.6522, -1.5652, -1.4783, -1.3913,\n",
       "        -1.3043, -1.2174, -1.1304, -1.0435, -0.9565, -0.8696, -0.7826, -0.6957,\n",
       "        -0.6087, -0.5217, -0.4348, -0.3478, -0.2609, -0.1739, -0.0870,  0.0000,\n",
       "         0.0870,  0.1739,  0.2609,  0.3478,  0.4348,  0.5217,  0.6087,  0.6957,\n",
       "         0.7826,  0.8696,  0.9565,  1.0435,  1.1304,  1.2174,  1.3043,  1.3913,\n",
       "         1.4783,  1.5652,  1.6522,  1.7391,  1.8261,  1.9130,  2.0000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(-2, 2, z.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([541])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(-2, 2, z.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = \"/Users/nikny/musing_instruments/data/stravinski_wav/01 Petroushka (Original 1911 Version), First Scene_ I. The Shrove-tide Fair.wav\"\n",
    "model_path = \"/Users/nikny/Downloads/percussion.ts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.678006649017334\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "x = li.load(\"/Users/nikny/musing_instruments/data/stravinski_wav/01 Petroushka (Original 1911 Version), First Scene_ I. The Shrove-tide Fair.wav\")[0]\n",
    "x = torch.from_numpy(x).reshape(1, 1, -1)\n",
    "t = time.time()\n",
    "z = model.encode(x)\n",
    "#z[:, 3] += torch.linspace(-2, 2, z.shape[-1])\n",
    "y = model.decode(z).numpy().reshape(-1)\n",
    "print(time.time()-t)\n",
    "sf.write(\"out.wav\", y, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1364128589630127\n"
     ]
    }
   ],
   "source": [
    "wav_path = \"/Users/nikny/musing_instruments/data/stravinski_wav/01 Petroushka (Original 1911 Version), First Scene_ I. The Shrove-tide Fair.wav\"\n",
    "model_path = \"/Users/nikny/Downloads/percussion.ts\"\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')\n",
    "torch.set_grad_enabled(False)\n",
    "model = torch.jit.load(model_path).eval()\n",
    "x = li.load(\"output_path_float32.wav\")[0]\n",
    "x = torch.from_numpy(x).reshape(1, 1, -1)\n",
    "t = time.time()\n",
    "z = model.encode(x)\n",
    "#z[:, 3] += torch.linspace(-2, 2, z.shape[-1])\n",
    "y = model.decode(z).numpy().reshape(-1)\n",
    "print(time.time()-t)\n",
    "sf.write(\"out.wav\", y, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[2] // buffer_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.shape[2] // buffer_length) - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code works\n"
     ]
    }
   ],
   "source": [
    "i = 258\n",
    "z = model.encode(x[:, :, (96000 + i*buffer_length):(96000 + 2*i*buffer_length)])\n",
    "y = model.decode(z).numpy().reshape(-1)\n",
    "print('code works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49152,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96256.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "192512/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa as li \n",
    "import soundfile as sf\n",
    "from typing import Any, Callable, Optional, Union\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "from torch import nn\n",
    "import time\n",
    "import pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y134sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(x[:, :, (\u001b[39m96000\u001b[39m \u001b[39m+\u001b[39m i\u001b[39m*\u001b[39mbuffer_length):(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m96000\u001b[39m \u001b[39m+\u001b[39m i\u001b[39m*\u001b[39mbuffer_length)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y134sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m z[:, \u001b[39m3\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, z\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y134sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(z)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y134sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m last_samples \u001b[39m=\u001b[39m y[\u001b[39m-\u001b[39m\u001b[39m8192\u001b[39m:]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y134sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Write to PyAudio stream\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def convert_wav_to_float32(input_path, output_path):\n",
    "    # Open the wav file\n",
    "    with wave.open(input_path, 'rb') as wav_file:\n",
    "        # Extract audio data and parameters\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        framerate = wav_file.getframerate()\n",
    "        n_frames = wav_file.getnframes()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "\n",
    "        # Convert audio data to numpy array depending on the sample width\n",
    "        if sample_width == 1:  # 8-bit WAV files are unsigned\n",
    "            data = np.frombuffer(audio_data, dtype=np.uint8) - 128\n",
    "        elif sample_width == 2:  # 16-bit WAV files are signed\n",
    "            data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        else:\n",
    "            raise ValueError(\"Only supports 8 or 16 bit audio formats.\")\n",
    "\n",
    "        # Normalize the data to the range between -1.0 and 1.0\n",
    "        max_int_value = float(2 ** (8 * sample_width - 1))\n",
    "        data = data / max_int_value\n",
    "\n",
    "        # Write the data to a new file\n",
    "        sf.write(output_path, data, framerate, 'FLOAT')\n",
    "\n",
    "# Replace 'input_path.wav' and 'output_path.wav' with the actual paths\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2215936,), torch.Size([1, 1, 1107936]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2215872"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1107936*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 541])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 541])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1107936])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1107936])"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 541])"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:, 2] += torch.linspace(-2, 2, z.shape[-1])\n",
    "y = model.decode(z).numpy().reshape(-1)\n",
    "sf.write(\"out.wav\", y, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03487528344671202"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1538/44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(-2, 2, z.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app workflow:\n",
    "# input 2 sec of audio to the model streamed from a .wav file\n",
    "# select last buffer of 8192 samples from the model output and put to audio output stream buffer, then move timewindow of audiofile forward one 8192 s buffer\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding time =  0.0077631473541259766\n",
      "model processing time =  0.05507612228393555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 47])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 1*96000)\n",
    "t = time.time()\n",
    "z = model.encode(x)\n",
    "print('encoding time = ' ,time.time()-t)\n",
    "z[:, 2] += torch.linspace(-2, 2, z.shape[-1])\n",
    "t = time.time()\n",
    "y = model.decode(z)\n",
    "print('model processing time = ', time.time()-t)\n",
    "z.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: BlackHole 64ch (Input channels: 64)\n",
      "Device 1: MacBook Pro Microphone (Input channels: 1)\n",
      "Device 2: MacBook Pro Speakers (Input channels: 0)\n",
      "Device 3: Serato Virtual Audio (Input channels: 2)\n",
      "Device 4: Multi-Output Device (Input channels: 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = pyaudio.PyAudio()\n",
    "def list_audio_devices():\n",
    "    num_devices = p.get_device_count()\n",
    "    for i in range(num_devices):\n",
    "        info = p.get_device_info_by_index(i)\n",
    "        print(f\"Device {i}: {info['name']} (Input channels: {info['maxInputChannels']})\")\n",
    "\n",
    "# List all audio devices\n",
    "list_audio_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def convert_wav_to_float32(input_path, output_path):\n",
    "    # Open the wav file\n",
    "    with wave.open(input_path, 'rb') as wav_file:\n",
    "        # Extract audio data and parameters\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        framerate = wav_file.getframerate()\n",
    "        n_frames = wav_file.getnframes()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "\n",
    "        # Convert audio data to numpy array depending on the sample width\n",
    "        if sample_width == 1:  # 8-bit WAV files are unsigned\n",
    "            data = np.frombuffer(audio_data, dtype=np.uint8) - 128\n",
    "        elif sample_width == 2:  # 16-bit WAV files are signed\n",
    "            data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        else:\n",
    "            raise ValueError(\"Only supports 8 or 16 bit audio formats.\")\n",
    "\n",
    "        # Normalize the data to the range between -1.0 and 1.0\n",
    "        max_int_value = float(2 ** (8 * sample_width - 1))\n",
    "        data = data / max_int_value\n",
    "\n",
    "        # Write the data to a new file\n",
    "        sf.write(output_path, data, framerate, 'FLOAT')\n",
    "\n",
    "# Replace 'input_path.wav' and 'output_path.wav' with the actual paths\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_wav_to_float32(input_path, output_path):\n",
    "    # Open the wav file\n",
    "    with wave.open(input_path, 'rb') as wav_file:\n",
    "        # Extract audio data and parameters\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        framerate = wav_file.getframerate()\n",
    "        n_frames = wav_file.getnframes()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "\n",
    "        # Check the format of the file and convert accordingly\n",
    "        if sample_width == 1:  # 8-bit WAV files are unsigned\n",
    "            data = np.frombuffer(audio_data, dtype=np.uint8) - 128\n",
    "        elif sample_width == 2:  # 16-bit WAV files are signed\n",
    "            data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported audio format.\")\n",
    "\n",
    "        # Normalize the data to the range between -1.0 and 1.0\n",
    "        data = data.astype(np.float32)  # First convert to float32\n",
    "        max_int_value = float(2 ** (8 * sample_width - 1))\n",
    "        data = data / max_int_value\n",
    "\n",
    "        # Reshape the data if it's stereo to 2 columns\n",
    "        if n_channels == 2:\n",
    "            data = np.reshape(data, (-1, n_channels))\n",
    "\n",
    "        # Write the data to a new file with the same sample rate as the original\n",
    "        sf.write(output_path, data, framerate, 'FLOAT')\n",
    "\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maudio_callback_\u001b[39m(in_data, frame_count, time_info, flag):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m         audio_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfrombuffer(in_data, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m         x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(audio_data)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         \u001b[39m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         p\u001b[39m.\u001b[39mterminate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# Replace 'path_to_your_file.wav' with the path to the .wav file you want to play\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m play_wav(\u001b[39m'\u001b[39;49m\u001b[39moutput_path_float32.wav\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m# Wait for the stream to finish\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mwhile\u001b[39;00m stream\u001b[39m.\u001b[39mis_active():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39m# Stop and close the stream\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y115sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m stream\u001b[39m.\u001b[39mstop_stream()\n",
      "\u001b[0;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import time\n",
    "import wave\n",
    "\n",
    "wav_path = \"/Users/nikny/musing_instruments/data/stravinski_wav/01 Petroushka (Original 1911 Version), First Scene_ I. The Shrove-tide Fair.wav\"\n",
    "\n",
    "vae = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "\n",
    "# Constants for the audio properties\n",
    "FORMAT = pyaudio.paFloat32#paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 2048\n",
    "\n",
    "\n",
    "def convert_wav_to_float32(input_path, output_path):\n",
    "    # Open the wav file\n",
    "    with wave.open(input_path, 'rb') as wav_file:\n",
    "        # Extract audio data and parameters\n",
    "        n_channels = wav_file.getnchannels()\n",
    "        sample_width = wav_file.getsampwidth()\n",
    "        framerate = wav_file.getframerate()\n",
    "        n_frames = wav_file.getnframes()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "\n",
    "        # Check the format of the file and convert accordingly\n",
    "        if sample_width == 1:  # 8-bit WAV files are unsigned\n",
    "            data = np.frombuffer(audio_data, dtype=np.uint8) - 128\n",
    "        elif sample_width == 2:  # 16-bit WAV files are signed\n",
    "            data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported audio format.\")\n",
    "\n",
    "        # Normalize the data to the range between -1.0 and 1.0\n",
    "        data = data.astype(np.float32)  # First convert to float32\n",
    "        max_int_value = float(2 ** (8 * sample_width - 1))\n",
    "        data = data / max_int_value\n",
    "\n",
    "        # Reshape the data if it's stereo to 2 columns\n",
    "        if n_channels == 2:\n",
    "            data = np.reshape(data, (-1, n_channels))\n",
    "\n",
    "        # Write the data to a new file with the same sample rate as the original\n",
    "        sf.write(output_path, data, framerate, 'FLOAT')\n",
    "\n",
    "convert_wav_to_float32(wav_path, 'output_path_float32.wav')\n",
    "\n",
    "\n",
    "def audio_callback_(in_data, frame_count, time_info, flag):\n",
    "    with torch.no_grad():\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        x = torch.tensor(audio_data).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        z = vae.encode(x)\n",
    "        y = vae.decode(z)\n",
    "\n",
    "        out_data = y.numpy().astype(np.float32).tobytes()\n",
    "\n",
    "\n",
    "# Constants for the audio properties\n",
    "FORMAT = pyaudio.paFloat32#paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 2048\n",
    "\n",
    "\n",
    "def play_wav(path):\n",
    "    with sf.SoundFile(path) as sf_data:\n",
    "        # Initialize the PyAudio interface\n",
    "        p = pyaudio.PyAudio()\n",
    "\n",
    "        # Open a stream\n",
    "        stream = p.open(format=FORMAT,\n",
    "                        channels=sf_data.channels,\n",
    "                        rate=sf_data.samplerate,\n",
    "                        output=True,\n",
    "                        stream_callback=audio_callback_)\n",
    "\n",
    "        # Start the stream\n",
    "        stream.start_stream()\n",
    "\n",
    "        # Wait for the stream to finish\n",
    "        while stream.is_active():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        # Stop and close the stream\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "\n",
    "        # Terminate the PyAudio object\n",
    "        p.terminate()\n",
    "\n",
    "# Replace 'path_to_your_file.wav' with the path to the .wav file you want to play\n",
    "\n",
    "\n",
    "\n",
    "play_wav('output_path_float32.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import time\n",
    "import wave\n",
    "\n",
    "vae = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "pert = torch.linspace(-2, 2, 1)\n",
    "output = []\n",
    "SAMPLE_RATE = 44100\n",
    "BUFFER_SIZE = 2048\n",
    "p = pyaudio.PyAudio()\n",
    "# Audio callback function\n",
    "def audio_callback(in_data, frame_count, time_info, flag):\n",
    "    # No gradients required for inference\n",
    "    with torch.no_grad():\n",
    "        # Convert the input buffer to a torch tensor\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        x = torch.tensor(audio_data).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        z = vae.encode(x)\n",
    "        #z += pert# input_tensors.get(0)\n",
    "        y = vae.decode(z)\n",
    "        print(y)\n",
    "        # Convert the output to bytes and return it\n",
    "        out_data = y.numpy().astype(np.float32).tobytes()\n",
    "        output.append(out_data)\n",
    "        return (out_data, pyaudio.paContinue)\n",
    "    \n",
    "def audio_callback_(in_data, frame_count, time_info, flag):\n",
    "    with torch.no_grad():\n",
    "        #out.append(0)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        x = torch.tensor(audio_data).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        z = vae.encode(x)\n",
    "        z += pert  # Assuming 'pert' is a tensor with the right shape to be added to 'z'\n",
    "        y = vae.decode(z)\n",
    "        output.append(y.numpy().astype(np.float32))\n",
    "        #print(y.shape)\n",
    "\n",
    "        # Check if y contains valid audio data\n",
    "\n",
    "        # Convert the tensor to stereo if needed\n",
    "        #y_stereo = y.repeat(1, 2, 1)  # Repeat the mono signal across two channels for stereo\n",
    "\n",
    "        # Make sure the audio data is in the right range and type\n",
    "        out_data = y.numpy().astype(np.float32).tobytes()\n",
    "        \n",
    "\n",
    "        return (out_data, pyaudio.paContinue)\n",
    "\n",
    "\n",
    "\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                    channels=1,  # Use 1 channel for input\n",
    "                    #output_channels=2,  # Use 2 channels for output if your device supports it\n",
    "                    rate=SAMPLE_RATE,\n",
    "                    output=True,\n",
    "                    output_device_index=2,\n",
    "                    frames_per_buffer=BUFFER_SIZE,\n",
    "                    stream_callback=audio_callback_)\n",
    "\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Keep the main thread alive\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        # You could do some processing here, or just keep the thread alive\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop and close the stream and server\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down and saving the recording to output.wav\n",
      "Shutting down\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "import time\n",
    "import wave\n",
    "\n",
    "vae = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "pert = torch.linspace(-2, 2, 1)\n",
    "output = []\n",
    "SAMPLE_RATE = 44100\n",
    "BUFFER_SIZE = 2048\n",
    "p = pyaudio.PyAudio()\n",
    "# Audio callback function\n",
    "def audio_callback(in_data, frame_count, time_info, flag):\n",
    "    # No gradients required for inference\n",
    "    with torch.no_grad():\n",
    "        # Convert the input buffer to a torch tensor\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        x = torch.tensor(audio_data).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        z = vae.encode(x)\n",
    "        #z += pert# input_tensors.get(0)\n",
    "        y = vae.decode(z)\n",
    "        print(y)\n",
    "        # Convert the output to bytes and return it\n",
    "        out_data = y.numpy().astype(np.float32).tobytes()\n",
    "        output.append(out_data)\n",
    "        return (out_data, pyaudio.paContinue)\n",
    "    \n",
    "def audio_callback_(in_data, frame_count, time_info, flag):\n",
    "    with torch.no_grad():\n",
    "        #out.append(0)\n",
    "        audio_data = np.frombuffer(in_data, dtype=np.float32)\n",
    "        x = torch.tensor(audio_data).view(1, 1, -1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        z = vae.encode(x)\n",
    "        z += pert  # Assuming 'pert' is a tensor with the right shape to be added to 'z'\n",
    "        y = vae.decode(z)\n",
    "        output.append(y.numpy().astype(np.float32))\n",
    "        #print(y.shape)\n",
    "\n",
    "        # Check if y contains valid audio data\n",
    "\n",
    "        # Convert the tensor to stereo if needed\n",
    "        #y_stereo = y.repeat(1, 2, 1)  # Repeat the mono signal across two channels for stereo\n",
    "\n",
    "        # Make sure the audio data is in the right range and type\n",
    "        out_data = y.numpy().astype(np.float32).tobytes()\n",
    "        \n",
    "\n",
    "        return (out_data, pyaudio.paContinue)\n",
    "\n",
    "\n",
    "\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                    channels=1,  # Use 1 channel for input\n",
    "                    #output_channels=2,  # Use 2 channels for output if your device supports it\n",
    "                    rate=SAMPLE_RATE,\n",
    "                    input=True,\n",
    "                    output=True,\n",
    "                    input_device_index=1,\n",
    "                    output_device_index=2,\n",
    "                    frames_per_buffer=BUFFER_SIZE,\n",
    "                    stream_callback=audio_callback_)\n",
    "\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Keep the main thread alive\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        # You could do some processing here, or just keep the thread alive\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop and close the stream and server\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    # Convert the list of byte strings to a single bytes object\n",
    "    audio_bytes = b''.join(output)\n",
    "\n",
    "    # Define the output WAV file\n",
    "    output_filename = 'output.wav'\n",
    "\n",
    "    # Open the output file\n",
    "    wf = wave.open(output_filename, 'wb')\n",
    "\n",
    "    # Set the number of channels\n",
    "    wf.setnchannels(1)\n",
    "\n",
    "    # Set the sample width to 4 bytes (32 bits)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paFloat32))\n",
    "\n",
    "    # Set the frame rate\n",
    "    wf.setframerate(SAMPLE_RATE)\n",
    "\n",
    "    # Write the frames to the file\n",
    "    wf.writeframes(audio_bytes)\n",
    "\n",
    "    # Close the file\n",
    "    wf.close()\n",
    "\n",
    "    print(f\"Shutting down and saving the recording to {output_filename}\")\n",
    "    \n",
    "\n",
    "print(\"Shutting down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(output)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y106sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mabs(output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][i]) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y106sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(i, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m,output[i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y106sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(output)):\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for i in range(len(output)):\n",
    "    if np.abs(output[0][0][i]) > 1:\n",
    "        print(i, ' ',output[i])\n",
    "\n",
    "for i in range(len(output)):\n",
    "    if np.abs(output[1][0][i]) > 1:\n",
    "        print(i, ' ',output[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -9998] Invalid number of channels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m p \u001b[39m=\u001b[39m pyaudio\u001b[39m.\u001b[39mPyAudio()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Open the audio stream\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m stream \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mopen(\u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mpyaudio\u001b[39m.\u001b[39;49mpaFloat32,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                 channels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                 rate\u001b[39m=\u001b[39;49mSAMPLE_RATE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m                 \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m                 output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m                 frames_per_buffer\u001b[39m=\u001b[39;49mBUFFER_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m                 stream_callback\u001b[39m=\u001b[39;49maudio_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Start the OSC server in a new thread\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#Y102sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m osc_thread \u001b[39m=\u001b[39m Thread(target\u001b[39m=\u001b[39mstart_osc_server, args\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m127.0.0.1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m12345\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/pyaudio/__init__.py:639\u001b[0m, in \u001b[0;36mPyAudio.open\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    632\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Opens a new stream.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \n\u001b[1;32m    634\u001b[0m \u001b[39m    See constructor for :py:func:`PyAudio.Stream.__init__` for parameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39m    :returns: A new :py:class:`PyAudio.Stream`\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     stream \u001b[39m=\u001b[39m PyAudio\u001b[39m.\u001b[39;49mStream(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_streams\u001b[39m.\u001b[39madd(stream)\n\u001b[1;32m    641\u001b[0m     \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/pyaudio/__init__.py:441\u001b[0m, in \u001b[0;36mPyAudio.Stream.__init__\u001b[0;34m(self, PA_manager, rate, channels, format, input, output, input_device_index, output_device_index, frames_per_buffer, start, input_host_api_specific_stream_info, output_host_api_specific_stream_info, stream_callback)\u001b[0m\n\u001b[1;32m    438\u001b[0m     arguments[\u001b[39m'\u001b[39m\u001b[39mstream_callback\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m stream_callback\n\u001b[1;32m    440\u001b[0m \u001b[39m# calling pa.open returns a stream object\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mopen(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marguments)\n\u001b[1;32m    443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_latency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39minputLatency\n\u001b[1;32m    444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_latency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39moutputLatency\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -9998] Invalid number of channels"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pythonosc import dispatcher, osc_server\n",
    "from collections import deque\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "\n",
    "# Define the CircularBuffer class as per the user's description\n",
    "class CircularBuffer:\n",
    "    def __init__(self, size, tensor_shape):\n",
    "        self.size = size\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.tensor_shape = tensor_shape\n",
    "\n",
    "    def add(self, tensor):\n",
    "        if tensor.shape == self.tensor_shape:\n",
    "            self.buffer.append(tensor)\n",
    "        else:\n",
    "            raise ValueError(\"Tensor shape does not match the buffer's tensor shape\")\n",
    "\n",
    "    def get(self, index):\n",
    "        if index < 0 or index >= len(self.buffer):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.buffer[index]\n",
    "\n",
    "# Define constants for the audio stream\n",
    "SAMPLE_RATE = 44100\n",
    "BUFFER_SIZE = 2048\n",
    "\n",
    "# Initialize the circular buffer for input tensors\n",
    "input_tensors = CircularBuffer(size=2, tensor_shape=(1, 4, 1))\n",
    "\n",
    "# Load the model\n",
    "model = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\").eval()\n",
    "\n",
    "# Audio callback function\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    x = torch.tensor(np.frombuffer(in_data, dtype=np.float32), dtype=torch.float32)\n",
    "    x = x.reshape(1, 1, -1)  # Reshape to (1, 1, 2048)\n",
    "    \n",
    "    z = model.encode(x)\n",
    "    z += input_tensors.get(0)\n",
    "    y = model.decode(z)\n",
    "    \n",
    "    # Convert y to stereo and float32 for PyAudio output\n",
    "    y_np = y.detach().numpy().astype(np.float32).reshape(-1)\n",
    "    out_data = y_np.tobytes()\n",
    "    \n",
    "    return (out_data, pyaudio.paContinue)\n",
    "\n",
    "# OSC callback function\n",
    "def osc_callback(addr, *args):\n",
    "    # Convert the incoming float values to a torch tensor with the shape (1, 4, 1)\n",
    "    tensor = torch.tensor([[args]], dtype=torch.float32)\n",
    "    # Add the tensor to the circular buffer\n",
    "    input_tensors.add(tensor)\n",
    "\n",
    "# Setting up the OSC server\n",
    "def start_osc_server(ip, port):\n",
    "    disp = dispatcher.Dispatcher()\n",
    "    disp.map(\"/latent_perturbations\", osc_callback)\n",
    "    server = osc_server.ThreadingOSCUDPServer((ip, port), disp)\n",
    "    print(f\"Serving on {server.server_address}\")\n",
    "    server.serve_forever()\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Open the audio stream\n",
    "stream = p.open(format=pyaudio.paFloat32,\n",
    "                channels=2,\n",
    "                rate=SAMPLE_RATE,\n",
    "                input=True,\n",
    "                output=True,\n",
    "                frames_per_buffer=BUFFER_SIZE,\n",
    "                stream_callback=audio_callback)\n",
    "\n",
    "# Start the OSC server in a new thread\n",
    "osc_thread = Thread(target=start_osc_server, args=('127.0.0.1', 12345))\n",
    "osc_thread.start()\n",
    "\n",
    "# Start the audio stream\n",
    "stream.start_stream()\n",
    "\n",
    "# Keep the main thread alive\n",
    "try:\n",
    "    while stream.is_active():\n",
    "        # You could do some processing here, or just keep the thread alive\n",
    "        time.sleep(0.1)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop and close the stream and server\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    osc_thread.join()\n",
    "\n",
    "print(\"Shutting down\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2048])"
      ]
     },
     "execution_count": 975,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 90, in encode\n    x3 = (resample).from_target_sampling_rate(x, )\n    pqmf = self.pqmf\n    x4 = (pqmf).forward(x3, )\n          ~~~~~~~~~~~~~ <--- HERE\n    encoder = self.encoder\n    mean, scale, = (encoder).forward(x4, )\n  File \"code/__torch__/rave/pqmf.py\", line 15, in forward\n    x: Tensor) -> Tensor:\n    forward_conv = self.forward_conv\n    x0 = (forward_conv).forward(x, )\n          ~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    x1 = __torch__.rave.pqmf.reverse_half(x0, )\n    return x1\n  File \"code/__torch__/cached_conv/convs.py\", line 30, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = torch.conv1d(x1, weight, bias, [16], [0], [1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _0\nclass CachedPadding1d(Module):\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 114, in encode\n    \n        if self.pqmf is not None:\n            x = self.pqmf(x)\n                ~~~~~~~~~ <--- HERE\n    \n        mean, scale = self.encoder(x)\n  File \"/slow-2/antoine/projects/instances/release/rave/pqmf.py\", line 266, in forward\n    def forward(self, x):\n        x = self.forward_conv(x)\n            ~~~~~~~~~~~~~~~~~ <--- HERE\n        x = reverse_half(x)\n        return x\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 121, in forward\n        x = self.downsampling_delay(x)\n        x = self.cache(x)\n        return nn.functional.conv1d(\n               ~~~~~~~~~~~~~~~~~~~~ <--- HERE\n            x,\n            self.weight,\nRuntimeError: Calculated padded input size per channel: (256). Kernel size: (257). Kernel size can't be greater than actual input size\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m d \u001b[39m=\u001b[39m  i\u001b[39m*\u001b[39m\u001b[39m2048\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X66sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, d)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__.py\", line 90, in encode\n    x3 = (resample).from_target_sampling_rate(x, )\n    pqmf = self.pqmf\n    x4 = (pqmf).forward(x3, )\n          ~~~~~~~~~~~~~ <--- HERE\n    encoder = self.encoder\n    mean, scale, = (encoder).forward(x4, )\n  File \"code/__torch__/rave/pqmf.py\", line 15, in forward\n    x: Tensor) -> Tensor:\n    forward_conv = self.forward_conv\n    x0 = (forward_conv).forward(x, )\n          ~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    x1 = __torch__.rave.pqmf.reverse_half(x0, )\n    return x1\n  File \"code/__torch__/cached_conv/convs.py\", line 30, in forward\n    weight = self.weight\n    bias = self.bias\n    _0 = torch.conv1d(x1, weight, bias, [16], [0], [1])\n         ~~~~~~~~~~~~ <--- HERE\n    return _0\nclass CachedPadding1d(Module):\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/slow-2/antoine/projects/instances/release/export_rave.py\", line 114, in encode\n    \n        if self.pqmf is not None:\n            x = self.pqmf(x)\n                ~~~~~~~~~ <--- HERE\n    \n        mean, scale = self.encoder(x)\n  File \"/slow-2/antoine/projects/instances/release/rave/pqmf.py\", line 266, in forward\n    def forward(self, x):\n        x = self.forward_conv(x)\n            ~~~~~~~~~~~~~~~~~ <--- HERE\n        x = reverse_half(x)\n        return x\n  File \"/slow-2/antoine/miniconda3/envs/RAVE/lib/python3.9/site-packages/cached_conv/convs.py\", line 121, in forward\n        x = self.downsampling_delay(x)\n        x = self.cache(x)\n        return nn.functional.conv1d(\n               ~~~~~~~~~~~~~~~~~~~~ <--- HERE\n            x,\n            self.weight,\nRuntimeError: Calculated padded input size per channel: (256). Kernel size: (257). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    d =  i*2048\n",
    "    x = torch.rand(1, 1, d)\n",
    "    z = model.encode(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max nr of samples for 1 code is  2048\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    x = torch.rand(1, 1, i*2048)\n",
    "    z = model.encode(x)\n",
    "    if z.shape[2] ==2:\n",
    "        print('max nr of samples for 1 code is ' ,2000+i -1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(-2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"/Users/nikny/Downloads/percussion.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instrospect the loaded .ts model file\n",
    "\n",
    "# Load the model\n",
    "model_path = \"/Users/nikny/Downloads/percussion.ts\"\n",
    "model = torch.jit.load(model_path)\n",
    "\n",
    "# 1. named_modules\n",
    "print(\"Named Modules:\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n",
    "\n",
    "# 2. named_parameters\n",
    "print(\"\\nNamed Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# 3. named_buffers\n",
    "print(\"\\nNamed Buffers:\")\n",
    "for name, buff in model.named_buffers():\n",
    "    print(name, buff.size())\n",
    "\n",
    "# 4. children\n",
    "print(\"\\nChildren Modules:\")\n",
    "for child in model.children():\n",
    "    print(child)\n",
    "\n",
    "# 5. modules\n",
    "print(\"\\nAll Modules:\")\n",
    "for module in model.modules():\n",
    "    print(module)\n",
    "\n",
    "# 6. parameters\n",
    "print(\"\\nParameters:\")\n",
    "for param in model.parameters():\n",
    "    print(param.size())\n",
    "\n",
    "# 7. buffers\n",
    "print(\"\\nBuffers:\")\n",
    "for buff in model.buffers():\n",
    "    print(buff.size())\n",
    "\n",
    "# 8. state_dict\n",
    "print(\"\\nState Dict:\")\n",
    "for key, value in model.state_dict().items():\n",
    "    print(key, value.size())\n",
    "\n",
    "# 9. code\n",
    "print(\"\\nCode:\")\n",
    "print(model.code)\n",
    "\n",
    "# 10. graph\n",
    "print(\"\\nGraph:\")\n",
    "print(model.graph)\n",
    "\n",
    "# 11. inlined_graph\n",
    "print(\"\\nInlined Graph:\")\n",
    "print(model.inlined_graph)\n",
    "\n",
    "# 12. get_debug_state\n",
    "# This method is more internal and may not be available or may not provide\n",
    "# meaningful output in a textual format.\n",
    "# print(\"\\nDebug State:\")\n",
    "# print(model.get_debug_state())\n",
    "\n",
    "# 13. save (not applicable for printing, as it saves the model to a file)\n",
    "\n",
    "# 14. to\n",
    "# This operation is used to move or cast the parameters and buffers, not for printing.\n",
    "\n",
    "print(\"Model operations have been printed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 541])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:, 0] += torch.linspace(-2, 2, z.shape[-1])\n",
    "y = model.decode(z).numpy().reshape(-1)\n",
    "sf.write(\"out.wav\", y, 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1107936])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model.state_dict().keys():\n",
    "    if model.state_dict()[k].requires_grad:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    if getattr(v, 'requires_grad', False):\n",
    "        print(k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_pca False\n",
      "latent_mean False\n",
      "latent_size False\n",
      "sampling_rate False\n",
      "encode_params False\n",
      "decode_params False\n",
      "forward_params False\n",
      "pqmf.hk False\n",
      "pqmf.h False\n",
      "pqmf.forward_conv.weight False\n",
      "pqmf.forward_conv.cache.pad False\n",
      "pqmf.forward_conv.downsampling_delay.pad False\n",
      "pqmf.inverse_conv.weight False\n",
      "pqmf.inverse_conv.cache.pad False\n",
      "pqmf.inverse_conv.downsampling_delay.pad False\n",
      "encoder.net.0.weight False\n",
      "encoder.net.0.bias False\n",
      "encoder.net.0.cache.pad False\n",
      "encoder.net.0.downsampling_delay.pad False\n",
      "encoder.net.1.weight False\n",
      "encoder.net.1.bias False\n",
      "encoder.net.1.running_mean False\n",
      "encoder.net.1.running_var False\n",
      "encoder.net.1.num_batches_tracked False\n",
      "encoder.net.3.weight False\n",
      "encoder.net.3.bias False\n",
      "encoder.net.3.cache.pad False\n",
      "encoder.net.3.downsampling_delay.pad False\n",
      "encoder.net.4.weight False\n",
      "encoder.net.4.bias False\n",
      "encoder.net.4.running_mean False\n",
      "encoder.net.4.running_var False\n",
      "encoder.net.4.num_batches_tracked False\n",
      "encoder.net.6.weight False\n",
      "encoder.net.6.bias False\n",
      "encoder.net.6.cache.pad False\n",
      "encoder.net.6.downsampling_delay.pad False\n",
      "encoder.net.7.weight False\n",
      "encoder.net.7.bias False\n",
      "encoder.net.7.running_mean False\n",
      "encoder.net.7.running_var False\n",
      "encoder.net.7.num_batches_tracked False\n",
      "encoder.net.9.weight False\n",
      "encoder.net.9.bias False\n",
      "encoder.net.9.cache.pad False\n",
      "encoder.net.9.downsampling_delay.pad False\n",
      "encoder.net.10.weight False\n",
      "encoder.net.10.bias False\n",
      "encoder.net.10.running_mean False\n",
      "encoder.net.10.running_var False\n",
      "encoder.net.10.num_batches_tracked False\n",
      "encoder.net.12.weight False\n",
      "encoder.net.12.bias False\n",
      "encoder.net.12.cache.pad False\n",
      "encoder.net.12.downsampling_delay.pad False\n",
      "encoder.net.14.weight False\n",
      "encoder.net.14.bias False\n",
      "encoder.net.14.cache.pad False\n",
      "encoder.net.14.downsampling_delay.pad False\n",
      "decoder.net.0.bias False\n",
      "decoder.net.0.weight False\n",
      "decoder.net.0.cache.pad False\n",
      "decoder.net.0.downsampling_delay.pad False\n",
      "decoder.net.1.net.1.weight False\n",
      "decoder.net.1.net.1.cache False\n",
      "decoder.net.2.net.0.aligned.branches.0.1.weight False\n",
      "decoder.net.2.net.0.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.2.net.0.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.2.net.0.aligned.branches.0.3.weight False\n",
      "decoder.net.2.net.0.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.2.net.0.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.2.net.0.aligned.paddings.0.pad False\n",
      "decoder.net.2.net.0.aligned.paddings.1.pad False\n",
      "decoder.net.2.net.1.aligned.branches.0.1.weight False\n",
      "decoder.net.2.net.1.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.2.net.1.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.2.net.1.aligned.branches.0.3.weight False\n",
      "decoder.net.2.net.1.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.2.net.1.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.2.net.1.aligned.paddings.0.pad False\n",
      "decoder.net.2.net.1.aligned.paddings.1.pad False\n",
      "decoder.net.2.net.2.aligned.branches.0.1.weight False\n",
      "decoder.net.2.net.2.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.2.net.2.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.2.net.2.aligned.branches.0.3.weight False\n",
      "decoder.net.2.net.2.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.2.net.2.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.2.net.2.aligned.paddings.0.pad False\n",
      "decoder.net.2.net.2.aligned.paddings.1.pad False\n",
      "decoder.net.3.net.1.weight False\n",
      "decoder.net.3.net.1.cache False\n",
      "decoder.net.4.net.0.aligned.branches.0.1.weight False\n",
      "decoder.net.4.net.0.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.4.net.0.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.4.net.0.aligned.branches.0.3.weight False\n",
      "decoder.net.4.net.0.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.4.net.0.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.4.net.0.aligned.paddings.0.pad False\n",
      "decoder.net.4.net.0.aligned.paddings.1.pad False\n",
      "decoder.net.4.net.1.aligned.branches.0.1.weight False\n",
      "decoder.net.4.net.1.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.4.net.1.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.4.net.1.aligned.branches.0.3.weight False\n",
      "decoder.net.4.net.1.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.4.net.1.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.4.net.1.aligned.paddings.0.pad False\n",
      "decoder.net.4.net.1.aligned.paddings.1.pad False\n",
      "decoder.net.4.net.2.aligned.branches.0.1.weight False\n",
      "decoder.net.4.net.2.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.4.net.2.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.4.net.2.aligned.branches.0.3.weight False\n",
      "decoder.net.4.net.2.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.4.net.2.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.4.net.2.aligned.paddings.0.pad False\n",
      "decoder.net.4.net.2.aligned.paddings.1.pad False\n",
      "decoder.net.5.net.1.weight False\n",
      "decoder.net.5.net.1.cache False\n",
      "decoder.net.6.net.0.aligned.branches.0.1.weight False\n",
      "decoder.net.6.net.0.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.6.net.0.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.6.net.0.aligned.branches.0.3.weight False\n",
      "decoder.net.6.net.0.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.6.net.0.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.6.net.0.aligned.paddings.0.pad False\n",
      "decoder.net.6.net.0.aligned.paddings.1.pad False\n",
      "decoder.net.6.net.1.aligned.branches.0.1.weight False\n",
      "decoder.net.6.net.1.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.6.net.1.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.6.net.1.aligned.branches.0.3.weight False\n",
      "decoder.net.6.net.1.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.6.net.1.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.6.net.1.aligned.paddings.0.pad False\n",
      "decoder.net.6.net.1.aligned.paddings.1.pad False\n",
      "decoder.net.6.net.2.aligned.branches.0.1.weight False\n",
      "decoder.net.6.net.2.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.6.net.2.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.6.net.2.aligned.branches.0.3.weight False\n",
      "decoder.net.6.net.2.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.6.net.2.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.6.net.2.aligned.paddings.0.pad False\n",
      "decoder.net.6.net.2.aligned.paddings.1.pad False\n",
      "decoder.net.7.net.1.weight False\n",
      "decoder.net.7.net.1.cache False\n",
      "decoder.net.8.net.0.aligned.branches.0.1.weight False\n",
      "decoder.net.8.net.0.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.8.net.0.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.8.net.0.aligned.branches.0.3.weight False\n",
      "decoder.net.8.net.0.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.8.net.0.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.8.net.0.aligned.paddings.0.pad False\n",
      "decoder.net.8.net.0.aligned.paddings.1.pad False\n",
      "decoder.net.8.net.1.aligned.branches.0.1.weight False\n",
      "decoder.net.8.net.1.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.8.net.1.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.8.net.1.aligned.branches.0.3.weight False\n",
      "decoder.net.8.net.1.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.8.net.1.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.8.net.1.aligned.paddings.0.pad False\n",
      "decoder.net.8.net.1.aligned.paddings.1.pad False\n",
      "decoder.net.8.net.2.aligned.branches.0.1.weight False\n",
      "decoder.net.8.net.2.aligned.branches.0.1.cache.pad False\n",
      "decoder.net.8.net.2.aligned.branches.0.1.downsampling_delay.pad False\n",
      "decoder.net.8.net.2.aligned.branches.0.3.weight False\n",
      "decoder.net.8.net.2.aligned.branches.0.3.cache.pad False\n",
      "decoder.net.8.net.2.aligned.branches.0.3.downsampling_delay.pad False\n",
      "decoder.net.8.net.2.aligned.paddings.0.pad False\n",
      "decoder.net.8.net.2.aligned.paddings.1.pad False\n",
      "decoder.synth.branches.0.bias False\n",
      "decoder.synth.branches.0.weight False\n",
      "decoder.synth.branches.0.cache.pad False\n",
      "decoder.synth.branches.0.downsampling_delay.pad False\n",
      "decoder.synth.branches.1.bias False\n",
      "decoder.synth.branches.1.weight False\n",
      "decoder.synth.branches.1.cache.pad False\n",
      "decoder.synth.branches.1.downsampling_delay.pad False\n",
      "decoder.synth.branches.2.target_size False\n",
      "decoder.synth.branches.2.net.0.weight False\n",
      "decoder.synth.branches.2.net.0.bias False\n",
      "decoder.synth.branches.2.net.0.cache.pad False\n",
      "decoder.synth.branches.2.net.0.downsampling_delay.pad False\n",
      "decoder.synth.branches.2.net.2.weight False\n",
      "decoder.synth.branches.2.net.2.bias False\n",
      "decoder.synth.branches.2.net.2.cache.pad False\n",
      "decoder.synth.branches.2.net.2.downsampling_delay.pad False\n",
      "decoder.synth.branches.2.net.4.weight False\n",
      "decoder.synth.branches.2.net.4.bias False\n",
      "decoder.synth.branches.2.net.4.cache.pad False\n",
      "decoder.synth.branches.2.net.4.downsampling_delay.pad False\n",
      "decoder.synth.paddings.0.pad False\n",
      "decoder.synth.paddings.1.pad False\n",
      "decoder.synth.paddings.2.pad False\n"
     ]
    }
   ],
   "source": [
    "for key, v in model.state_dict().items():\n",
    "    print(key, model.state_dict()[key].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pqmf.forward_conv.weight: requires grad , shape :  torch.Size([16, 1, 257])\n",
      "pqmf.inverse_conv.weight: requires grad , shape :  torch.Size([16, 16, 17])\n",
      "encoder.net.0.weight: requires grad , shape :  torch.Size([64, 16, 7])\n",
      "encoder.net.0.bias: requires grad , shape :  torch.Size([64])\n",
      "encoder.net.1.weight: requires grad , shape :  torch.Size([64])\n",
      "encoder.net.1.bias: requires grad , shape :  torch.Size([64])\n",
      "encoder.net.3.weight: requires grad , shape :  torch.Size([128, 64, 9])\n",
      "encoder.net.3.bias: requires grad , shape :  torch.Size([128])\n",
      "encoder.net.4.weight: requires grad , shape :  torch.Size([128])\n",
      "encoder.net.4.bias: requires grad , shape :  torch.Size([128])\n",
      "encoder.net.6.weight: requires grad , shape :  torch.Size([256, 128, 9])\n",
      "encoder.net.6.bias: requires grad , shape :  torch.Size([256])\n",
      "encoder.net.7.weight: requires grad , shape :  torch.Size([256])\n",
      "encoder.net.7.bias: requires grad , shape :  torch.Size([256])\n",
      "encoder.net.9.weight: requires grad , shape :  torch.Size([512, 256, 9])\n",
      "encoder.net.9.bias: requires grad , shape :  torch.Size([512])\n",
      "encoder.net.10.weight: requires grad , shape :  torch.Size([512])\n",
      "encoder.net.10.bias: requires grad , shape :  torch.Size([512])\n",
      "encoder.net.12.weight: requires grad , shape :  torch.Size([1024, 512, 5])\n",
      "encoder.net.12.bias: requires grad , shape :  torch.Size([1024])\n",
      "encoder.net.14.weight: requires grad , shape :  torch.Size([256, 512, 5])\n",
      "encoder.net.14.bias: requires grad , shape :  torch.Size([256])\n",
      "decoder.net.0.bias: requires grad , shape :  torch.Size([1024])\n",
      "decoder.net.0.weight: requires grad , shape :  torch.Size([1024, 128, 7])\n",
      "decoder.net.1.net.1.weight: requires grad , shape :  torch.Size([1024, 512, 8])\n",
      "decoder.net.2.net.0.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.2.net.0.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.2.net.1.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.2.net.1.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.2.net.2.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.2.net.2.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([512, 512, 3])\n",
      "decoder.net.3.net.1.weight: requires grad , shape :  torch.Size([512, 256, 8])\n",
      "decoder.net.4.net.0.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.4.net.0.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.4.net.1.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.4.net.1.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.4.net.2.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.4.net.2.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([256, 256, 3])\n",
      "decoder.net.5.net.1.weight: requires grad , shape :  torch.Size([256, 128, 8])\n",
      "decoder.net.6.net.0.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.6.net.0.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.6.net.1.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.6.net.1.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.6.net.2.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.6.net.2.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([128, 128, 3])\n",
      "decoder.net.7.net.1.weight: requires grad , shape :  torch.Size([128, 64, 4])\n",
      "decoder.net.8.net.0.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.net.8.net.0.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.net.8.net.1.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.net.8.net.1.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.net.8.net.2.aligned.branches.0.1.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.net.8.net.2.aligned.branches.0.3.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.synth.branches.0.bias: requires grad , shape :  torch.Size([16])\n",
      "decoder.synth.branches.0.weight: requires grad , shape :  torch.Size([16, 64, 7])\n",
      "decoder.synth.branches.1.bias: requires grad , shape :  torch.Size([1])\n",
      "decoder.synth.branches.1.weight: requires grad , shape :  torch.Size([1, 64, 3])\n",
      "decoder.synth.branches.2.net.0.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.synth.branches.2.net.0.bias: requires grad , shape :  torch.Size([64])\n",
      "decoder.synth.branches.2.net.2.weight: requires grad , shape :  torch.Size([64, 64, 3])\n",
      "decoder.synth.branches.2.net.2.bias: requires grad , shape :  torch.Size([64])\n",
      "decoder.synth.branches.2.net.4.weight: requires grad , shape :  torch.Size([80, 64, 3])\n",
      "decoder.synth.branches.2.net.4.bias: requires grad , shape :  torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "grad_params = {name for name, param in model.named_parameters() if param.requires_grad}\n",
    "\n",
    "# Compare with state dict keys\n",
    "for key in model.state_dict().keys():\n",
    "    grad_status = \"requires grad\" if key in grad_params else \"does not require grad\"\n",
    "    if grad_status==\"requires grad\":\n",
    "        print(f\"{key}: {grad_status}\", ', shape : ', model.state_dict()[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17615601"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=TraceModel\n",
       "  (resample): RecursiveScriptModule(\n",
       "    original_name=Resampling\n",
       "    (upsample): RecursiveScriptModule(original_name=Identity)\n",
       "    (downsample): RecursiveScriptModule(original_name=Identity)\n",
       "  )\n",
       "  (pqmf): RecursiveScriptModule(\n",
       "    original_name=CachedPQMF\n",
       "    (forward_conv): RecursiveScriptModule(\n",
       "      original_name=CachedConv1d\n",
       "      (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "    )\n",
       "    (inverse_conv): RecursiveScriptModule(\n",
       "      original_name=CachedConv1d\n",
       "      (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "    )\n",
       "  )\n",
       "  (encoder): RecursiveScriptModule(\n",
       "    original_name=Encoder\n",
       "    (net): RecursiveScriptModule(\n",
       "      original_name=CachedSequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (1): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "      (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "      (3): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (4): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "      (5): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "      (6): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (7): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "      (8): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "      (9): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (10): RecursiveScriptModule(original_name=BatchNorm1d)\n",
       "      (11): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "      (12): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (13): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "      (14): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RecursiveScriptModule(\n",
       "    original_name=Generator\n",
       "    (net): RecursiveScriptModule(\n",
       "      original_name=CachedSequential\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=CachedConv1d\n",
       "        (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "      (1): RecursiveScriptModule(\n",
       "        original_name=UpsampleLayer\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          (1): RecursiveScriptModule(original_name=CachedConvTranspose1d)\n",
       "        )\n",
       "      )\n",
       "      (2): RecursiveScriptModule(\n",
       "        original_name=ResidualStack\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): RecursiveScriptModule(\n",
       "        original_name=UpsampleLayer\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          (1): RecursiveScriptModule(original_name=CachedConvTranspose1d)\n",
       "        )\n",
       "      )\n",
       "      (4): RecursiveScriptModule(\n",
       "        original_name=ResidualStack\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): RecursiveScriptModule(\n",
       "        original_name=UpsampleLayer\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          (1): RecursiveScriptModule(original_name=CachedConvTranspose1d)\n",
       "        )\n",
       "      )\n",
       "      (6): RecursiveScriptModule(\n",
       "        original_name=ResidualStack\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): RecursiveScriptModule(\n",
       "        original_name=UpsampleLayer\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "          (1): RecursiveScriptModule(original_name=CachedConvTranspose1d)\n",
       "        )\n",
       "      )\n",
       "      (8): RecursiveScriptModule(\n",
       "        original_name=ResidualStack\n",
       "        (net): RecursiveScriptModule(\n",
       "          original_name=CachedSequential\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=Residual\n",
       "            (aligned): RecursiveScriptModule(\n",
       "              original_name=AlignBranches\n",
       "              (branches): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(\n",
       "                  original_name=CachedSequential\n",
       "                  (0): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (1): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                  (2): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "                  (3): RecursiveScriptModule(\n",
       "                    original_name=CachedConv1d\n",
       "                    (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                    (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                  )\n",
       "                )\n",
       "                (1): RecursiveScriptModule(original_name=Identity)\n",
       "              )\n",
       "              (paddings): RecursiveScriptModule(\n",
       "                original_name=ModuleList\n",
       "                (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "                (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (synth): RecursiveScriptModule(\n",
       "      original_name=AlignBranches\n",
       "      (branches): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=CachedConv1d\n",
       "          (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "          (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=CachedConv1d\n",
       "          (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "          (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=NoiseGenerator\n",
       "          (net): RecursiveScriptModule(\n",
       "            original_name=CachedSequential\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=CachedConv1d\n",
       "              (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=CachedConv1d\n",
       "              (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(original_name=LeakyReLU)\n",
       "            (4): RecursiveScriptModule(\n",
       "              original_name=CachedConv1d\n",
       "              (cache): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "              (downsampling_delay): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (paddings): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (1): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "        (2): RecursiveScriptModule(original_name=CachedPadding1d)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = torch.rand(1, 1, 4096)\n",
    "model(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "769*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(2, 100, 1, 1, 4096)\n",
    "\n",
    "data[1][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    assert param.requires_grad, \"Some model parameters do not require gradients.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikny/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([1, 2, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m model(data[\u001b[39m0\u001b[39m][i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_function(y, data[\u001b[39m1\u001b[39m][i])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "model.train()\n",
    "data = torch.randn(2, 100, 1, 1, 4096)\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    y = model(data[0][i])\n",
    "    loss = loss_function(y, data[1][i])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0163421630859375\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1538)\n",
    "t = time.time()\n",
    "model(c)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 541])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def ema_inplace(moving_avg, new, decay: float):\n",
    "    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))\n",
    "\n",
    "\n",
    "def laplace_smoothing(x, n_categories: int, epsilon: float = 1e-5):\n",
    "    return (x + epsilon) / (x.sum() + n_categories * epsilon)\n",
    "\n",
    "\n",
    "def uniform_init(*shape: int):\n",
    "    t = torch.empty(shape)\n",
    "    nn.init.kaiming_uniform_(t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def sample_vectors(samples, num: int):\n",
    "    num_samples, device = samples.shape[0], samples.device\n",
    "\n",
    "    if num_samples >= num:\n",
    "        indices = torch.randperm(num_samples, device=device)[:num]\n",
    "    else:\n",
    "        indices = torch.randint(0, num_samples, (num, ), device=device)\n",
    "\n",
    "    return samples[indices]\n",
    "\n",
    "\n",
    "def kmeans(samples, num_clusters: int, num_iters: int = 10):\n",
    "    dim, dtype = samples.shape[-1], samples.dtype\n",
    "\n",
    "    means = sample_vectors(samples, num_clusters)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        diffs = samples[:, None] - means[None]\n",
    "        dists = -(diffs**2).sum(dim=-1)\n",
    "\n",
    "        buckets = dists.max(dim=-1).indices\n",
    "        bins = torch.bincount(buckets, minlength=num_clusters)\n",
    "        zero_mask = bins == 0\n",
    "        bins_min_clamped = bins.masked_fill(zero_mask, 1)\n",
    "\n",
    "        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)\n",
    "        new_means.scatter_add_(0, repeat(buckets, \"n -> n d\", d=dim), samples)\n",
    "        new_means = new_means / bins_min_clamped[..., None]\n",
    "\n",
    "        means = torch.where(zero_mask[..., None], means, new_means)\n",
    "\n",
    "    return means, bins\n",
    "\n",
    "\n",
    "class EuclideanCodebook(nn.Module):\n",
    "    \"\"\"Codebook with Euclidean distance.\n",
    "    Args:\n",
    "        dim (int): Dimension.\n",
    "        codebook_size (int): Codebook size.\n",
    "        kmeans_init (bool): Whether to use k-means to initialize the codebooks.\n",
    "            If set to true, run the k-means algorithm on the first training batch and use\n",
    "            the learned centroids as initialization.\n",
    "        kmeans_iters (int): Number of iterations used for k-means algorithm at initialization.\n",
    "        decay (float): Decay for exponential moving average over the codebooks.\n",
    "        epsilon (float): Epsilon value for numerical stability.\n",
    "        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes\n",
    "            that have an exponential moving average cluster size less than the specified threshold with\n",
    "            randomly selected vector from the current batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        codebook_size: int,\n",
    "        kmeans_init: int = False,\n",
    "        kmeans_iters: int = 10,\n",
    "        decay: float = 0.99,\n",
    "        epsilon: float = 1e-5,\n",
    "        threshold_ema_dead_code: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        init_fn: Union[Callable[..., torch.Tensor],\n",
    "                       Any] = uniform_init if not kmeans_init else torch.zeros\n",
    "        embed = init_fn(codebook_size, dim)\n",
    "\n",
    "        self.codebook_size = codebook_size\n",
    "\n",
    "        self.kmeans_iters = kmeans_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.threshold_ema_dead_code = threshold_ema_dead_code\n",
    "\n",
    "        self.register_buffer(\"inited\", torch.Tensor([not kmeans_init]))\n",
    "        self.register_buffer(\"cluster_size\", torch.zeros(codebook_size))\n",
    "        self.register_buffer(\"embed\", embed)\n",
    "        self.register_buffer(\"embed_avg\", embed.clone())\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def init_embed_(self, data):\n",
    "        embed, cluster_size = kmeans(data, self.codebook_size,\n",
    "                                     self.kmeans_iters)\n",
    "        self.embed.data.copy_(embed)\n",
    "        self.embed_avg.data.copy_(embed.clone())\n",
    "        self.cluster_size.data.copy_(cluster_size)\n",
    "        self.inited.data.copy_(torch.Tensor([True]))\n",
    "\n",
    "    def replace_(self, samples, mask):\n",
    "        modified_codebook = torch.where(\n",
    "            mask[..., None], sample_vectors(samples, self.codebook_size),\n",
    "            self.embed)\n",
    "        self.embed.data.copy_(modified_codebook)\n",
    "\n",
    "    def expire_codes_(self, batch_samples):\n",
    "        if self.threshold_ema_dead_code == 0:\n",
    "            return\n",
    "\n",
    "        expired_codes = self.cluster_size < self.threshold_ema_dead_code\n",
    "        if not torch.any(expired_codes):\n",
    "            return\n",
    "\n",
    "        batch_samples = batch_samples.reshape(-1, batch_samples.shape[-1])\n",
    "        self.replace_(batch_samples, mask=expired_codes)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        return x.reshape(-1, x.shape[-1])\n",
    "\n",
    "    def quantize(self, x):\n",
    "        embed = self.embed.t()\n",
    "        dist = -(x.pow(2).sum(1, keepdim=True) - 2 * x @ embed +\n",
    "                 embed.pow(2).sum(0, keepdim=True))\n",
    "        embed_ind = dist.max(dim=-1).indices\n",
    "        return embed_ind\n",
    "\n",
    "    def dequantize(self, embed_ind):\n",
    "        quantize = F.embedding(embed_ind, self.embed)\n",
    "        return quantize\n",
    "\n",
    "    def encode(self, x):\n",
    "        shape = x.shape\n",
    "        # pre-process\n",
    "        x = self.preprocess(x)\n",
    "        # quantize\n",
    "        embed_ind = self.quantize(x)\n",
    "        # post-process\n",
    "        embed_ind = embed_ind.reshape(shape[0], shape[1])\n",
    "        return embed_ind\n",
    "\n",
    "    def decode(self, embed_ind):\n",
    "        quantize = self.dequantize(embed_ind)\n",
    "        return quantize\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape, dtype = x.shape, x.dtype\n",
    "        x = self.preprocess(x)\n",
    "\n",
    "        if not self.inited:\n",
    "            self.init_embed_(x)\n",
    "\n",
    "        embed_ind = self.quantize(x)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n",
    "        embed_ind = embed_ind.reshape(shape[0], shape[1])\n",
    "        quantize = self.dequantize(embed_ind)\n",
    "\n",
    "        if self.training:\n",
    "            # We do the expiry of code at that point as buffers are in sync\n",
    "            # and all the workers will take the same decision.\n",
    "            self.expire_codes_(x)\n",
    "            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n",
    "            embed_sum = x.t() @ embed_onehot\n",
    "            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)\n",
    "            cluster_size = (laplace_smoothing(\n",
    "                self.cluster_size, self.codebook_size, self.epsilon) *\n",
    "                            self.cluster_size.sum())\n",
    "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "\n",
    "        return quantize, embed_ind\n",
    "\n",
    "\n",
    "class VectorQuantization(nn.Module):\n",
    "    \"\"\"Vector quantization implementation.\n",
    "    Currently supports only euclidean distance.\n",
    "    Args:\n",
    "        dim (int): Dimension\n",
    "        codebook_size (int): Codebook size\n",
    "        codebook_dim (int): Codebook dimension. If not defined, uses the specified dimension in dim.\n",
    "        decay (float): Decay for exponential moving average over the codebooks.\n",
    "        epsilon (float): Epsilon value for numerical stability.\n",
    "        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.\n",
    "        kmeans_iters (int): Number of iterations used for kmeans initialization.\n",
    "        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes\n",
    "            that have an exponential moving average cluster size less than the specified threshold with\n",
    "            randomly selected vector from the current batch.\n",
    "        commitment_weight (float): Weight for commitment loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        codebook_size: int,\n",
    "        codebook_dim: Optional[int] = None,\n",
    "        decay: float = 0.99,\n",
    "        epsilon: float = 1e-5,\n",
    "        kmeans_init: bool = True,\n",
    "        kmeans_iters: int = 50,\n",
    "        threshold_ema_dead_code: int = 2,\n",
    "        commitment_weight: float = 1.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _codebook_dim: int = codebook_dim or dim\n",
    "\n",
    "        requires_projection = _codebook_dim != dim\n",
    "        self.project_in = (nn.Linear(dim, _codebook_dim)\n",
    "                           if requires_projection else nn.Identity())\n",
    "        self.project_out = (nn.Linear(_codebook_dim, dim)\n",
    "                            if requires_projection else nn.Identity())\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.commitment_weight = commitment_weight\n",
    "\n",
    "        self._codebook = EuclideanCodebook(\n",
    "            dim=_codebook_dim,\n",
    "            codebook_size=codebook_size,\n",
    "            kmeans_init=kmeans_init,\n",
    "            kmeans_iters=kmeans_iters,\n",
    "            decay=decay,\n",
    "            epsilon=epsilon,\n",
    "            threshold_ema_dead_code=threshold_ema_dead_code)\n",
    "        self.codebook_size = codebook_size\n",
    "\n",
    "    @property\n",
    "    def codebook(self):\n",
    "        return self._codebook.embed\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.project_in(x)\n",
    "        embed_in = self._codebook.encode(x)\n",
    "        return embed_in\n",
    "\n",
    "    def decode(self, embed_ind):\n",
    "        quantize = self._codebook.decode(embed_ind)\n",
    "        quantize = self.project_out(quantize)\n",
    "        quantize = quantize.permute(0, 2, 1)\n",
    "        return quantize\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.project_in(x)\n",
    "\n",
    "        quantize, embed_ind = self._codebook(x)\n",
    "\n",
    "        if self.training:\n",
    "            quantize = x + (quantize - x).detach()\n",
    "\n",
    "        loss = torch.tensor([0.0], device=device, requires_grad=self.training)\n",
    "\n",
    "        if self.training:\n",
    "            if self.commitment_weight > 0:\n",
    "                commit_loss = F.mse_loss(quantize.detach(), x)\n",
    "                loss = loss + commit_loss * self.commitment_weight\n",
    "\n",
    "        quantize = self.project_out(quantize)\n",
    "        quantize = quantize.permute(0, 2, 1)\n",
    "        return quantize, embed_ind, loss\n",
    "\n",
    "\n",
    "class ResidualVectorQuantization(nn.Module):\n",
    "    \"\"\"Residual vector quantization implementation.\n",
    "    Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_quantizers, **kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [VectorQuantization(**kwargs) for _ in range(num_quantizers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantized_out = 0.0\n",
    "        residual = x\n",
    "\n",
    "        all_losses = []\n",
    "        all_indices = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            quantized, indices, loss = layer(residual)\n",
    "            residual = residual - quantized\n",
    "            quantized_out = quantized_out + quantized\n",
    "\n",
    "            all_indices.append(indices)\n",
    "            all_losses.append(loss)\n",
    "\n",
    "        out_losses = torch.stack(all_losses, 0).sum()\n",
    "        all_indices = torch.stack(all_indices, 1)\n",
    "        return quantized_out, out_losses, all_indices\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        all_indices = []\n",
    "        for layer in self.layers:\n",
    "            indices = layer.encode(residual)\n",
    "            quantized = layer.decode(indices)\n",
    "            residual = residual - quantized\n",
    "            all_indices.append(indices)\n",
    "        out_indices = torch.stack(all_indices, 1)\n",
    "        return out_indices\n",
    "\n",
    "    def decode(self, q_indices: torch.Tensor) -> torch.Tensor:\n",
    "        quantized_out = torch.tensor(0.0, device=q_indices.device)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            quantized = layer.decode(q_indices[:, i])\n",
    "            quantized_out = quantized_out + quantized\n",
    "        return quantized_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nn = nn.Linear(1024, 1024)\n",
    "        self.vr = VectorQuantization(1024, 8)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.nn(x).reshape(1, -1, 1)\n",
    "        z, _, loss = self.vr(x)\n",
    "        return z, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(data[1][45])[0].reshape(1, 1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = testnet()\n",
    "t.train()\n",
    "x = torch.rand(1,1, 1024)\n",
    "t(x)\n",
    "\n",
    "optimizer = Adam(t.parameters(), lr = 0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "data = torch.rand(2, 10000, 1, 1, 1024)\n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    y, l = t(data[0][i])\n",
    "    loss = criterion(y, data[1][i].reshape(1, 1, -1)) - l\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016429424285888672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0]]), tensor([268475.9375], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = torch.rand(1, 1024, 1)\n",
    "t = time.time()\n",
    "z = vr(x)\n",
    "print(time.time()-t)\n",
    "z[1], z[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m EuclideanCodebook(dim, codebook_size, kmeans_init\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Initialize the optimizer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Loss function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_decay value: \u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39mlr, betas\u001b[39m=\u001b[39mbetas, eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[39m=\u001b[39mweight_decay, amsgrad\u001b[39m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[39m=\u001b[39mmaximize, foreach\u001b[39m=\u001b[39mforeach, capturable\u001b[39m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[39m=\u001b[39mdifferentiable, fused\u001b[39m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:261\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    259\u001b[0m param_groups \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(params)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(param_groups) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39moptimizer got an empty parameter list\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_groups[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m):\n\u001b[1;32m    263\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming `vectors` is your dataset as a 2D tensor of shape (num_samples, dim)\n",
    "vectors = torch.randn(1000, 64)  # Example dataset with 1000 samples of dimension 64\n",
    "dataset = TensorDataset(vectors)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the EuclideanCodebook model\n",
    "codebook_size = 256  # for example\n",
    "dim = vectors.shape[1]\n",
    "model = EuclideanCodebook(dim, codebook_size, kmeans_init=True)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0014])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr(x)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m optim\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vr\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1024\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vr' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "\n",
    "vr.train()\n",
    "data = torch.randn(2, 100, 1, 1024, 1)\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    optimizer.zero_grad()\n",
    "    y = vr(data[0][i])[0]\n",
    "    loss = loss_function(y, data[1][i])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5164],\n",
       "         [0.1769],\n",
       "         [0.8250],\n",
       "         ...,\n",
       "         [0.7038],\n",
       "         [0.9020],\n",
       "         [0.7556]]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = EuclideanCodebook(1024, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in vr.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param, \"require gradients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)\n",
      "tensor([[[ 0.0000e+00, -3.6213e-05,  4.4914e-04,  ...,  4.4914e-04,\n",
      "           3.6213e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -3.6213e-05,  4.4914e-04,  ...,  4.4914e-04,\n",
      "           3.6213e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -3.6213e-05,  4.4914e-04,  ...,  4.4915e-04,\n",
      "           3.6213e-05,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00, -3.6213e-05,  4.4915e-04,  ...,  4.4913e-04,\n",
      "           3.6214e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -3.6213e-05,  4.4915e-04,  ...,  4.4914e-04,\n",
      "           3.6214e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -3.6214e-05,  4.4915e-04,  ...,  4.4914e-04,\n",
      "           3.6213e-05,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00, -4.9724e-05,  4.9400e-04,  ...,  4.8014e-04,\n",
      "           1.7656e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -6.9125e-05,  3.0125e-04,  ...,  2.9280e-04,\n",
      "           2.4545e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00, -2.2753e-05,  6.1155e-04,  ...,  5.9439e-04,\n",
      "           8.0788e-06,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  2.2752e-05, -6.1155e-04,  ..., -5.9438e-04,\n",
      "          -8.0796e-06,  0.0000e+00],\n",
      "         [ 0.0000e+00,  6.9126e-05, -3.0125e-04,  ..., -2.9279e-04,\n",
      "          -2.4545e-05,  0.0000e+00],\n",
      "         [ 0.0000e+00,  4.9723e-05, -4.9400e-04,  ..., -4.8014e-04,\n",
      "          -1.7656e-05,  0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00, -6.0714e-05,  5.2495e-04,  ...,  4.9751e-04,\n",
      "           4.5503e-06,  0.0000e+00],\n",
      "         [ 0.0000e+00, -1.0718e-04,  1.2317e-04,  ...,  1.1673e-04,\n",
      "           8.0329e-06,  0.0000e+00],\n",
      "         [ 0.0000e+00,  2.1320e-05,  6.1922e-04,  ...,  5.8685e-04,\n",
      "          -1.5979e-06,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  2.1320e-05,  6.1922e-04,  ...,  5.8686e-04,\n",
      "          -1.5978e-06,  0.0000e+00],\n",
      "         [ 0.0000e+00, -1.0718e-04,  1.2317e-04,  ...,  1.1673e-04,\n",
      "           8.0329e-06,  0.0000e+00],\n",
      "         [ 0.0000e+00, -6.0714e-05,  5.2495e-04,  ...,  4.9751e-04,\n",
      "           4.5503e-06,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.9056e-06,  2.6789e-04, -4.3071e-04,  ...,  1.2680e-04,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-7.6751e-07,  5.6556e-04,  4.7869e-05,  ..., -1.4092e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.0528e-06, -3.6052e-04, -3.7752e-04,  ...,  1.1114e-04,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-6.0528e-06,  3.6052e-04,  3.7752e-04,  ..., -1.1114e-04,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 7.6761e-07, -5.6556e-04, -4.7869e-05,  ...,  1.4091e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-6.9055e-06, -2.6789e-04,  4.3071e-04,  ..., -1.2680e-04,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-6.8100e-06,  3.3243e-04, -5.7137e-04,  ...,  9.0865e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.5978e-06,  5.8686e-04, -1.3406e-04,  ...,  2.1320e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-8.0329e-06, -1.1673e-04, -6.7397e-04,  ...,  1.0718e-04,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-8.0329e-06, -1.1673e-04, -6.7397e-04,  ...,  1.0718e-04,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.5979e-06,  5.8686e-04, -1.3406e-04,  ...,  2.1320e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-6.8100e-06,  3.3242e-04, -5.7137e-04,  ...,  9.0866e-05,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-2.1514e-05,  3.9404e-04, -6.9393e-04,  ...,  6.0589e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.3119e-05,  5.4779e-04, -4.2317e-04,  ...,  3.6948e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-2.6633e-05,  1.8031e-04, -8.5904e-04,  ...,  7.5005e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 2.6632e-05, -1.8032e-04,  8.5904e-04,  ..., -7.5005e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.3119e-05, -5.4779e-04,  4.2317e-04,  ..., -3.6948e-05,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.1514e-05, -3.9405e-04,  6.9393e-04,  ..., -6.0589e-05,\n",
      "           0.0000e+00,  0.0000e+00]]], requires_grad=True)\n",
      "tensor([[[ 1.7458e-04, -5.0454e-04,  4.9619e-04,  ...,  3.6459e-04,\n",
      "           1.7831e-04,  6.5846e-05],\n",
      "         [-4.4261e-04,  5.3122e-03, -3.3604e-03,  ..., -8.8087e-04,\n",
      "           3.9683e-03, -1.5394e-03],\n",
      "         [-3.9842e-03, -9.8709e-03, -5.7770e-03,  ...,  1.1759e-02,\n",
      "          -2.1371e-03,  6.7926e-03],\n",
      "         ...,\n",
      "         [-1.3761e-01, -9.3422e-02, -5.3809e-03,  ...,  1.5217e-02,\n",
      "          -1.3724e-01, -1.9232e-01],\n",
      "         [ 2.3592e-02,  3.1939e-03, -4.2650e-02,  ...,  1.9759e-01,\n",
      "          -2.9595e-01,  3.5203e-01],\n",
      "         [-3.3314e-01,  7.5276e-03, -2.6897e-01,  ...,  7.0685e-02,\n",
      "          -1.7229e-01, -1.1982e+00]],\n",
      "\n",
      "        [[ 5.4060e-04,  3.1526e-05,  5.0456e-04,  ..., -2.6387e-04,\n",
      "          -3.0909e-04,  2.7742e-04],\n",
      "         [-4.3421e-04, -4.6991e-04, -7.4579e-04,  ..., -1.3218e-03,\n",
      "          -7.5554e-04, -6.6455e-04],\n",
      "         [-1.1615e-03,  1.9757e-04, -7.3005e-04,  ..., -7.1403e-04,\n",
      "           8.7890e-04, -8.9772e-04],\n",
      "         ...,\n",
      "         [ 5.9461e-01,  2.3549e+00, -2.8265e+00,  ...,  1.5686e+00,\n",
      "          -2.3124e+00, -2.3792e+00],\n",
      "         [-1.1767e-01,  9.0571e-01,  5.4363e-01,  ..., -1.3491e+00,\n",
      "          -2.7581e-02,  1.0177e+00],\n",
      "         [ 1.3467e-01, -4.1338e-01, -1.2408e-02,  ...,  2.9649e-01,\n",
      "           3.6225e-02, -1.8997e-01]],\n",
      "\n",
      "        [[-1.2017e-02, -3.4174e-02, -8.4837e-03,  ..., -3.5311e-02,\n",
      "           1.1796e-01, -2.0193e-01],\n",
      "         [ 9.5614e-02,  9.4058e-02,  5.6242e-02,  ..., -1.5342e-01,\n",
      "          -9.8073e-02, -2.7927e-02],\n",
      "         [-9.4016e-02,  4.8404e-02,  5.2390e-03,  ...,  1.0889e-01,\n",
      "          -4.6773e-02,  9.7735e-02],\n",
      "         ...,\n",
      "         [ 7.9861e-02,  1.2083e-01,  9.2195e-02,  ..., -5.2627e-03,\n",
      "          -4.4936e-02, -3.9470e-02],\n",
      "         [-1.8147e-03, -3.8655e-02,  5.4576e-02,  ...,  9.5706e-02,\n",
      "          -4.2498e-02,  1.2767e-02],\n",
      "         [-8.3338e-03, -1.5024e-01, -1.3659e-01,  ..., -1.6930e-01,\n",
      "           3.7075e-01,  1.5888e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6273e-01, -6.8874e-03, -7.6282e-02,  ...,  2.7064e-02,\n",
      "           1.2167e-01,  1.1270e-01],\n",
      "         [ 2.1740e-02,  2.3227e-02,  1.0966e-02,  ..., -2.4540e-02,\n",
      "           1.5505e-02, -1.7553e-02],\n",
      "         [-3.7110e-02, -6.8541e-02, -6.2989e-02,  ...,  3.5046e-02,\n",
      "           4.4409e-02,  7.5639e-02],\n",
      "         ...,\n",
      "         [-5.8379e-02, -3.5963e-02, -2.4893e-02,  ..., -6.1902e-02,\n",
      "          -5.2319e-02, -1.6798e-02],\n",
      "         [ 3.7892e-02,  4.1270e-02, -6.7023e-02,  ...,  1.4870e-01,\n",
      "          -2.5710e-01,  2.3329e-01],\n",
      "         [ 1.2672e-01,  1.5178e-01, -1.0471e-01,  ..., -4.0429e-02,\n",
      "          -1.0798e-01, -1.1233e+00]],\n",
      "\n",
      "        [[-1.4644e-02, -3.9193e-03, -5.4401e-02,  ..., -1.8267e-01,\n",
      "          -1.8135e-01, -2.4838e-01],\n",
      "         [ 8.8941e-02, -7.2388e-02,  8.1412e-02,  ...,  5.6216e-02,\n",
      "          -1.1616e-01,  2.6800e-02],\n",
      "         [ 9.1694e-03,  4.3679e-02,  7.4421e-02,  ..., -5.6735e-02,\n",
      "          -2.5673e-02, -9.8388e-02],\n",
      "         ...,\n",
      "         [ 8.7496e-02,  8.0799e-02, -1.9518e-02,  ..., -1.2462e-01,\n",
      "          -1.3472e-01, -6.5530e-02],\n",
      "         [ 2.7341e-02,  9.0621e-02,  4.6843e-02,  ..., -8.6042e-02,\n",
      "          -8.0948e-02, -3.0112e-01],\n",
      "         [ 1.8792e-01,  9.4120e-02,  1.9173e-01,  ..., -1.1912e-01,\n",
      "          -2.5697e-01,  5.5302e-01]],\n",
      "\n",
      "        [[-5.7589e-05,  3.2115e-04, -6.0943e-05,  ...,  8.8569e-05,\n",
      "          -3.3492e-04,  5.4006e-04],\n",
      "         [-2.4052e-03,  8.7224e-04,  6.9885e-04,  ...,  8.3542e-04,\n",
      "          -1.0450e-03,  7.7143e-04],\n",
      "         [-5.7559e-03, -3.8274e-03, -3.7810e-03,  ...,  5.2513e-04,\n",
      "          -3.8787e-03,  4.8090e-03],\n",
      "         ...,\n",
      "         [-5.4773e-02, -3.6136e-03,  4.6270e-02,  ..., -3.8349e-02,\n",
      "           7.2875e-02,  1.3599e-02],\n",
      "         [-4.8722e-02, -4.7176e-02,  4.2638e-02,  ..., -1.9434e-02,\n",
      "           7.8689e-02, -6.4934e-02],\n",
      "         [ 1.1498e-01, -1.3825e-01,  1.8888e-01,  ..., -4.3766e-01,\n",
      "           3.9242e-01,  5.8907e-01]]], requires_grad=True)\n",
      "tensor([ 0.0485,  0.0623,  0.0610, -0.0064,  0.1264, -0.0815, -0.0996, -0.1164,\n",
      "        -0.0243,  0.1571,  0.0578,  0.0658,  0.0444,  0.1025,  0.0412, -0.0782,\n",
      "        -0.1287, -0.0680,  0.0282, -0.0925, -0.0258, -0.0962,  0.1037,  0.2012,\n",
      "        -0.1128, -0.0050, -0.0612,  0.0057, -0.0363,  0.0194, -0.1489,  0.0289,\n",
      "         0.0745,  0.1811, -0.0097,  0.0385,  0.1685,  0.1131,  0.1466,  0.0373,\n",
      "         0.0309, -0.0458, -0.0241, -0.0277, -0.0258, -0.0368, -0.1445, -0.0390,\n",
      "         0.0149, -0.0764,  0.1282,  0.0973,  0.0053,  0.0084,  0.1776, -0.0338,\n",
      "        -0.0951,  0.0426,  0.0424, -0.1051, -0.0119,  0.0816, -0.1288, -0.0627],\n",
      "       requires_grad=True)\n",
      "tensor([1.1847, 1.3158, 1.0096, 1.4364, 1.0062, 0.8917, 0.7506, 0.6897, 1.1664,\n",
      "        0.8105, 0.9611, 0.9251, 0.8283, 0.9382, 0.7807, 0.8273, 0.7053, 1.1576,\n",
      "        0.7573, 0.7793, 0.9441, 1.0960, 1.0445, 1.0555, 0.6855, 0.8907, 0.9147,\n",
      "        1.0485, 0.8069, 0.8320, 1.5387, 0.6255, 0.8146, 0.8089, 1.0136, 0.7208,\n",
      "        1.1552, 0.7751, 1.0704, 0.8049, 0.7157, 0.7953, 0.9518, 1.0531, 0.8700,\n",
      "        0.6931, 0.7749, 1.0436, 0.7249, 0.9786, 1.1415, 1.0982, 1.1328, 0.7745,\n",
      "        0.9396, 0.6358, 1.0441, 1.0460, 0.8907, 1.0936, 0.7956, 0.9706, 0.7018,\n",
      "        1.2741], requires_grad=True)\n",
      "tensor([-3.6101e-04, -1.9765e-03, -2.7834e-04, -9.8722e-04,  2.5691e-04,\n",
      "         6.9376e-02, -2.1165e-05,  6.4073e-04,  3.3646e-04,  4.0670e-02,\n",
      "         1.3613e-03, -4.0006e-04,  1.5662e-01,  3.5090e-02,  2.2684e-02,\n",
      "         1.2123e-01,  6.5821e-02,  1.4974e-04,  3.9060e-02,  8.9594e-03,\n",
      "        -3.9966e-04, -3.3173e-04,  1.5136e-04,  2.2015e-04,  4.2988e-02,\n",
      "         1.5550e-02,  1.9542e-02,  1.6335e-03,  1.1862e-02,  4.0587e-01,\n",
      "        -1.7014e-03,  2.1369e-02,  2.5002e-01,  7.7472e-02, -6.3356e-04,\n",
      "         2.5846e-02, -9.5374e-04,  1.7867e-01, -4.9384e-04,  2.2190e-02,\n",
      "         2.6554e-02,  2.8569e-02,  2.2141e-02,  8.4914e-05,  1.9673e-02,\n",
      "         2.5657e-02,  1.9834e-02,  5.1441e-04,  9.2227e-03, -2.0626e-04,\n",
      "         2.9571e-04, -9.2497e-04,  3.0827e-01,  5.1884e-04, -7.2564e-05,\n",
      "         5.3596e-02,  8.3564e-02, -2.0212e-03,  1.4520e-02, -8.9705e-04,\n",
      "         1.7753e-02, -7.4557e-04, -1.1913e-02, -2.3278e-05],\n",
      "       requires_grad=True)\n",
      "tensor([[[-8.9434e-03, -5.6319e-02, -4.9962e-02,  ..., -4.9992e-02,\n",
      "          -3.6841e-02, -5.4392e-02],\n",
      "         [ 4.0953e-02,  5.4035e-02,  8.3495e-02,  ...,  8.3727e-02,\n",
      "           1.5690e-01,  5.8990e-02],\n",
      "         [-6.6905e-02, -9.6982e-02, -2.4681e-02,  ...,  1.4574e-02,\n",
      "          -3.0513e-02, -1.2287e-01],\n",
      "         ...,\n",
      "         [-2.4245e-04, -8.8969e-03,  1.9684e-02,  ..., -3.5346e-02,\n",
      "          -1.1122e-01, -1.1674e-01],\n",
      "         [-3.4150e-02, -1.5861e-01, -1.8140e-01,  ..., -1.2976e-01,\n",
      "          -1.6279e-01, -2.5505e-01],\n",
      "         [-3.6766e-02, -5.7177e-02, -4.5445e-02,  ..., -3.2779e-02,\n",
      "          -2.0965e-02, -2.3760e-02]],\n",
      "\n",
      "        [[-7.7262e-02, -3.0273e-02, -9.3640e-02,  ...,  3.0465e-02,\n",
      "           3.4983e-02,  1.8652e-01],\n",
      "         [-1.4556e-01, -1.3512e-01, -1.7359e-01,  ...,  5.0757e-02,\n",
      "           1.0748e-01,  1.0678e-01],\n",
      "         [ 8.5329e-03,  3.5196e-02,  1.6669e-02,  ..., -4.7469e-02,\n",
      "          -1.0361e-01,  9.4534e-02],\n",
      "         ...,\n",
      "         [-4.3393e-02,  1.8869e-03, -8.9882e-03,  ...,  3.0941e-02,\n",
      "           1.3130e-02,  7.9101e-02],\n",
      "         [ 1.4583e-02,  9.0714e-02,  5.4178e-02,  ...,  3.9519e-02,\n",
      "           5.1189e-02,  5.1480e-02],\n",
      "         [-1.0110e-01, -3.2568e-02, -5.8598e-02,  ...,  1.2246e-02,\n",
      "           8.4396e-02,  2.2442e-01]],\n",
      "\n",
      "        [[ 4.7544e-02, -7.2710e-02, -5.4023e-03,  ..., -1.2069e-01,\n",
      "          -2.1561e-01, -3.3645e-01],\n",
      "         [-5.7787e-02, -1.3805e-01, -1.2408e-01,  ..., -7.9597e-02,\n",
      "          -9.5572e-02, -2.4528e-01],\n",
      "         [-7.2369e-02, -5.9344e-02, -1.0740e-01,  ..., -1.3021e-01,\n",
      "          -8.1896e-02, -1.0998e-01],\n",
      "         ...,\n",
      "         [-2.4288e-03, -9.7340e-02, -4.9351e-02,  ..., -1.1390e-01,\n",
      "          -1.1779e-01, -8.1253e-02],\n",
      "         [ 3.3538e-02,  6.2052e-02,  7.9090e-02,  ...,  8.1745e-02,\n",
      "           9.0158e-02,  2.0755e-02],\n",
      "         [-9.8824e-02, -1.7040e-01, -1.7402e-01,  ..., -2.8448e-01,\n",
      "          -3.4307e-01, -5.6310e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.9906e-02, -9.9390e-03, -2.0928e-02,  ...,  9.9311e-03,\n",
      "           3.8786e-02,  1.5321e-02],\n",
      "         [-5.2979e-02, -6.4966e-02, -3.1736e-02,  ..., -2.0645e-02,\n",
      "          -4.1563e-02, -2.0790e-02],\n",
      "         [ 1.6711e-02,  1.2950e-02, -2.1346e-02,  ...,  3.8455e-02,\n",
      "           8.2035e-02,  7.0455e-02],\n",
      "         ...,\n",
      "         [ 1.9574e-02,  4.2424e-03, -5.1912e-02,  ...,  1.9397e-02,\n",
      "           3.3755e-02,  7.5206e-04],\n",
      "         [ 2.9343e-02,  2.4422e-02,  3.2898e-02,  ..., -4.0945e-02,\n",
      "           8.5715e-03,  2.2957e-03],\n",
      "         [-3.9370e-02,  1.1114e-02, -4.1822e-03,  ...,  4.6375e-02,\n",
      "           6.0297e-02,  6.7945e-02]],\n",
      "\n",
      "        [[ 1.0040e-01,  1.1885e-01,  1.0049e-01,  ...,  2.9286e-01,\n",
      "           2.3913e-01,  3.7683e-01],\n",
      "         [-5.0366e-02, -1.3161e-01, -1.1625e-01,  ..., -1.4510e-01,\n",
      "          -1.2554e-01, -1.0092e-01],\n",
      "         [ 5.2327e-02,  2.7396e-02,  2.6779e-02,  ...,  9.5499e-03,\n",
      "           1.1829e-02,  4.1076e-02],\n",
      "         ...,\n",
      "         [-5.1949e-02,  1.5812e-02,  1.2946e-02,  ..., -2.5454e-02,\n",
      "           9.1315e-03, -3.3414e-02],\n",
      "         [-2.5454e-02, -2.0679e-02,  4.2499e-02,  ...,  6.1640e-02,\n",
      "           1.7565e-02, -2.4975e-02],\n",
      "         [ 4.8330e-02,  1.0145e-01,  7.7749e-02,  ...,  1.5110e-01,\n",
      "           2.2335e-01,  3.4884e-01]],\n",
      "\n",
      "        [[ 9.4152e-02,  9.9222e-02,  9.7979e-02,  ...,  5.2570e-02,\n",
      "           7.9701e-02,  3.7632e-04],\n",
      "         [ 8.7997e-02,  1.7421e-01,  1.3700e-01,  ...,  9.6025e-02,\n",
      "           8.4562e-02, -3.1088e-02],\n",
      "         [ 6.1208e-02, -9.1251e-03,  1.7994e-02,  ...,  8.0161e-02,\n",
      "          -9.4836e-03,  7.5255e-02],\n",
      "         ...,\n",
      "         [-6.7248e-03, -4.7437e-02,  3.3107e-02,  ...,  7.8588e-03,\n",
      "           4.7554e-03, -9.0306e-04],\n",
      "         [ 3.7452e-02,  3.6043e-02,  3.0940e-02,  ...,  3.4710e-03,\n",
      "           2.9127e-02,  7.3170e-02],\n",
      "         [ 1.0499e-01,  4.8617e-02,  8.8542e-02,  ...,  5.6919e-02,\n",
      "          -8.3799e-02, -2.1065e-01]]], requires_grad=True)\n",
      "tensor([ 0.0170,  0.0179,  0.0279,  0.0639, -0.0349, -0.0229,  0.0155,  0.0101,\n",
      "         0.0367, -0.0439, -0.0167,  0.0019,  0.0332,  0.0073,  0.0098,  0.0096,\n",
      "         0.0110,  0.0090, -0.0494,  0.0709,  0.0669,  0.0184, -0.0153,  0.0185,\n",
      "         0.0337,  0.0434, -0.0209, -0.0528,  0.0488,  0.0162,  0.0069,  0.0016,\n",
      "        -0.0424,  0.0153, -0.0246, -0.0629, -0.0091, -0.0216,  0.0249, -0.0090,\n",
      "         0.0284,  0.0112,  0.0357,  0.0144,  0.0422,  0.0062,  0.0159, -0.0081,\n",
      "         0.0292,  0.0384, -0.0611,  0.0220, -0.0678,  0.0249, -0.0334,  0.0139,\n",
      "         0.0172, -0.0259,  0.0149,  0.0199,  0.0215, -0.0018, -0.0174, -0.0226,\n",
      "         0.0203, -0.0106,  0.0396,  0.0601, -0.0080,  0.0045, -0.0346, -0.0334,\n",
      "         0.0045,  0.0107,  0.0325,  0.0664, -0.0548, -0.0274,  0.0286, -0.0073,\n",
      "        -0.0205, -0.0191, -0.0160, -0.0715, -0.0717,  0.0647, -0.0177,  0.0503,\n",
      "         0.0027, -0.0402, -0.0035, -0.0531, -0.0519, -0.0010,  0.0102, -0.0243,\n",
      "        -0.0244,  0.0740, -0.0528, -0.0250, -0.0343, -0.0213, -0.0058, -0.0289,\n",
      "        -0.0805, -0.0132, -0.0831,  0.0151, -0.0141, -0.0078,  0.0622,  0.0166,\n",
      "        -0.0418, -0.0285, -0.0266, -0.0354,  0.0343, -0.0732,  0.0114, -0.0143,\n",
      "        -0.0398, -0.0568,  0.0339, -0.0297,  0.0087,  0.0146, -0.0381, -0.0172],\n",
      "       requires_grad=True)\n",
      "tensor([0.8736, 0.8975, 1.1252, 1.2818, 1.1267, 0.8592, 1.0656, 0.9061, 1.2026,\n",
      "        0.9486, 0.7698, 1.0494, 0.5577, 1.0093, 1.0127, 0.9585, 0.7019, 0.7407,\n",
      "        1.1313, 0.8372, 0.9381, 0.9703, 0.9826, 0.6182, 0.7628, 0.6283, 0.9588,\n",
      "        0.8779, 1.0448, 0.8019, 0.7422, 1.0976, 0.7329, 0.6134, 1.0826, 1.1559,\n",
      "        1.3844, 0.8810, 1.0026, 0.9909, 1.3805, 0.9869, 1.1944, 1.0997, 1.0180,\n",
      "        0.9742, 1.0041, 0.9015, 0.8557, 1.0164, 1.2589, 1.2093, 1.1690, 0.8714,\n",
      "        0.8199, 1.0023, 0.9384, 0.9671, 1.1104, 0.9537, 0.8362, 0.7566, 0.6739,\n",
      "        0.9933, 0.9696, 1.0176, 0.8401, 0.7647, 0.8369, 1.0607, 0.8379, 1.0405,\n",
      "        0.9187, 0.9088, 0.8788, 1.0310, 0.9129, 0.8599, 0.8136, 1.0510, 0.9201,\n",
      "        1.3198, 0.9150, 0.8486, 0.8220, 1.0515, 0.8123, 0.8941, 0.8520, 1.3250,\n",
      "        0.8109, 1.1356, 0.7863, 1.1104, 1.2988, 0.7845, 0.9225, 1.2693, 0.8459,\n",
      "        0.9655, 1.1210, 0.9241, 1.1366, 1.4227, 1.1062, 1.0789, 1.6412, 0.8921,\n",
      "        0.9870, 1.2284, 0.9418, 1.1375, 0.7718, 1.1798, 1.2758, 0.8073, 1.2409,\n",
      "        0.9097, 0.9307, 0.8085, 0.9679, 0.8098, 0.9061, 0.8893, 1.0749, 1.0636,\n",
      "        0.8425, 0.8938], requires_grad=True)\n",
      "tensor([ 0.1213, -0.2593, -0.4650,  0.0096,  0.2148, -0.1498, -0.2439,  0.0273,\n",
      "        -0.3696, -0.1052, -0.0974,  0.0450, -0.0877, -0.2657, -0.0512,  0.0571,\n",
      "        -0.1392,  0.0112, -0.0769,  0.0061,  0.1520, -0.1011, -0.0061, -0.1571,\n",
      "        -0.0761, -0.2039, -0.1450,  0.0293, -0.1162, -0.0688,  0.1562,  0.1001,\n",
      "        -0.0109, -0.0841,  0.0557, -0.0748, -0.2904,  0.2608, -0.3562, -0.3580,\n",
      "        -0.0555, -0.1715, -0.0169, -0.2432,  0.1019, -0.0634, -0.0483,  0.1798,\n",
      "        -0.1795, -0.3744, -0.7255, -0.0537,  0.0138, -0.1820, -0.0568,  0.1860,\n",
      "        -0.1185,  0.0256, -0.4591,  0.0495, -0.1131,  0.1371,  0.0673, -0.0831,\n",
      "        -0.0969,  0.1009, -0.0106, -0.2307,  0.0339, -0.0837, -0.1613,  0.1067,\n",
      "        -0.4743, -0.3725, -0.3021,  0.0550, -0.2679, -0.1201, -0.1023, -0.0879,\n",
      "        -0.0078, -0.1805, -0.3030, -0.0849, -0.0070, -0.3443, -0.1570, -0.1898,\n",
      "        -0.3579, -0.0386, -0.1833, -0.1093,  0.0540, -0.1607, -0.3183,  0.0477,\n",
      "         0.1058, -0.3708, -0.2831,  0.0090,  0.2789,  0.0332, -0.4864,  0.2068,\n",
      "        -0.4721, -0.0339, -0.6631,  0.0143,  0.0287, -0.0538, -0.1397, -0.2289,\n",
      "        -0.3476, -0.0405,  0.1445, -0.3045,  0.1336, -0.2288, -0.0470,  0.0464,\n",
      "         0.0764, -0.7867,  0.0329, -0.1891,  0.3277, -0.0092, -0.1825, -0.1396],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.0368, -0.0329, -0.0414,  ..., -0.0674, -0.1098, -0.0584],\n",
      "         [ 0.0492,  0.0730, -0.1062,  ...,  0.0057, -0.0318, -0.1677],\n",
      "         [-0.0420, -0.0167, -0.0476,  ...,  0.0363,  0.0185,  0.0914],\n",
      "         ...,\n",
      "         [-0.0248, -0.0659, -0.0033,  ...,  0.0275,  0.0340,  0.0861],\n",
      "         [-0.0328,  0.0113,  0.0156,  ..., -0.0884, -0.1129, -0.2071],\n",
      "         [-0.0418, -0.0314,  0.0308,  ...,  0.0747,  0.0035,  0.2461]],\n",
      "\n",
      "        [[-0.0524, -0.0229,  0.0318,  ...,  0.0354, -0.0131,  0.0320],\n",
      "         [-0.0335, -0.0428, -0.0214,  ..., -0.0078, -0.0302,  0.0533],\n",
      "         [-0.0145, -0.0146,  0.0216,  ...,  0.0944,  0.0828,  0.1499],\n",
      "         ...,\n",
      "         [-0.0565, -0.0056, -0.0296,  ..., -0.0046,  0.0080,  0.0037],\n",
      "         [-0.0496,  0.0127,  0.0209,  ...,  0.1101,  0.1180,  0.2515],\n",
      "         [ 0.0159,  0.0681,  0.0810,  ...,  0.0998,  0.0507,  0.1030]],\n",
      "\n",
      "        [[ 0.0701,  0.0480,  0.0071,  ..., -0.0205, -0.0260, -0.0258],\n",
      "         [ 0.0288, -0.0382, -0.0437,  ..., -0.0393, -0.1004, -0.3501],\n",
      "         [-0.0919, -0.0632, -0.0216,  ..., -0.0427, -0.0552, -0.0498],\n",
      "         ...,\n",
      "         [-0.0253, -0.0638, -0.0363,  ..., -0.0181, -0.0010, -0.0775],\n",
      "         [-0.0532, -0.0737, -0.0732,  ..., -0.1432, -0.1583, -0.2093],\n",
      "         [-0.0250, -0.0276, -0.0135,  ..., -0.0534, -0.0513, -0.0218]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0379, -0.0411, -0.0676,  ..., -0.0152, -0.0306, -0.0900],\n",
      "         [ 0.0254,  0.0544,  0.0418,  ...,  0.1173,  0.1121,  0.1677],\n",
      "         [-0.0181, -0.0606, -0.0511,  ..., -0.0481, -0.0992, -0.0639],\n",
      "         ...,\n",
      "         [-0.0728, -0.0254, -0.0519,  ..., -0.0554, -0.0498, -0.0544],\n",
      "         [-0.0924, -0.0984, -0.0830,  ..., -0.0598, -0.0736, -0.0981],\n",
      "         [ 0.0509,  0.0004,  0.0445,  ...,  0.1100,  0.1305,  0.0794]],\n",
      "\n",
      "        [[-0.0010,  0.0094, -0.0624,  ..., -0.1657, -0.1532, -0.1253],\n",
      "         [-0.0685, -0.0297,  0.0493,  ..., -0.0399,  0.0183, -0.1157],\n",
      "         [-0.0390, -0.0261,  0.0412,  ...,  0.0105,  0.0303,  0.0325],\n",
      "         ...,\n",
      "         [-0.0440,  0.0072, -0.0309,  ..., -0.0074,  0.0418,  0.0661],\n",
      "         [-0.0321,  0.0018,  0.0529,  ..., -0.0069, -0.0112,  0.0292],\n",
      "         [ 0.1197,  0.1010,  0.0952,  ...,  0.0240,  0.0702,  0.0422]],\n",
      "\n",
      "        [[ 0.0901,  0.1099,  0.1092,  ...,  0.0620,  0.0397, -0.0203],\n",
      "         [-0.0668, -0.0593, -0.1449,  ..., -0.1604, -0.1522, -0.1619],\n",
      "         [-0.0251, -0.0300, -0.0722,  ..., -0.0290, -0.0602,  0.2112],\n",
      "         ...,\n",
      "         [ 0.0436,  0.0621,  0.0918,  ...,  0.0261,  0.1035,  0.0032],\n",
      "         [-0.1006, -0.0701, -0.1069,  ..., -0.0885, -0.0992, -0.1002],\n",
      "         [ 0.0163,  0.0349,  0.0123,  ...,  0.0863,  0.0787,  0.2673]]],\n",
      "       requires_grad=True)\n",
      "tensor([-0.0177, -0.0082, -0.0040, -0.0087,  0.0011, -0.0019,  0.0019,  0.0136,\n",
      "         0.0270, -0.0235,  0.0034,  0.0159, -0.0106,  0.0091, -0.0065,  0.0382,\n",
      "        -0.0061, -0.0022, -0.0256,  0.0217,  0.0038,  0.0133, -0.0067, -0.0228,\n",
      "        -0.0246,  0.0038,  0.0113, -0.0226, -0.0029,  0.0291, -0.0189, -0.0125,\n",
      "         0.0194,  0.0118, -0.0075, -0.0082, -0.0322, -0.0087, -0.0045, -0.0027,\n",
      "        -0.0324,  0.0090, -0.0597, -0.0294,  0.0269, -0.0087,  0.0255,  0.0171,\n",
      "         0.0241, -0.0011, -0.0027,  0.0024, -0.0099, -0.0208, -0.0327, -0.0314,\n",
      "        -0.0196,  0.0587, -0.0221, -0.0100,  0.0244, -0.0220, -0.0318, -0.0049,\n",
      "         0.0132, -0.0549, -0.0115, -0.0212, -0.0211, -0.0052, -0.0168,  0.0082,\n",
      "         0.0013, -0.0210, -0.0123, -0.0127, -0.0164,  0.0031,  0.0153, -0.0124,\n",
      "        -0.0132,  0.0099, -0.0231, -0.0394, -0.0268,  0.0130,  0.0174,  0.0076,\n",
      "         0.0050, -0.0030, -0.0176, -0.0132, -0.0046, -0.0217,  0.0062, -0.0100,\n",
      "        -0.0150,  0.0165, -0.0228, -0.0376, -0.0046, -0.0141, -0.0119, -0.0051,\n",
      "        -0.0239, -0.0217,  0.0370, -0.0009, -0.0105,  0.0211,  0.0141,  0.0019,\n",
      "         0.0109,  0.0265,  0.0210, -0.0178, -0.0177,  0.0028,  0.0222, -0.0012,\n",
      "         0.0047,  0.0106,  0.0093,  0.0240,  0.0126,  0.0070, -0.0084,  0.0016,\n",
      "         0.0052, -0.0331, -0.0130, -0.0080, -0.0301,  0.0268,  0.0263,  0.0259,\n",
      "        -0.0154, -0.0085,  0.0019,  0.0036, -0.0126,  0.0169,  0.0006,  0.0356,\n",
      "         0.0050, -0.0302,  0.0099,  0.0162, -0.0189, -0.0131,  0.0128,  0.0221,\n",
      "         0.0118, -0.0113,  0.0018, -0.0151, -0.0109, -0.0039,  0.0247,  0.0213,\n",
      "        -0.0129, -0.0232, -0.0090,  0.0012, -0.0201, -0.0081,  0.0107,  0.0176,\n",
      "        -0.0191, -0.0177,  0.0054, -0.0039, -0.0048,  0.0139, -0.0265, -0.0268,\n",
      "        -0.0316,  0.0036,  0.0330,  0.0263, -0.0092,  0.0120,  0.0005, -0.0317,\n",
      "        -0.0134,  0.0191, -0.0193,  0.0168,  0.0045,  0.0018,  0.0002, -0.0036,\n",
      "         0.0020, -0.0232, -0.0031, -0.0138, -0.0116, -0.0267, -0.0098,  0.0051,\n",
      "         0.0032, -0.0037, -0.0037, -0.0259,  0.0338,  0.0006,  0.0142,  0.0100,\n",
      "         0.0132,  0.0169,  0.0058, -0.0133,  0.0019, -0.0143,  0.0214,  0.0107,\n",
      "         0.0334,  0.0171,  0.0010,  0.0226,  0.0074, -0.0006, -0.0481,  0.0314,\n",
      "        -0.0141, -0.0189, -0.0092,  0.0086, -0.0080,  0.0200,  0.0252, -0.0126,\n",
      "         0.0081, -0.0261,  0.0134, -0.0050,  0.0285,  0.0214,  0.0189, -0.0046,\n",
      "        -0.0080, -0.0135,  0.0120, -0.0211,  0.0060, -0.0085,  0.0042,  0.0323,\n",
      "        -0.0186,  0.0075,  0.0132,  0.0279,  0.0065, -0.0208, -0.0351, -0.0294],\n",
      "       requires_grad=True)\n",
      "tensor([0.7601, 1.1236, 0.9130, 0.8978, 1.0118, 0.9539, 1.2406, 0.6033, 0.9274,\n",
      "        0.9641, 0.8350, 0.7996, 0.9646, 0.7315, 0.8086, 0.7760, 0.6921, 0.9043,\n",
      "        1.1533, 1.0260, 1.4038, 0.9357, 0.9736, 1.0946, 1.2029, 1.2129, 1.2413,\n",
      "        1.0568, 1.1560, 0.7368, 0.9252, 0.8446, 1.2590, 0.8846, 0.8883, 0.8743,\n",
      "        1.0059, 1.1860, 0.9172, 1.1343, 0.9844, 0.9360, 1.3335, 0.7003, 1.0341,\n",
      "        0.8616, 1.2809, 0.9473, 1.3828, 1.0294, 0.9678, 1.0237, 1.1754, 0.9677,\n",
      "        0.9509, 0.7662, 1.3196, 0.9604, 0.6862, 1.2691, 1.0973, 1.2765, 0.9176,\n",
      "        1.0821, 0.9899, 1.0405, 1.0732, 1.2927, 0.9446, 0.8100, 1.2859, 1.0614,\n",
      "        1.1882, 1.0704, 0.8787, 1.0783, 0.9037, 0.9150, 0.5808, 1.1961, 1.0862,\n",
      "        0.9598, 1.3222, 1.1336, 1.5114, 0.9094, 1.2477, 1.0524, 0.7961, 1.0216,\n",
      "        0.9755, 1.1487, 1.1806, 0.9650, 0.9473, 1.1157, 1.7502, 1.0048, 1.1112,\n",
      "        1.1775, 0.9130, 0.7884, 1.1745, 1.0260, 0.9234, 0.8030, 1.2126, 1.0906,\n",
      "        1.1798, 1.0216, 0.7625, 1.2780, 1.0461, 1.1197, 0.8383, 1.2733, 1.2243,\n",
      "        0.9496, 1.1293, 0.8700, 1.2402, 1.1224, 1.4120, 0.9086, 1.0620, 1.1709,\n",
      "        0.9937, 1.0726, 1.1281, 1.0038, 1.1775, 1.1837, 1.0534, 1.0091, 0.8764,\n",
      "        1.1745, 1.2841, 1.2962, 0.6156, 1.1989, 1.1307, 1.1340, 1.0054, 0.7914,\n",
      "        1.3821, 1.1353, 0.9601, 1.1094, 0.8439, 0.8108, 1.0198, 1.1920, 1.0378,\n",
      "        1.2249, 1.2633, 0.9849, 1.0053, 0.9126, 1.2108, 0.9688, 1.2621, 0.8029,\n",
      "        1.2562, 1.2758, 0.7747, 1.0040, 0.9565, 1.0612, 1.4440, 0.7603, 0.9869,\n",
      "        0.8895, 1.0461, 1.1196, 0.8256, 0.9968, 1.4292, 1.1665, 0.9525, 0.6876,\n",
      "        0.6645, 0.8887, 1.1246, 0.9606, 1.0581, 0.9159, 0.7023, 1.1064, 0.7662,\n",
      "        1.0414, 1.2889, 0.7689, 1.2012, 1.1593, 0.8738, 1.0699, 1.0757, 1.2876,\n",
      "        1.2572, 1.2630, 1.0693, 1.3400, 0.7433, 1.3530, 0.7667, 1.1052, 0.9296,\n",
      "        0.8583, 0.8814, 1.0489, 1.1179, 0.9092, 1.2106, 1.0690, 1.2069, 0.8590,\n",
      "        1.3021, 1.0620, 0.9251, 0.8984, 0.5832, 0.8638, 0.8969, 1.1455, 0.9381,\n",
      "        0.8146, 0.8011, 0.9734, 0.7905, 0.8438, 1.1633, 1.2753, 1.0825, 1.2663,\n",
      "        0.8139, 0.9197, 1.1474, 0.8376, 1.0105, 0.8506, 1.2181, 0.7284, 1.1632,\n",
      "        1.2340, 1.2519, 1.0115, 0.8540, 0.6117, 0.8766, 0.7853, 1.0449, 1.0311,\n",
      "        1.0950, 0.9276, 1.2960, 0.8839], requires_grad=True)\n",
      "tensor([-0.4435, -0.8034,  0.1738,  0.1058, -0.5565, -0.1170, -0.2641, -0.2388,\n",
      "        -0.5382, -0.0608, -0.5503,  0.0495, -0.2025, -0.1844, -0.1903,  0.0451,\n",
      "        -0.2977, -0.2193,  0.1337, -0.1707,  0.4698, -0.2831, -0.0781, -0.3212,\n",
      "        -0.1121, -0.5153, -0.0187, -0.2060, -0.2217, -0.3591, -0.9737, -0.2771,\n",
      "         0.2335, -0.2429, -0.3004, -0.2470, -0.6723, -0.2688, -0.1953,  0.1686,\n",
      "         0.0204,  0.0469, -0.6862, -0.0711,  0.0974, -0.1395, -0.0163, -0.2434,\n",
      "         0.1210, -0.7928, -0.1032,  0.2289, -0.2520, -0.1689, -0.5795, -0.5539,\n",
      "         0.2280, -0.5865, -0.4820,  0.0211,  0.0999, -0.1267, -0.0184, -0.3332,\n",
      "        -0.2610,  0.0012, -0.9814, -0.0511, -0.0923, -0.1925, -0.2004, -0.1125,\n",
      "        -0.1493, -0.0356, -0.0721, -0.0196, -0.2328,  0.0730, -0.2890,  0.1897,\n",
      "        -0.1770, -0.0840, -0.1237, -0.2454, -0.1281, -0.0965, -0.3319, -0.1574,\n",
      "        -0.4009, -0.8511, -0.2677, -0.3659, -0.1406, -0.2067, -0.0068,  0.0657,\n",
      "        -0.1209, -0.0062,  0.0855, -0.1749, -0.4708, -0.2486,  0.0399, -0.0975,\n",
      "        -0.1038, -0.2555, -0.5868,  0.0486, -0.1374, -0.1641, -0.0967, -0.7096,\n",
      "        -0.2098,  0.1649, -0.4136, -0.1467, -0.2829, -0.2079, -0.1056, -0.3408,\n",
      "        -0.0988, -0.1561, -0.4039, -0.3486, -0.1724, -0.0580, -0.0704, -0.1725,\n",
      "        -0.0242, -0.3632, -0.0611,  0.1582, -0.3131, -0.0074, -0.2996, -0.1486,\n",
      "        -0.2142, -0.2481, -0.2244,  0.2373, -0.0803, -0.3711, -0.4296, -0.0935,\n",
      "         0.0036, -0.1159, -0.1364, -0.1734, -0.5419, -0.2610, -0.1813, -0.6674,\n",
      "        -0.1213, -0.0997, -0.2821, -0.3170, -0.1497, -0.2445, -0.1817, -0.0594,\n",
      "        -0.5936,  0.0847, -0.3742,  0.4093, -0.3424, -0.1853, -0.0994, -0.3344,\n",
      "         0.2458, -0.6804, -0.4674, -0.1803,  0.0398, -0.0578, -0.4599,  0.1312,\n",
      "        -0.1533,  0.2707, -0.3034, -0.1537, -0.2998, -0.3934,  0.0330, -0.2875,\n",
      "        -0.2519, -0.1387, -0.0967, -0.4245, -0.4807, -0.1301, -0.7322, -0.2394,\n",
      "         0.1248,  0.1945, -0.1905,  0.0504, -0.1841, -0.1447, -0.0417, -0.6424,\n",
      "        -0.1994,  0.2380, -0.2347, -0.3270, -0.0858, -0.2463, -0.1757, -0.8884,\n",
      "        -0.1664, -0.1424, -0.1947, -0.2091,  0.2629,  0.2331, -0.1059, -0.0126,\n",
      "        -0.3075, -0.2783,  0.0954, -0.2491, -0.2477, -0.1735, -0.9454, -0.2387,\n",
      "        -0.0895, -0.1120, -0.2282, -0.2705, -0.5722, -0.3078, -0.2804,  0.0394,\n",
      "        -0.0946, -0.7069, -0.1250, -0.2536,  0.0990, -0.5185, -0.8184, -0.4958,\n",
      "        -0.2580, -0.2160,  0.0446, -0.3768, -0.0625,  0.0530, -0.3155, -0.2421,\n",
      "        -0.0969, -0.9517, -0.0758, -0.1774,  0.0218, -0.3595, -0.3069, -0.3516],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.0379,  0.0061,  0.0570,  ...,  0.1420,  0.1617,  0.1685],\n",
      "         [-0.0138, -0.0630, -0.0761,  ..., -0.0132,  0.0039,  0.0235],\n",
      "         [-0.0210, -0.0278,  0.0091,  ...,  0.0849,  0.0478,  0.0740],\n",
      "         ...,\n",
      "         [-0.0231, -0.0784, -0.0074,  ..., -0.0526, -0.1014, -0.1735],\n",
      "         [-0.0487, -0.0949, -0.0758,  ..., -0.0679, -0.0749, -0.0287],\n",
      "         [ 0.0069,  0.0177, -0.0330,  ..., -0.0015,  0.0281,  0.1257]],\n",
      "\n",
      "        [[ 0.0021,  0.0103, -0.0018,  ...,  0.0049,  0.0086, -0.0256],\n",
      "         [ 0.0983,  0.1038,  0.1504,  ...,  0.0652,  0.0815,  0.0321],\n",
      "         [ 0.0563,  0.0281, -0.0029,  ..., -0.0397,  0.0254, -0.0670],\n",
      "         ...,\n",
      "         [ 0.1162,  0.1347,  0.0426,  ..., -0.0132,  0.0028, -0.0653],\n",
      "         [-0.1086, -0.0723, -0.0564,  ..., -0.0904, -0.0483, -0.0408],\n",
      "         [ 0.0555,  0.0254,  0.0243,  ...,  0.0525,  0.0804,  0.0090]],\n",
      "\n",
      "        [[ 0.1313,  0.0417,  0.0595,  ..., -0.0164, -0.0289, -0.0063],\n",
      "         [ 0.0250, -0.0871, -0.0905,  ..., -0.1024, -0.0821, -0.1176],\n",
      "         [ 0.1347,  0.1311,  0.0635,  ...,  0.0460,  0.0256, -0.0456],\n",
      "         ...,\n",
      "         [ 0.0548,  0.0177, -0.0371,  ..., -0.1119, -0.0778, -0.1328],\n",
      "         [ 0.0399, -0.0058, -0.0193,  ...,  0.1089,  0.0722,  0.0859],\n",
      "         [ 0.1276,  0.0798,  0.0706,  ...,  0.0322, -0.0077, -0.0179]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0120,  0.0026, -0.0381,  ...,  0.1004,  0.0926,  0.1819],\n",
      "         [ 0.0575,  0.0425,  0.0169,  ..., -0.0472, -0.0273, -0.1112],\n",
      "         [-0.0488, -0.0462, -0.0672,  ..., -0.1742, -0.1493, -0.1969],\n",
      "         ...,\n",
      "         [ 0.1386,  0.1291,  0.0325,  ...,  0.0339,  0.0859,  0.1472],\n",
      "         [ 0.0323, -0.0015, -0.0138,  ...,  0.0698,  0.0318, -0.0569],\n",
      "         [ 0.0494,  0.0221,  0.0543,  ...,  0.0569,  0.0759,  0.1018]],\n",
      "\n",
      "        [[-0.1565, -0.0663, -0.0893,  ...,  0.1777,  0.1420,  0.2764],\n",
      "         [ 0.0338, -0.0231, -0.0146,  ...,  0.0910,  0.1100,  0.0634],\n",
      "         [ 0.0619,  0.0035,  0.0311,  ...,  0.0240, -0.0386, -0.0675],\n",
      "         ...,\n",
      "         [-0.1041, -0.1006, -0.0785,  ..., -0.0753, -0.1310, -0.1596],\n",
      "         [ 0.0271,  0.0847,  0.0274,  ..., -0.0126, -0.0575, -0.0453],\n",
      "         [-0.0549, -0.0235, -0.0555,  ..., -0.1199, -0.1372, -0.1252]],\n",
      "\n",
      "        [[ 0.1066,  0.1057,  0.0854,  ...,  0.1746,  0.1262,  0.1463],\n",
      "         [-0.0119,  0.0153,  0.0413,  ...,  0.0205, -0.0119,  0.0590],\n",
      "         [-0.0424, -0.0079, -0.0393,  ...,  0.0689,  0.1035,  0.0991],\n",
      "         ...,\n",
      "         [ 0.0845,  0.0127, -0.0470,  ..., -0.0369,  0.0196,  0.0952],\n",
      "         [-0.0189,  0.0271,  0.0059,  ..., -0.0023, -0.0694, -0.0343],\n",
      "         [-0.0082,  0.0207, -0.0164,  ...,  0.0157,  0.0042,  0.0838]]],\n",
      "       requires_grad=True)\n",
      "tensor([ 9.7663e-03, -1.7510e-02, -3.2309e-03,  7.6644e-03, -3.7274e-03,\n",
      "        -9.0855e-03, -5.8782e-03,  1.8352e-02, -3.6626e-03, -1.9930e-02,\n",
      "        -8.3953e-03,  1.4854e-02,  1.4935e-02, -2.3540e-02, -2.1851e-03,\n",
      "        -8.9374e-03, -6.1047e-03, -8.7909e-03, -7.1442e-03, -8.9469e-03,\n",
      "        -5.0174e-04,  1.7986e-02,  3.9406e-03, -1.5544e-02, -1.8827e-02,\n",
      "         2.1784e-02,  1.1141e-02, -1.8823e-02, -1.6797e-02, -3.3473e-03,\n",
      "        -5.3346e-03,  1.3526e-02,  7.9431e-03, -1.3853e-03, -1.7560e-02,\n",
      "         5.6385e-03, -8.9471e-03,  4.7848e-03, -1.8012e-02, -1.6972e-02,\n",
      "         1.8876e-02,  1.1797e-02,  2.1037e-02,  4.5373e-03, -4.6150e-03,\n",
      "         1.3409e-02, -7.9207e-03, -6.9072e-03,  1.5097e-03, -2.6310e-03,\n",
      "         3.6746e-03,  6.2086e-03,  3.5637e-03, -4.5695e-03,  1.5048e-02,\n",
      "        -1.3329e-02, -1.4655e-02,  1.7052e-04,  7.2189e-03,  1.1959e-02,\n",
      "        -1.1575e-02, -9.4107e-03, -3.0971e-03,  1.6347e-03, -2.2921e-02,\n",
      "        -1.4185e-02, -6.0053e-03,  1.6921e-02,  1.4678e-02, -2.4736e-03,\n",
      "        -1.2929e-02, -1.3682e-02, -6.8183e-03, -3.4499e-03,  1.3610e-02,\n",
      "         1.3630e-02,  2.7600e-03, -5.4842e-03, -3.1460e-03,  1.7479e-02,\n",
      "         1.6494e-02, -1.7059e-02, -1.1945e-02,  7.5600e-03, -9.7979e-03,\n",
      "         1.5913e-03, -6.5907e-03,  5.6777e-03, -9.5835e-03, -1.0628e-02,\n",
      "        -5.6408e-03,  1.3253e-02,  7.7205e-03, -1.3032e-02,  1.2065e-02,\n",
      "        -1.6228e-02, -1.2749e-02,  1.1225e-02, -2.4250e-03, -2.6157e-03,\n",
      "        -1.7328e-02,  1.6698e-02, -2.7990e-02,  4.9781e-04, -9.6830e-03,\n",
      "         1.9109e-02,  1.7414e-02, -1.8543e-02, -1.3089e-02, -4.2925e-04,\n",
      "        -1.0865e-02, -1.1778e-02, -6.4887e-03, -4.8938e-03,  8.4946e-03,\n",
      "         9.5389e-03, -9.8447e-03,  9.3935e-04, -2.0790e-03,  5.7062e-03,\n",
      "         1.7898e-02, -1.4128e-02,  2.0011e-02, -1.0645e-02,  1.8839e-02,\n",
      "         9.5837e-03, -8.6495e-04, -6.8305e-03,  7.8026e-03, -1.4279e-02,\n",
      "        -9.5952e-03,  3.7691e-03, -5.3205e-03,  1.8691e-02, -5.6137e-03,\n",
      "        -3.3903e-03, -1.5312e-02,  1.8554e-03,  1.7030e-02, -6.9595e-03,\n",
      "        -7.4662e-03,  2.8643e-03, -1.1426e-02,  9.6514e-03, -1.3277e-02,\n",
      "         9.6048e-03, -8.8502e-03,  2.4587e-03, -1.0345e-02,  1.1976e-02,\n",
      "        -1.6901e-02, -1.0725e-02, -5.1904e-03,  1.3673e-02,  1.0531e-02,\n",
      "        -4.4806e-03, -1.7353e-02, -2.0802e-02, -1.9742e-02, -1.9206e-02,\n",
      "        -1.6356e-02, -9.1091e-03, -1.5340e-03,  9.7375e-03,  1.9733e-02,\n",
      "         1.3322e-02,  6.8477e-03, -1.2125e-02,  4.4640e-03, -7.3319e-03,\n",
      "        -5.3892e-03, -1.3522e-02, -1.9876e-02, -4.3071e-03, -2.5437e-02,\n",
      "         1.1440e-02,  8.1139e-03,  7.1560e-04,  9.8970e-03, -5.3219e-03,\n",
      "        -7.6372e-03, -1.6340e-02,  1.5896e-02,  6.0100e-03, -1.6379e-02,\n",
      "        -8.6564e-03, -7.4752e-03, -3.7866e-03,  1.0820e-02,  6.9813e-03,\n",
      "        -3.3132e-03, -1.3417e-02, -2.2309e-02,  4.5124e-03,  1.6678e-02,\n",
      "         1.1550e-02, -2.9266e-03,  1.4184e-02,  1.7824e-02, -1.7665e-03,\n",
      "        -2.3282e-02, -1.0274e-02,  1.4428e-02, -1.7807e-02, -1.3194e-02,\n",
      "         7.7812e-03, -1.0167e-02, -1.0629e-02, -1.8499e-02, -4.2128e-03,\n",
      "         1.4201e-03, -1.3829e-02, -9.8784e-03,  1.2160e-02, -8.7455e-03,\n",
      "        -1.9071e-02,  9.3857e-03,  1.8132e-02, -1.1503e-02,  8.5255e-03,\n",
      "        -1.7617e-03,  6.6547e-03,  1.8360e-03, -6.5375e-03,  1.4111e-03,\n",
      "        -7.6856e-03,  6.8066e-03,  2.0142e-02, -1.5383e-02, -1.6235e-02,\n",
      "        -1.5063e-02, -3.1138e-03,  7.9549e-03, -9.0228e-03, -2.7944e-03,\n",
      "        -1.4882e-02,  3.4203e-03,  9.7776e-03, -1.4606e-02,  6.1927e-03,\n",
      "        -4.9140e-03, -1.0126e-02,  2.0119e-02,  1.6199e-03,  1.3527e-02,\n",
      "        -9.7302e-03,  1.0682e-02, -2.3218e-03,  1.7898e-02, -1.6169e-02,\n",
      "         6.2442e-03, -3.5115e-03, -1.1700e-03,  1.0200e-02,  6.1114e-03,\n",
      "         1.3479e-02,  1.6480e-02, -5.2822e-03, -1.5645e-02,  1.0418e-02,\n",
      "         2.7255e-03,  1.0333e-02,  1.3282e-02, -1.2266e-02, -1.1771e-02,\n",
      "         1.2353e-02,  1.7729e-02,  1.2252e-02, -7.5751e-03,  1.0605e-02,\n",
      "         5.0193e-03, -2.3500e-02,  2.4162e-03,  1.3849e-02,  1.4173e-05,\n",
      "        -1.2551e-03, -1.5159e-02,  1.0865e-02,  9.1057e-03,  1.7838e-03,\n",
      "         8.4773e-03, -5.3557e-03,  9.2137e-03, -1.9765e-02, -1.6293e-02,\n",
      "         9.9331e-03, -6.2281e-03, -9.6011e-03,  1.3072e-02, -1.1031e-02,\n",
      "        -1.8726e-02,  2.9788e-03, -1.5941e-02,  1.7586e-02,  1.8772e-02,\n",
      "        -1.3608e-02, -1.6668e-03,  1.5383e-02,  8.4010e-03, -1.1368e-03,\n",
      "        -1.0169e-02,  1.9437e-02,  2.2321e-02, -1.9272e-02,  1.9009e-02,\n",
      "        -6.9112e-03,  1.6664e-02, -4.2149e-03, -1.8153e-02,  1.0711e-03,\n",
      "         2.4153e-02, -1.2928e-02,  7.5231e-03, -2.3966e-03, -6.8598e-03,\n",
      "         8.1435e-03, -2.3862e-03, -2.0164e-02,  1.6548e-03, -1.1478e-02,\n",
      "         1.2751e-02,  1.2098e-02,  8.1441e-03,  1.2941e-02, -1.4603e-02,\n",
      "        -1.0487e-02, -6.7739e-03, -8.2051e-04, -1.3836e-02, -6.5870e-03,\n",
      "        -2.7504e-02, -8.5526e-03,  1.0473e-02, -6.4303e-03, -1.0700e-02,\n",
      "        -2.7534e-02,  7.7936e-04, -1.2150e-02,  1.6357e-02, -1.5470e-02,\n",
      "        -5.6031e-03, -1.5038e-02,  1.4177e-02, -2.3873e-03, -8.7260e-03,\n",
      "        -1.6540e-02,  1.1678e-03, -1.7612e-02,  1.3014e-02, -2.0174e-02,\n",
      "         3.2725e-03, -2.0378e-02,  1.1816e-02, -1.6290e-02,  1.3320e-02,\n",
      "         1.1679e-02, -1.7151e-02,  2.3816e-03, -1.8207e-02,  3.0714e-03,\n",
      "         2.3126e-02, -4.6503e-03, -1.9803e-04, -1.1959e-02, -1.2437e-03,\n",
      "         1.2758e-02,  1.4808e-02, -1.3998e-03, -2.3466e-03,  1.1599e-02,\n",
      "        -9.3551e-03,  2.1071e-03,  7.5492e-04, -1.0092e-02,  1.6244e-02,\n",
      "        -1.5913e-02,  1.2503e-02,  7.1233e-03, -5.6008e-03, -1.3464e-02,\n",
      "         9.6293e-03,  5.5778e-04, -4.2977e-03,  2.0649e-03,  1.1743e-02,\n",
      "         1.7527e-03, -2.2211e-02,  7.5052e-03, -2.4184e-02,  1.3058e-02,\n",
      "         1.7128e-02, -9.7662e-03, -1.7675e-02, -1.9808e-02,  7.2217e-03,\n",
      "         2.2680e-03,  4.4004e-03,  6.4992e-04,  2.6307e-03, -1.4813e-02,\n",
      "         9.5080e-03,  9.2858e-03, -1.3696e-02, -9.1245e-03, -1.4132e-03,\n",
      "        -1.3867e-02,  1.1848e-02, -2.4195e-03, -1.2320e-02, -5.7467e-03,\n",
      "         1.2160e-02,  1.1377e-02,  2.0154e-02, -4.4300e-03, -2.9751e-03,\n",
      "         1.7581e-02, -2.9977e-02, -2.5507e-03,  2.9545e-03,  1.5599e-02,\n",
      "        -3.8626e-03,  1.8548e-02,  5.7114e-03, -1.6755e-02, -9.6719e-04,\n",
      "        -1.5503e-02, -1.2071e-02, -1.2792e-02, -1.1988e-02,  1.2159e-02,\n",
      "        -2.2551e-02, -2.6645e-02,  1.2454e-02,  3.0661e-03, -4.3987e-03,\n",
      "        -2.0235e-02,  2.0450e-03, -1.3020e-02,  1.5118e-02,  3.3206e-03,\n",
      "        -1.4763e-02,  1.3143e-02, -1.1647e-02, -5.8490e-03, -1.7908e-02,\n",
      "         1.9746e-03,  3.3111e-03, -1.1759e-02, -2.1944e-02,  1.9145e-02,\n",
      "        -1.0774e-04, -2.3384e-03, -1.3797e-02, -2.6411e-03, -6.7411e-04,\n",
      "        -1.0157e-02,  2.0010e-02,  1.8472e-02,  1.5589e-02, -1.7513e-02,\n",
      "         5.5347e-03, -4.9159e-03, -1.9898e-03,  1.7906e-02,  1.4529e-02,\n",
      "        -1.0498e-02,  3.7172e-03, -1.5226e-02, -1.0747e-02, -6.3582e-03,\n",
      "         1.2789e-02,  1.3487e-02,  9.8841e-04,  1.4704e-02, -8.7892e-03,\n",
      "        -5.0368e-03,  5.2168e-03,  2.8041e-05,  1.8976e-02,  3.8313e-04,\n",
      "        -2.0293e-02,  1.7347e-02, -3.1989e-03,  1.2228e-02,  1.6885e-02,\n",
      "        -1.2550e-03, -3.1079e-03,  1.5903e-02,  3.4764e-05,  9.4890e-03,\n",
      "         2.0843e-02, -1.3155e-02, -1.3154e-02, -3.8317e-03, -1.1231e-03,\n",
      "        -1.3302e-02, -1.0125e-02,  5.3555e-03,  1.0393e-02,  6.8747e-03,\n",
      "         2.1576e-02, -1.2146e-02, -1.3417e-02, -7.4901e-03,  2.7284e-03,\n",
      "        -6.7830e-03, -7.3330e-03,  1.5146e-02,  1.4980e-02, -1.4006e-02,\n",
      "         8.1722e-03, -2.5258e-03], requires_grad=True)\n",
      "tensor([0.3943, 0.3463, 0.4307, 0.2609, 0.4989, 0.4752, 0.3932, 0.4902, 0.5440,\n",
      "        0.5013, 0.4295, 0.5239, 0.4572, 0.4201, 0.3741, 0.3184, 0.4627, 0.4802,\n",
      "        0.4414, 0.3706, 0.3707, 0.3045, 0.4458, 0.5147, 0.4188, 0.4778, 0.5167,\n",
      "        0.3961, 0.5550, 0.3402, 0.3551, 0.3607, 0.4059, 0.4224, 0.4164, 0.4791,\n",
      "        0.5739, 0.4561, 0.4019, 0.4376, 0.3416, 0.3057, 0.6490, 0.5901, 0.4155,\n",
      "        0.4512, 0.3879, 0.3738, 0.5475, 0.4553, 0.3706, 0.5324, 0.5838, 0.4373,\n",
      "        0.3514, 0.4449, 0.3192, 0.5041, 0.4805, 0.5431, 0.5860, 0.3922, 0.4504,\n",
      "        0.4705, 0.3304, 0.5756, 0.3517, 0.5082, 0.4969, 0.5106, 0.4442, 0.3292,\n",
      "        0.6927, 0.6795, 0.3302, 0.3905, 0.6798, 0.3771, 0.4434, 0.4669, 0.4407,\n",
      "        0.4928, 0.4156, 0.5729, 0.4187, 0.4862, 0.4103, 0.3850, 0.4048, 0.4838,\n",
      "        0.5469, 0.4428, 0.4201, 0.5190, 0.3547, 0.4476, 0.4364, 0.4984, 0.5525,\n",
      "        0.3852, 0.4239, 0.3724, 0.4349, 0.3760, 0.4590, 0.3927, 0.5093, 0.4627,\n",
      "        0.3900, 0.5483, 0.4959, 0.4438, 0.4005, 0.6281, 0.3923, 0.5250, 0.4436,\n",
      "        0.3975, 0.2550, 0.3314, 0.3900, 0.4780, 0.4500, 0.5438, 0.4039, 0.6770,\n",
      "        0.4313, 0.2596, 0.4277, 0.4619, 0.5006, 0.4240, 0.3126, 0.4216, 0.4268,\n",
      "        0.4510, 0.5679, 0.4200, 0.4731, 0.3911, 0.4860, 0.4631, 0.6181, 0.4855,\n",
      "        0.5099, 0.2885, 0.4541, 0.4754, 0.4812, 0.3456, 0.4527, 0.8112, 0.4096,\n",
      "        0.5402, 0.4177, 0.4491, 0.2893, 0.4332, 0.4885, 0.4966, 0.4042, 0.4684,\n",
      "        0.4182, 0.4424, 0.4936, 0.3806, 0.3830, 0.4552, 0.6369, 0.4081, 0.5445,\n",
      "        0.6388, 0.4013, 0.4148, 0.3798, 0.3429, 0.4523, 0.4588, 0.9028, 0.4153,\n",
      "        0.3677, 0.4742, 0.4491, 0.4888, 0.4710, 0.3832, 0.3411, 0.4213, 0.4349,\n",
      "        0.4031, 0.5011, 0.5572, 0.6583, 0.7155, 0.4747, 0.4061, 0.4893, 0.4960,\n",
      "        0.3525, 0.4005, 0.3620, 0.3668, 0.5472, 0.6018, 0.4148, 0.4737, 0.3138,\n",
      "        0.6696, 0.4124, 0.4359, 0.4458, 0.4035, 0.3703, 0.3172, 0.4081, 0.5330,\n",
      "        0.3655, 0.4382, 0.3797, 0.5120, 0.4026, 0.3956, 0.3719, 0.1834, 0.5059,\n",
      "        0.4336, 0.4190, 0.3133, 0.3927, 0.4474, 0.4418, 0.4323, 0.4566, 0.5719,\n",
      "        0.4118, 0.3383, 0.4752, 0.5002, 0.4097, 0.4134, 0.4829, 0.4776, 0.5035,\n",
      "        0.4340, 0.3655, 0.4392, 0.3841, 0.5196, 0.5037, 0.7595, 0.4607, 0.4253,\n",
      "        0.3372, 0.3521, 0.4339, 0.4706, 0.4419, 0.4067, 0.4396, 0.5290, 0.3962,\n",
      "        0.4638, 0.3107, 0.5347, 0.3622, 0.3169, 0.3992, 0.3608, 0.4042, 0.3593,\n",
      "        0.4868, 0.3204, 0.4169, 0.4244, 0.4547, 0.4113, 0.4742, 0.3916, 0.3649,\n",
      "        0.3804, 0.4337, 0.3416, 0.2060, 0.4927, 0.7199, 0.4325, 0.5852, 0.6481,\n",
      "        0.4337, 0.3066, 0.4714, 0.3758, 0.4139, 0.4775, 0.6013, 0.3975, 0.4214,\n",
      "        0.4371, 0.5133, 0.5558, 0.4346, 0.2741, 0.3119, 0.4575, 0.3666, 0.2919,\n",
      "        0.2809, 0.3920, 0.3992, 0.3418, 0.5749, 0.4662, 0.3137, 0.3141, 0.4288,\n",
      "        0.3248, 0.4303, 0.4544, 0.5106, 0.5189, 0.4635, 0.3371, 0.4580, 0.4897,\n",
      "        0.3938, 0.4573, 0.4844, 0.3201, 0.4701, 0.7220, 0.5127, 0.6263, 0.4885,\n",
      "        0.5347, 0.4359, 0.7511, 0.4104, 0.5670, 0.4366, 0.4183, 0.3946, 0.5466,\n",
      "        0.4314, 0.3898, 0.3896, 0.6184, 0.3360, 0.3693, 0.3814, 0.5577, 0.3749,\n",
      "        0.3785, 0.3216, 0.5904, 0.6001, 0.4709, 0.4002, 0.5220, 0.4306, 0.5153,\n",
      "        0.4776, 0.4970, 0.4204, 0.4065, 0.6484, 0.5704, 0.4818, 0.4380, 0.4573,\n",
      "        0.4220, 0.4586, 0.4653, 0.4352, 0.7288, 0.4489, 0.3934, 0.2991, 0.4314,\n",
      "        0.4692, 0.3414, 0.4208, 0.4061, 0.4285, 0.4866, 0.4481, 0.4366, 0.3811,\n",
      "        0.4430, 0.4624, 0.4815, 0.4419, 0.4305, 0.9903, 0.4288, 0.4534, 0.4873,\n",
      "        0.3709, 0.3941, 0.3354, 0.4782, 0.3154, 0.3247, 0.5351, 0.5389, 0.4410,\n",
      "        0.4737, 0.4870, 0.4390, 0.3649, 0.3909, 0.3378, 0.4663, 0.4339, 0.4400,\n",
      "        0.3206, 0.2387, 0.5124, 0.4620, 0.4924, 0.4524, 0.4463, 0.4742, 0.5027,\n",
      "        0.4300, 0.4181, 0.6054, 0.3665, 0.3507, 0.3346, 0.4390, 0.5131, 0.5509,\n",
      "        0.3788, 0.4490, 0.4320, 0.4921, 0.4059, 0.4767, 0.3484, 0.4762, 0.5135,\n",
      "        0.4307, 0.3834, 0.4167, 0.7411, 0.4671, 0.4992, 0.3564, 0.5840, 0.3941,\n",
      "        0.3643, 0.3180, 0.4505, 1.0303, 0.4047, 0.4784, 0.4228, 0.4666, 0.4165,\n",
      "        0.4656, 0.4275, 0.5320, 0.3627, 0.4901, 0.4237, 0.3706, 0.4267, 0.4831,\n",
      "        0.3493, 0.4782, 0.5663, 0.4641, 0.4417, 0.4216, 0.2979, 0.5075, 0.3782,\n",
      "        0.3001, 0.3955, 0.4452, 0.3774, 0.4299, 0.4279, 0.5151, 0.4986, 0.4702,\n",
      "        0.3810, 0.5698, 0.2934, 0.3962, 0.4626, 0.5063, 0.4285, 0.3581, 0.4246,\n",
      "        0.3862, 0.4112, 0.4156, 0.3869, 0.4552, 0.4462, 0.4312, 0.3981, 0.5570,\n",
      "        0.4007, 0.3680, 0.4060, 0.3664, 0.4939, 0.5601, 0.4242, 0.3498],\n",
      "       requires_grad=True)\n",
      "tensor([-0.2592, -0.2709, -0.3199, -0.1240, -0.2618, -0.4035, -0.3802, -0.1406,\n",
      "        -0.2854, -0.5181, -0.3393, -0.3217, -0.2489, -0.2634, -0.2556, -0.2070,\n",
      "        -0.3298, -0.2897, -0.3186, -0.0361, -0.2414, -0.1689, -0.3190, -0.4177,\n",
      "        -0.3290, -0.3072, -0.2121, -0.7089, -0.4880, -0.1721, -0.5466, -0.2722,\n",
      "        -0.2174, -0.2499, -0.2279, -0.2392, -0.3477, -0.1865, -0.1036, -0.2664,\n",
      "        -0.2276, -0.2233, -0.3991, -0.4297, -0.2284, -0.1401, -0.1223, -0.2162,\n",
      "        -0.3318, -0.2841, -0.2018, -0.5205, -0.5101, -0.2419, -0.2893, -0.2414,\n",
      "        -0.2131, -0.3450, -0.3307, -0.3353, -0.2628, -0.2425, -0.3466, -0.2941,\n",
      "        -0.2039, -0.5107, -0.2517, -0.3907, -0.2214, -0.3028, -0.3127, -0.2249,\n",
      "        -0.2073, -0.5328, -0.1915, -0.2472, -0.4090, -0.1098, -0.3247, -0.1752,\n",
      "        -0.2443, -0.0766, -0.1916, -0.3560, -0.2289, -0.2862, -0.3498, -0.2231,\n",
      "        -0.1236, -0.3233, -0.1987, -0.4654, -0.1815, -0.3239, -0.6756, -0.1637,\n",
      "        -0.2957, -0.2856, -0.3057, -0.6542, -0.2825, -0.1957, -0.2481, -0.3032,\n",
      "        -0.2983, -0.2386, -0.4161, -0.4229, -0.1203, -0.3964, -0.3011, -0.2274,\n",
      "        -0.1806, -0.3355, -0.1850, -0.3410, -0.2407, -0.3001, -0.1258, -0.1921,\n",
      "        -0.2350, -0.2455, -0.3478, -0.4409, -0.1759, -0.5221, -0.2609, -0.3539,\n",
      "        -0.1552, -0.2867, -0.3740, -0.2436, -0.2812, -0.2587, -0.2154, -0.4414,\n",
      "        -0.1248, -0.2802, -0.2714, -0.2331, -0.2911, -0.3324, -0.4809, -0.1448,\n",
      "        -0.3336, -0.1670, -0.2797, -0.3068, -0.4384, -0.2227, -0.2608, -0.0454,\n",
      "        -0.3064, -0.2853, -0.3601, -0.2628, -0.1498, -0.2899, -0.2872, -0.2852,\n",
      "        -0.1308, -0.4783, -0.1959, -0.2619, -0.1733, -0.2787, -0.2271, -0.2826,\n",
      "        -0.1657, -0.1389, -0.3630, -0.2483, -0.1542, -0.2641, -0.1943, -0.2153,\n",
      "        -0.3084, -0.3745, -0.1640, -0.2516, -0.2164, -0.2548, -0.2091, -0.2761,\n",
      "        -0.3617, -0.3889, -0.1782, -0.2764, -0.3055, -0.1881, -0.5882, -0.3754,\n",
      "        -0.2055, -0.3524, -0.2447, -0.3145,  0.0529, -0.3156, -0.2032, -0.3445,\n",
      "        -0.1825, -0.1849, -0.5490, -0.1899, -0.2826, -0.2580, -0.6011, -0.0958,\n",
      "        -0.1720, -0.1845, -0.2347, -0.0384, -0.1268, -0.1523, -0.3245, -0.3232,\n",
      "        -0.2669, -0.2796, -0.2517, -0.2168, -0.2156, -0.2386, -0.2119, -0.2115,\n",
      "        -0.3476, -0.2500, -0.2165, -0.1574, -0.2765, -0.6342, -0.2006, -0.2853,\n",
      "        -0.3697, -0.3459, -0.2812, -0.1918, -0.3628, -0.3307, -0.5905, -0.2926,\n",
      "        -0.2357, -0.2931, -0.3375, -0.2617, -0.2989, -0.2456, -0.2714, -0.4005,\n",
      "        -0.3893, -0.1838, -0.3462, -0.2342, -0.1608, -0.3118, -0.4579, -0.1224,\n",
      "        -0.1943, -0.2810, -0.2365, -0.3187, -0.4379, -0.3463, -0.1469, -0.2610,\n",
      "        -0.2236, -0.1615, -0.2659, -0.1511, -0.1904, -0.2010, -0.3105, -0.2518,\n",
      "        -0.2101, -0.0933, -0.2424, -0.2673, -0.0739, -0.3131, -0.2289, -0.4328,\n",
      "        -0.2558, -0.0930, -0.3017, -0.2412, -0.0941, -0.3338, -0.3174, -0.1250,\n",
      "        -0.2671, -0.2201, -0.2254, -0.3207, -0.1987, -0.2710, -0.2795, -0.2274,\n",
      "        -0.3343, -0.2953, -0.3035, -0.3314, -0.3518, -0.1352, -0.1385, -0.3968,\n",
      "        -0.2999, -0.1588, -0.1239, -0.1792, -0.2389, -0.1660, -0.5433, -0.2571,\n",
      "        -0.2217, -0.3681, -0.3518, -0.1784, -0.3115, -0.2312, -0.3673, -0.2758,\n",
      "        -0.1900, -0.1912, -0.2451, -0.2558, -0.0417, -0.1808, -0.1464, -0.1901,\n",
      "        -0.2679, -0.3382, -0.3897, -0.4846, -0.3315, -0.2770, -0.2735, -0.0143,\n",
      "        -0.2939,  0.1301, -0.2639, -0.2047, -0.2543, -0.2777, -0.3356, -0.3095,\n",
      "        -0.2187, -0.2938, -0.1733, -0.2255, -0.1950, -0.3638, -0.2015, -0.4179,\n",
      "        -0.1843, -0.3527,  0.0771, -0.1521, -0.2759, -0.3016, -0.3759, -0.2832,\n",
      "        -0.2953, -0.3136, -0.1574, -0.2475, -0.3340, -0.1561, -0.1939, -0.2551,\n",
      "        -0.3335, -0.2450, -0.2445, -0.5750, -0.2530, -0.5254, -0.2860, -0.3638,\n",
      "        -0.3162, -0.3473, -0.3221, -0.1024, -0.2706, -0.2139, -0.2073, -0.3330,\n",
      "        -0.2630, -0.3143, -0.1608, -0.3952, -0.3099, -0.2865, -0.2189, -0.2443,\n",
      "         0.0896, -0.2742, -0.3875, -0.2874, -0.2431, -0.1511, -0.2107, -0.2750,\n",
      "        -0.1365, -0.1593, -0.2923, -0.2842, -0.3797, -0.2981, -0.2779, -0.6203,\n",
      "        -0.2024, -0.2791, -0.2235, -0.2251, -0.1739, -0.3120, -0.2392, -0.0900,\n",
      "        -0.1308, -0.2662, -0.2499, -0.1945, -0.2106, -0.3444, -0.3897, -0.2343,\n",
      "        -0.3661, -0.2978, -0.2234, -0.1775, -0.1949, -0.2199, -0.3478, -0.4133,\n",
      "        -0.1108, -0.4330, -0.2874, -0.3483, -0.2856, -0.3043, -0.1984, -0.2183,\n",
      "        -0.1675, -0.4032, -0.4094, -0.3177, -0.0841, -0.2961, -0.3148, -0.2038,\n",
      "        -0.2001, -0.1918, -0.1602, -0.4570, -0.3418,  0.3857, -0.2472, -0.2452,\n",
      "        -0.1195, -0.1726, -0.2401, -0.2837, -0.3297, -0.3256, -0.2275, -0.2947,\n",
      "        -0.2530, -0.2426, -0.1752, -0.3058, -0.2063, -0.3244, -0.3946, -0.2809,\n",
      "        -0.2300, -0.2458, -0.2124, -0.3051, -0.2588, -0.1282, -0.1949, -0.2494,\n",
      "        -0.2149, -0.2719, -0.2432, -0.5322, -0.2386, -0.6319, -0.2118, -0.3344,\n",
      "        -0.2064, -0.2189, -0.2846, -0.1972, -0.2135, -0.6761, -0.3270, -0.1926,\n",
      "        -0.3746, -0.3419, -0.2894, -0.3466, -0.3058, -0.1997, -0.3102, -0.2647,\n",
      "        -0.2523, -0.2695, -0.3231, -0.1643, -0.3279, -0.4983, -0.3737, -0.2285],\n",
      "       requires_grad=True)\n",
      "tensor([[[-1.0708e-02,  2.8231e-02,  2.6220e-02,  9.5787e-03, -1.2813e-02],\n",
      "         [-1.1410e-02, -1.3711e-02,  4.0408e-03,  7.6029e-03,  4.4606e-02],\n",
      "         [ 7.9427e-03,  1.1207e-02, -2.5466e-02, -1.5899e-02,  3.8197e-02],\n",
      "         ...,\n",
      "         [-7.3535e-03, -2.2365e-02,  1.2694e-02,  2.3573e-02,  3.1207e-02],\n",
      "         [ 3.3570e-02, -1.5274e-02,  3.4145e-02, -1.3509e-02,  2.7485e-02],\n",
      "         [ 4.8018e-02,  2.3919e-02, -3.4383e-02, -2.0123e-02,  2.6223e-02]],\n",
      "\n",
      "        [[-7.3508e-03, -3.4741e-02, -3.5714e-02, -4.4851e-02, -1.3152e-01],\n",
      "         [ 7.8625e-02,  5.6906e-02,  7.9542e-02,  3.7300e-02,  1.0866e-01],\n",
      "         [ 2.2618e-02,  1.6580e-02, -9.1488e-02, -3.8463e-02, -3.4530e-02],\n",
      "         ...,\n",
      "         [ 3.7594e-02, -9.6170e-03,  4.4456e-02,  2.3966e-02,  2.8051e-02],\n",
      "         [ 7.8344e-02,  5.2870e-02,  3.6984e-03, -5.5841e-02, -8.7522e-02],\n",
      "         [ 3.6491e-02,  4.3693e-02,  2.7721e-02, -4.9790e-02, -5.1548e-02]],\n",
      "\n",
      "        [[ 1.6452e-01,  1.1571e-01, -1.5610e-02, -2.1722e-02, -2.5282e-03],\n",
      "         [-5.8599e-02, -5.2854e-02,  4.7043e-02, -8.6122e-03,  9.9602e-02],\n",
      "         [-3.2942e-02, -1.0026e-02,  1.7532e-02,  8.4594e-02, -5.5264e-02],\n",
      "         ...,\n",
      "         [ 1.1694e-02, -1.2466e-02, -2.6413e-02,  3.0252e-02, -3.0927e-02],\n",
      "         [ 1.7728e-04, -3.6938e-02,  1.5916e-02, -8.2752e-02,  5.2041e-02],\n",
      "         [ 7.3560e-02,  9.1929e-02,  4.5099e-02, -2.0938e-02, -4.5504e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.5283e-04,  4.9453e-03, -3.0126e-02, -1.2344e-02, -5.6782e-03],\n",
      "         [-1.0318e-02,  3.1098e-02,  6.5923e-03,  3.9845e-02,  4.9372e-02],\n",
      "         [-2.3399e-03,  9.1565e-03, -3.2461e-02, -2.3505e-02, -3.6451e-03],\n",
      "         ...,\n",
      "         [-4.6266e-02, -3.1184e-02, -1.8789e-02, -2.2262e-03,  4.1050e-02],\n",
      "         [ 2.7621e-02, -3.7582e-02,  5.0307e-03,  3.1517e-02,  2.3087e-02],\n",
      "         [-1.1505e-02,  1.0311e-02,  1.2231e-02,  2.3370e-03,  6.8241e-03]],\n",
      "\n",
      "        [[ 2.6426e-02,  1.3525e-01,  1.1011e-01,  7.5596e-02,  1.9475e-02],\n",
      "         [ 3.2755e-02,  2.4844e-02, -3.6828e-02,  4.7181e-02,  7.0503e-02],\n",
      "         [-1.9278e-02,  1.5332e-02,  8.1622e-03,  5.7164e-02,  1.0499e-01],\n",
      "         ...,\n",
      "         [ 8.4732e-02,  2.6161e-02,  1.1822e-01, -8.4307e-03, -1.2580e-01],\n",
      "         [ 3.7140e-02,  7.6700e-02,  3.6554e-02, -7.1801e-03, -5.1126e-02],\n",
      "         [-2.2639e-02, -3.1044e-02, -8.0320e-02,  1.9609e-02,  2.5355e-02]],\n",
      "\n",
      "        [[ 1.1840e-02, -3.3238e-02,  6.3485e-03, -5.9929e-03,  1.6213e-02],\n",
      "         [-3.1822e-03,  3.0438e-02,  1.2041e-02,  2.6989e-02, -2.3200e-02],\n",
      "         [-5.0772e-02,  1.4666e-02,  2.8541e-03, -1.3236e-03,  9.5777e-03],\n",
      "         ...,\n",
      "         [-1.4649e-02, -9.5065e-03,  2.7476e-02,  2.0065e-02,  1.8812e-02],\n",
      "         [-2.2576e-04,  2.9777e-02,  3.2547e-02, -2.5044e-03, -4.4673e-03],\n",
      "         [-3.5779e-02, -1.9916e-03, -2.8978e-02,  1.3411e-02,  4.9866e-03]]],\n",
      "       requires_grad=True)\n",
      "tensor([-0.1040, -0.0062, -0.0414,  ..., -0.1958, -0.0132, -0.1998],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 6.4019e-04,  3.3880e-03,  2.2522e-03,  4.3746e-03, -4.5345e-03],\n",
      "         [ 6.8993e-04, -1.2724e-03, -1.9429e-03, -1.7408e-04, -1.3596e-03],\n",
      "         [-4.8576e-04,  1.4723e-03, -8.3735e-05,  1.0725e-04,  5.8853e-04],\n",
      "         ...,\n",
      "         [-9.4215e-04,  5.9303e-04,  6.4032e-04, -7.2787e-04,  9.7898e-05],\n",
      "         [-2.3611e-03, -1.7780e-03, -1.9752e-04,  1.7079e-03,  3.1136e-03],\n",
      "         [ 1.2466e-04,  5.5244e-04,  1.9875e-03,  1.3286e-03, -8.8500e-04]],\n",
      "\n",
      "        [[ 1.5154e-03, -2.6269e-03,  7.9607e-04,  3.4141e-03,  2.5280e-03],\n",
      "         [-1.5036e-03,  3.4866e-04,  5.4715e-04, -7.0043e-04,  3.2343e-04],\n",
      "         [ 1.2148e-03, -1.1551e-03, -1.0780e-03, -7.4318e-05,  7.3057e-04],\n",
      "         ...,\n",
      "         [ 7.7361e-04,  3.1655e-04,  3.2227e-04,  1.0462e-03, -9.0462e-04],\n",
      "         [ 2.9385e-03,  8.4892e-04,  2.7987e-03,  7.9048e-04,  2.1502e-03],\n",
      "         [-3.3952e-03, -2.7912e-03, -3.5564e-03,  4.4079e-04, -3.0603e-03]],\n",
      "\n",
      "        [[ 2.1766e-03, -2.8340e-04, -1.2336e-04,  6.0655e-06, -2.8548e-03],\n",
      "         [ 1.2025e-03,  1.1794e-03,  8.7821e-04,  1.6259e-03, -8.5576e-07],\n",
      "         [ 4.3869e-04, -1.6421e-03,  1.5370e-03, -7.3461e-05, -8.5034e-04],\n",
      "         ...,\n",
      "         [-1.0270e-03, -5.6292e-04, -7.6738e-04, -3.8454e-04, -7.3046e-05],\n",
      "         [-1.5034e-03,  2.2623e-03,  1.6495e-03,  3.4393e-04,  3.6575e-04],\n",
      "         [-2.8675e-03,  2.6288e-03, -1.2077e-03,  7.2374e-04, -3.6525e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.3064e-03,  5.3153e-04,  2.8555e-03,  6.4465e-04, -2.2723e-03],\n",
      "         [ 2.8584e-03,  2.7159e-03,  2.0227e-03,  8.8235e-04,  4.4908e-04],\n",
      "         [ 2.7781e-04, -5.3579e-04,  3.6763e-03,  1.0730e-03, -8.6737e-04],\n",
      "         ...,\n",
      "         [ 4.9927e-03, -3.1848e-03,  8.2766e-04, -2.3376e-03,  1.0229e-03],\n",
      "         [ 5.9570e-04, -1.1696e-03,  6.4611e-04,  1.7838e-03, -5.2768e-04],\n",
      "         [-6.6227e-04,  1.4301e-03,  1.0115e-04,  8.8707e-04,  4.5424e-04]],\n",
      "\n",
      "        [[-1.7996e-04,  3.1445e-04,  1.3920e-05,  2.0641e-03,  2.2197e-03],\n",
      "         [-1.4059e-03,  3.4346e-03,  1.5309e-03,  1.5547e-04, -2.1900e-03],\n",
      "         [ 2.2990e-03,  5.5868e-05,  1.9150e-03,  9.9778e-04, -5.4303e-04],\n",
      "         ...,\n",
      "         [ 4.3749e-03, -1.5092e-03, -1.9475e-03,  2.2354e-03, -9.0439e-05],\n",
      "         [ 1.4048e-03, -5.1069e-05,  9.6287e-04, -1.4008e-05,  7.2225e-04],\n",
      "         [-2.3530e-03, -4.3642e-04, -5.8355e-04, -2.6502e-03, -7.7403e-04]],\n",
      "\n",
      "        [[ 8.5170e-04,  1.5542e-03,  1.1828e-03,  1.0485e-03, -1.4404e-05],\n",
      "         [ 9.3939e-04,  2.2053e-03, -6.4059e-04,  3.5704e-04, -2.0983e-03],\n",
      "         [-1.7033e-03, -1.6405e-03,  1.1207e-03,  1.9079e-03, -5.9804e-04],\n",
      "         ...,\n",
      "         [ 6.4196e-04,  4.6592e-03,  5.0023e-04, -2.7738e-03, -7.8005e-04],\n",
      "         [-2.7401e-04,  2.8079e-04,  2.0709e-04, -1.0290e-03,  3.7011e-04],\n",
      "         [ 4.1576e-05, -5.5403e-04, -3.2712e-03, -2.4806e-05,  1.4649e-03]]],\n",
      "       requires_grad=True)\n",
      "tensor([ 5.1970e-04, -2.9257e-05,  1.0186e-03,  1.9722e-03, -9.1436e-04,\n",
      "        -2.9528e-04,  9.8446e-04, -9.8448e-03, -1.0097e-03, -3.7141e-04,\n",
      "         7.7517e-04,  1.3168e-03, -3.4689e-02, -1.0573e-03,  8.4048e-04,\n",
      "         1.5598e-03, -5.6259e-04, -2.6165e-03, -2.1188e-03,  1.1308e-03,\n",
      "         1.2995e-03, -2.0638e-03, -5.4590e-04,  7.4660e-04, -5.9032e-04,\n",
      "         8.4194e-04, -7.7363e-04,  3.3269e-03, -2.9755e-04,  1.3339e-03,\n",
      "        -1.6779e-03, -2.5356e-03,  8.0082e-04, -1.1580e-04, -1.2940e-03,\n",
      "         1.1552e-03, -5.1570e-04, -2.7438e-02, -1.7285e-03, -6.0557e-05,\n",
      "         2.5532e-03,  1.7219e-02,  4.2101e-03, -1.1631e-03, -3.4607e-04,\n",
      "        -9.8473e-05, -2.2689e-03,  1.1350e-03,  1.6483e-03, -8.1157e-04,\n",
      "         1.1795e-03, -1.7108e-03,  1.7748e-04, -1.4633e-03,  6.6073e-04,\n",
      "        -1.2459e-03, -7.0406e-04,  5.3122e-05,  2.5158e-04,  8.4646e-04,\n",
      "         2.4554e-03, -1.6863e-03,  3.6228e-04, -3.1498e-04,  6.3462e-04,\n",
      "        -3.1041e-03, -2.1808e-03, -1.6618e-03, -5.1538e-05,  1.8966e-03,\n",
      "        -6.5398e-04, -1.2324e-03,  8.8737e-04,  2.1122e-04,  1.4078e-04,\n",
      "        -3.5587e-03,  1.2677e-03,  1.6163e-03, -2.1646e-03,  9.6144e-02,\n",
      "         2.5415e-03, -3.7253e-03, -6.6736e-04,  1.0739e-03, -1.4681e-04,\n",
      "        -1.4401e-03,  2.6250e-04, -1.6541e-03, -1.2600e-03,  2.2611e-03,\n",
      "         1.8864e-03,  5.4201e-04,  6.3122e-04,  9.2997e-04, -6.6488e-04,\n",
      "        -1.3174e-03,  2.4841e-03, -1.1294e-03, -2.2995e-03, -2.8735e-03,\n",
      "         3.3271e-03,  8.0842e-04,  1.5507e-03,  1.1821e-04, -3.6302e-04,\n",
      "        -2.6390e-04,  1.9270e-03, -1.7811e-03, -2.1105e-03, -1.3417e-03,\n",
      "        -9.7655e-04, -2.1693e-03,  9.4158e-04,  8.4561e-05,  2.8231e-03,\n",
      "        -3.3780e-03, -1.7381e-04,  3.1162e-03,  8.0573e-04,  2.8371e-03,\n",
      "        -2.3030e-01,  1.5902e-03,  8.8918e-04,  4.0256e-02, -7.0925e-04,\n",
      "         2.3321e-04, -1.4805e-04,  4.5775e-03,  4.7970e-01,  4.7246e-01,\n",
      "         4.6853e-01,  4.6214e-01,  4.7734e-01,  4.7415e-01,  4.6677e-01,\n",
      "        -2.8023e-01,  4.7624e-01,  4.8037e-01,  4.7756e-01,  4.7764e-01,\n",
      "        -6.7160e-02,  4.6441e-01,  4.6710e-01,  4.7244e-01,  4.6166e-01,\n",
      "         4.7990e-01,  4.7617e-01,  4.4218e-01,  4.7421e-01,  4.5342e-01,\n",
      "         4.7847e-01,  4.7229e-01,  4.7665e-01,  4.6324e-01,  4.7521e-01,\n",
      "         4.7744e-01,  4.7798e-01,  4.6115e-01,  4.7599e-01,  4.7191e-01,\n",
      "         4.7266e-01,  4.7306e-01,  4.7944e-01,  4.7381e-01,  4.4021e-01,\n",
      "        -2.9050e-01,  4.8218e-01,  4.5081e-01,  4.5493e-01,  2.9787e-01,\n",
      "         3.7373e-01,  4.8084e-01,  4.7720e-01,  4.7523e-01,  4.6579e-01,\n",
      "         4.6836e-01,  4.5728e-01,  4.7627e-01,  4.7668e-01,  4.5563e-01,\n",
      "         4.7705e-01,  4.7515e-01,  4.4716e-01,  4.8216e-01,  4.5159e-01,\n",
      "         4.7215e-01,  4.7174e-01,  4.6423e-01,  4.6778e-01,  4.7274e-01,\n",
      "         4.7951e-01,  4.3089e-01,  4.7799e-01,  4.6887e-01,  4.8063e-01,\n",
      "         4.7484e-01,  4.8092e-01,  4.7785e-01,  4.8130e-01,  4.6186e-01,\n",
      "         4.7468e-01,  4.7278e-01,  4.8026e-01,  4.7755e-01,  4.6970e-01,\n",
      "         4.7935e-01,  4.5070e-01, -2.5352e-01,  4.7681e-01,  4.7568e-01,\n",
      "         4.7378e-01,  4.7513e-01,  4.7544e-01,  4.6472e-01,  4.7758e-01,\n",
      "         4.7711e-01,  4.7467e-01,  4.6125e-01,  4.7714e-01,  4.7594e-01,\n",
      "         4.8238e-01,  4.8075e-01,  4.7639e-01,  4.0388e-01,  4.6023e-01,\n",
      "         4.7580e-01,  4.7126e-01,  4.6371e-01,  4.2298e-01,  4.6567e-01,\n",
      "         4.7187e-01,  4.7926e-01,  4.6424e-01,  4.7364e-01,  4.6579e-01,\n",
      "         4.6740e-01,  4.6501e-01,  4.6410e-01,  4.7553e-01,  4.7230e-01,\n",
      "         4.7634e-01,  4.5375e-01,  4.7380e-01,  4.7139e-01,  4.6284e-01,\n",
      "         4.7340e-01,  4.6253e-01,  4.7657e-01, -7.1958e-01,  4.7578e-01,\n",
      "         4.8138e-01,  2.1171e-01,  4.7235e-01,  4.7669e-01,  4.7282e-01,\n",
      "         4.8197e-01], requires_grad=True)\n",
      "tensor([-0.1237, -0.2945, -0.1263,  ..., -0.2622, -0.1651, -0.1175],\n",
      "       requires_grad=True)\n",
      "tensor([[[-1.1349e-02, -2.3640e-03,  5.8063e-03,  ..., -5.2513e-04,\n",
      "           4.3880e-03,  1.1038e-03],\n",
      "         [-1.4303e-02,  2.5419e-03, -3.3147e-03,  ...,  8.1616e-03,\n",
      "           3.3273e-03, -4.1152e-03],\n",
      "         [ 6.1842e-03, -1.7938e-02, -3.7723e-03,  ..., -9.0857e-04,\n",
      "          -1.5210e-03,  3.2630e-03],\n",
      "         ...,\n",
      "         [ 2.3017e-02, -1.3113e-02, -9.3108e-04,  ..., -5.4250e-03,\n",
      "          -5.5661e-05, -1.1963e-03],\n",
      "         [-1.2467e-02,  5.9902e-03,  2.5910e-04,  ...,  1.3643e-03,\n",
      "          -6.8782e-03, -9.6355e-05],\n",
      "         [ 3.5835e-03, -4.1493e-03,  3.9337e-03,  ...,  3.7253e-03,\n",
      "          -2.2249e-03, -1.5152e-03]],\n",
      "\n",
      "        [[ 4.3380e-03,  1.0300e-03,  3.4541e-03,  ..., -1.3339e-03,\n",
      "           1.1340e-02,  3.4676e-03],\n",
      "         [ 4.1177e-03, -5.7827e-03,  2.8043e-03,  ..., -8.7776e-05,\n",
      "          -1.5428e-03, -1.8229e-03],\n",
      "         [ 4.3210e-03, -9.2073e-04,  6.6858e-04,  ...,  5.3929e-04,\n",
      "           6.8811e-03,  2.4591e-04],\n",
      "         ...,\n",
      "         [-3.8528e-04,  6.6943e-03,  3.8531e-03,  ..., -1.6025e-04,\n",
      "          -3.7200e-03, -1.4003e-03],\n",
      "         [-4.7167e-03,  2.6522e-03,  2.0911e-03,  ...,  7.8776e-03,\n",
      "          -4.7741e-04, -5.9604e-04],\n",
      "         [-4.4935e-03,  1.2019e-03, -1.9746e-03,  ..., -2.2892e-03,\n",
      "           7.0832e-03,  9.4172e-03]],\n",
      "\n",
      "        [[ 4.4250e-03,  3.8798e-03, -1.1193e-03,  ..., -5.0168e-03,\n",
      "          -5.9754e-04, -3.4023e-04],\n",
      "         [ 6.8046e-03, -2.6773e-03,  8.5156e-03,  ..., -4.7598e-03,\n",
      "           1.1006e-03, -2.1435e-03],\n",
      "         [-6.0841e-04, -5.6668e-05,  7.7787e-04,  ...,  5.9388e-03,\n",
      "          -1.5360e-04, -2.2770e-03],\n",
      "         ...,\n",
      "         [ 9.5737e-03,  2.3779e-03,  2.9615e-03,  ..., -4.1188e-03,\n",
      "           7.6919e-03, -5.6624e-03],\n",
      "         [-4.1654e-03,  1.0106e-02, -9.1821e-04,  ..., -1.5403e-03,\n",
      "           4.0548e-03,  1.7017e-03],\n",
      "         [ 1.4244e-03, -2.0258e-03, -7.9778e-03,  ..., -2.8768e-03,\n",
      "          -1.7189e-03, -9.3538e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.5156e-03,  1.1169e-02,  1.1009e-03,  ..., -2.9597e-03,\n",
      "           3.3380e-03,  3.0518e-03],\n",
      "         [-2.9877e-03,  2.5511e-03, -1.5851e-03,  ...,  3.5569e-03,\n",
      "           3.5965e-03,  3.5643e-03],\n",
      "         [ 1.9930e-03, -1.8494e-03,  2.4764e-03,  ...,  8.3309e-04,\n",
      "          -5.6454e-03, -6.2058e-04],\n",
      "         ...,\n",
      "         [ 3.7475e-03, -8.3262e-03,  9.1562e-04,  ..., -2.4390e-03,\n",
      "           1.9214e-03, -1.0666e-03],\n",
      "         [ 1.5615e-03,  4.4258e-03, -4.3324e-03,  ..., -5.4313e-03,\n",
      "           7.3054e-03,  7.2723e-03],\n",
      "         [ 6.1055e-04, -3.8881e-03,  2.8740e-03,  ..., -1.1406e-03,\n",
      "           1.5696e-03, -1.0628e-02]],\n",
      "\n",
      "        [[ 6.8841e-03,  2.5777e-03, -6.2248e-04,  ..., -2.3065e-03,\n",
      "           2.7750e-03, -4.3492e-03],\n",
      "         [-5.0879e-03, -4.0160e-03,  7.5613e-03,  ..., -5.4164e-03,\n",
      "          -6.0005e-03, -3.3676e-03],\n",
      "         [ 1.0989e-03,  1.1957e-03,  4.6396e-03,  ...,  1.1639e-03,\n",
      "          -1.6558e-03, -9.2149e-03],\n",
      "         ...,\n",
      "         [ 1.8616e-03,  4.7104e-03,  9.3837e-04,  ...,  9.0611e-03,\n",
      "          -5.8188e-03,  1.4205e-03],\n",
      "         [ 1.8231e-03,  5.5272e-03,  1.7270e-03,  ..., -4.6792e-03,\n",
      "          -2.0629e-03, -1.8028e-03],\n",
      "         [-4.8270e-03,  5.2094e-03, -6.2280e-03,  ..., -5.3361e-03,\n",
      "           2.4044e-03, -8.5243e-04]],\n",
      "\n",
      "        [[-1.3036e-03,  3.2539e-03,  4.4577e-03,  ...,  1.0454e-03,\n",
      "           1.7770e-04, -1.4994e-03],\n",
      "         [ 1.5271e-03,  4.6732e-04, -3.8339e-03,  ...,  4.8341e-03,\n",
      "           1.7914e-03,  1.8358e-03],\n",
      "         [ 5.8808e-03,  1.6746e-03, -2.4208e-03,  ..., -3.1949e-03,\n",
      "          -6.2058e-03, -5.0718e-03],\n",
      "         ...,\n",
      "         [ 7.8775e-04, -8.0095e-03, -1.8020e-03,  ...,  1.1864e-03,\n",
      "          -8.2992e-04,  1.9305e-03],\n",
      "         [-1.0610e-02, -5.7050e-03, -3.1575e-04,  ...,  6.7904e-04,\n",
      "           5.7254e-03,  5.0466e-03],\n",
      "         [ 3.4915e-04,  4.2263e-04, -5.1817e-03,  ..., -1.2556e-03,\n",
      "          -1.4916e-04,  1.0500e-03]]], requires_grad=True)\n",
      "tensor([[[ 1.1499e-02,  7.4402e-03, -9.8164e-03,  ...,  1.1016e-03,\n",
      "          -6.5668e-03,  1.0142e-03],\n",
      "         [-3.7637e-03,  8.3350e-03, -4.3112e-03,  ...,  4.3855e-03,\n",
      "           5.9139e-04, -3.5251e-03],\n",
      "         [ 2.9335e-04,  9.9694e-05,  2.3960e-03,  ..., -8.7608e-03,\n",
      "          -5.6239e-03,  7.8870e-06],\n",
      "         ...,\n",
      "         [ 1.4741e-03,  1.4048e-03,  2.4717e-03,  ..., -1.2205e-03,\n",
      "           6.1361e-03,  2.9784e-05],\n",
      "         [-7.9310e-04,  1.5969e-03,  4.8962e-03,  ..., -3.2956e-03,\n",
      "           7.3138e-03, -3.9142e-03],\n",
      "         [ 1.3086e-03, -3.1401e-04,  1.9704e-03,  ..., -1.0162e-03,\n",
      "          -2.6873e-04,  3.4760e-03]],\n",
      "\n",
      "        [[ 7.2996e-04, -1.4614e-02,  7.3930e-03,  ..., -1.8306e-02,\n",
      "           3.9206e-03, -3.4877e-03],\n",
      "         [ 1.1798e-02,  1.7055e-03,  6.0087e-03,  ..., -1.3293e-02,\n",
      "           1.0691e-04,  6.2486e-03],\n",
      "         [ 7.7189e-03, -8.9924e-04,  1.0982e-03,  ...,  1.6260e-03,\n",
      "           9.5441e-04, -7.5517e-03],\n",
      "         ...,\n",
      "         [-2.5600e-02, -5.9084e-03, -8.6055e-03,  ..., -5.0220e-03,\n",
      "           1.2238e-04,  2.3492e-02],\n",
      "         [ 6.4115e-03,  5.8961e-03, -7.9882e-03,  ..., -3.1532e-03,\n",
      "          -1.3569e-02,  6.4777e-03],\n",
      "         [-1.5596e-02,  2.0907e-03,  1.3835e-03,  ..., -2.3804e-03,\n",
      "           4.8573e-04, -3.4465e-03]],\n",
      "\n",
      "        [[ 1.6299e-03, -7.2878e-03, -3.3759e-03,  ...,  1.2172e-03,\n",
      "          -2.7035e-03, -1.8673e-03],\n",
      "         [-1.6097e-03,  1.9158e-03,  3.1517e-03,  ...,  3.6213e-03,\n",
      "          -7.4702e-04, -3.1761e-03],\n",
      "         [-6.8292e-04, -4.0751e-03, -4.0833e-03,  ...,  1.8214e-03,\n",
      "          -1.4440e-02,  1.7928e-03],\n",
      "         ...,\n",
      "         [ 8.6755e-03,  2.0742e-04,  5.2675e-03,  ..., -5.7903e-03,\n",
      "           5.6012e-03, -2.5536e-03],\n",
      "         [-1.6967e-03, -8.5729e-04, -9.2552e-04,  ..., -2.6249e-03,\n",
      "           4.8508e-03,  4.1177e-03],\n",
      "         [ 3.4948e-03, -1.1401e-02, -5.7927e-03,  ..., -2.9195e-03,\n",
      "          -4.7952e-03, -2.4996e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.4150e-02,  6.2296e-03, -5.1955e-03,  ...,  1.8271e-02,\n",
      "           9.8278e-03,  1.2740e-02],\n",
      "         [ 6.9423e-03, -8.8851e-03,  2.4510e-03,  ...,  6.6423e-03,\n",
      "           1.6160e-02, -2.5833e-03],\n",
      "         [-7.1388e-04, -1.5779e-02,  1.7643e-02,  ..., -9.3235e-03,\n",
      "          -4.0857e-03,  1.3018e-03],\n",
      "         ...,\n",
      "         [ 1.2898e-03,  8.5689e-03,  1.2421e-03,  ..., -7.2548e-03,\n",
      "          -1.7013e-03,  1.0139e-02],\n",
      "         [-2.4738e-02,  1.6607e-02,  3.5278e-03,  ..., -2.0477e-02,\n",
      "          -6.0279e-03,  2.0647e-02],\n",
      "         [-1.3097e-02,  2.8961e-03,  1.7047e-02,  ...,  1.5297e-03,\n",
      "          -8.0043e-03, -1.2437e-02]],\n",
      "\n",
      "        [[-4.8428e-05,  5.2114e-04,  1.0700e-02,  ..., -1.4958e-03,\n",
      "           6.7410e-03,  7.9216e-03],\n",
      "         [ 5.7298e-04, -8.4721e-03,  1.4625e-03,  ..., -1.1859e-02,\n",
      "           6.4414e-04, -5.4815e-03],\n",
      "         [-1.0672e-02, -6.2627e-03, -1.0440e-02,  ..., -1.9233e-03,\n",
      "          -9.0708e-04, -6.5702e-03],\n",
      "         ...,\n",
      "         [-5.5593e-03, -1.2964e-02,  1.3641e-03,  ..., -4.8624e-03,\n",
      "          -5.6903e-03, -3.2907e-03],\n",
      "         [-1.2348e-02,  1.5057e-02,  1.7077e-02,  ..., -1.0009e-02,\n",
      "           1.7336e-02, -2.4987e-03],\n",
      "         [ 1.6174e-02,  8.8641e-03, -4.9082e-03,  ...,  2.5705e-03,\n",
      "          -5.8685e-03, -3.0854e-03]],\n",
      "\n",
      "        [[-4.5334e-03,  2.3936e-03, -7.1113e-03,  ..., -2.7256e-03,\n",
      "          -6.1331e-03, -3.0368e-03],\n",
      "         [-1.0962e-03,  9.4405e-04, -5.9475e-03,  ..., -2.5320e-03,\n",
      "          -3.9993e-03,  4.4303e-03],\n",
      "         [-3.3659e-03, -1.6571e-03,  5.8173e-04,  ..., -1.7083e-03,\n",
      "          -3.6325e-03, -5.6277e-03],\n",
      "         ...,\n",
      "         [-3.4783e-03, -5.3762e-04,  1.8211e-03,  ...,  2.6509e-03,\n",
      "          -4.3282e-03,  2.3858e-03],\n",
      "         [-9.9663e-04, -3.8140e-03, -3.8314e-04,  ..., -1.9781e-03,\n",
      "           2.0828e-04,  3.4178e-03],\n",
      "         [-4.2117e-03,  2.3260e-03, -4.5837e-03,  ...,  2.9927e-03,\n",
      "          -4.1446e-03, -3.4475e-03]]], requires_grad=True)\n",
      "tensor([[[ 0.0609, -0.0233,  0.0777],\n",
      "         [ 0.0151, -0.0031, -0.1168],\n",
      "         [ 0.0229, -0.0092, -0.0055],\n",
      "         ...,\n",
      "         [ 0.0365,  0.0390, -0.0824],\n",
      "         [-0.0419,  0.0208, -0.0142],\n",
      "         [ 0.0041, -0.0118, -0.0248]],\n",
      "\n",
      "        [[-0.0227, -0.0562,  0.0132],\n",
      "         [-0.0183, -0.0700, -0.0386],\n",
      "         [ 0.0161, -0.0096, -0.0240],\n",
      "         ...,\n",
      "         [-0.0177, -0.0003, -0.0455],\n",
      "         [-0.0157, -0.0499, -0.0533],\n",
      "         [ 0.0881,  0.0050,  0.0135]],\n",
      "\n",
      "        [[-0.0353, -0.0062,  0.0146],\n",
      "         [-0.0083,  0.0060,  0.0169],\n",
      "         [ 0.0142,  0.0022,  0.0190],\n",
      "         ...,\n",
      "         [-0.0387,  0.0106, -0.0268],\n",
      "         [ 0.0080,  0.0173, -0.0354],\n",
      "         [ 0.0137, -0.0396, -0.0332]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0260,  0.0032, -0.0634],\n",
      "         [-0.0339,  0.0008, -0.0110],\n",
      "         [ 0.0142,  0.0342,  0.0180],\n",
      "         ...,\n",
      "         [ 0.0188, -0.0280, -0.0223],\n",
      "         [-0.0145,  0.0062, -0.0171],\n",
      "         [-0.0334,  0.0077, -0.0253]],\n",
      "\n",
      "        [[-0.0016, -0.0186, -0.0114],\n",
      "         [-0.0211, -0.0280,  0.0218],\n",
      "         [ 0.0125,  0.0185, -0.0039],\n",
      "         ...,\n",
      "         [-0.0014, -0.0325, -0.0440],\n",
      "         [ 0.0165, -0.0160,  0.0119],\n",
      "         [ 0.0144,  0.0160, -0.0186]],\n",
      "\n",
      "        [[-0.0050,  0.0123,  0.0412],\n",
      "         [ 0.0338, -0.0186,  0.0293],\n",
      "         [ 0.0234,  0.0101,  0.0159],\n",
      "         ...,\n",
      "         [-0.0246,  0.0325,  0.0433],\n",
      "         [-0.0172, -0.0522, -0.0238],\n",
      "         [ 0.0299,  0.0190,  0.0074]]], requires_grad=True)\n",
      "tensor([[[ 0.0069,  0.0071,  0.0039],\n",
      "         [ 0.0289, -0.0141,  0.0485],\n",
      "         [ 0.0286,  0.0246, -0.0329],\n",
      "         ...,\n",
      "         [-0.0017,  0.0113,  0.0343],\n",
      "         [-0.0004, -0.0128, -0.0441],\n",
      "         [-0.0032,  0.0103, -0.0384]],\n",
      "\n",
      "        [[ 0.0136, -0.0123, -0.0464],\n",
      "         [ 0.0014, -0.0152,  0.0159],\n",
      "         [ 0.0168, -0.0044, -0.0121],\n",
      "         ...,\n",
      "         [-0.0070,  0.0281,  0.0311],\n",
      "         [ 0.0224, -0.0075, -0.0459],\n",
      "         [-0.0491, -0.0038, -0.0070]],\n",
      "\n",
      "        [[-0.0080, -0.0080,  0.0031],\n",
      "         [ 0.0105,  0.0033,  0.0152],\n",
      "         [ 0.0052, -0.0018,  0.0017],\n",
      "         ...,\n",
      "         [-0.0020, -0.0142,  0.0123],\n",
      "         [-0.0062, -0.0197, -0.0020],\n",
      "         [ 0.0026, -0.0168,  0.0033]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0112,  0.0136,  0.0118],\n",
      "         [-0.0180,  0.0139, -0.0187],\n",
      "         [-0.0035,  0.0056, -0.0121],\n",
      "         ...,\n",
      "         [-0.0082,  0.0197, -0.0179],\n",
      "         [ 0.0236,  0.0039,  0.0009],\n",
      "         [ 0.0269,  0.0020, -0.0121]],\n",
      "\n",
      "        [[-0.0078, -0.0198, -0.0688],\n",
      "         [ 0.0266,  0.0311,  0.0129],\n",
      "         [ 0.0172,  0.0110, -0.0056],\n",
      "         ...,\n",
      "         [ 0.0148,  0.0377,  0.0299],\n",
      "         [-0.0427,  0.0472, -0.0096],\n",
      "         [-0.0447, -0.0216, -0.0383]],\n",
      "\n",
      "        [[ 0.0431,  0.0016,  0.0190],\n",
      "         [ 0.0026, -0.0035,  0.0218],\n",
      "         [-0.0152,  0.0198, -0.0021],\n",
      "         ...,\n",
      "         [ 0.0075,  0.0253,  0.0375],\n",
      "         [-0.0269, -0.0101, -0.0027],\n",
      "         [-0.0286, -0.0182,  0.0034]]], requires_grad=True)\n",
      "tensor([[[-0.0283, -0.0397,  0.0382],\n",
      "         [-0.0824, -0.0133, -0.0069],\n",
      "         [ 0.0021,  0.0532,  0.0136],\n",
      "         ...,\n",
      "         [-0.0126,  0.0204, -0.0680],\n",
      "         [-0.0293,  0.0168,  0.0396],\n",
      "         [-0.0206,  0.0747,  0.0097]],\n",
      "\n",
      "        [[-0.0110, -0.0783,  0.0555],\n",
      "         [-0.0304,  0.0377,  0.0074],\n",
      "         [ 0.0335,  0.0419,  0.0355],\n",
      "         ...,\n",
      "         [-0.0049,  0.0176,  0.0133],\n",
      "         [ 0.0115, -0.0014, -0.0784],\n",
      "         [-0.0210, -0.0196, -0.0587]],\n",
      "\n",
      "        [[-0.0043,  0.0249,  0.0268],\n",
      "         [ 0.0605, -0.0064,  0.0054],\n",
      "         [ 0.0015,  0.0362,  0.0113],\n",
      "         ...,\n",
      "         [-0.0412, -0.0552, -0.0534],\n",
      "         [ 0.0098, -0.0404, -0.0020],\n",
      "         [-0.0190, -0.0629,  0.0191]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0126, -0.0018, -0.0052],\n",
      "         [ 0.0244, -0.0615, -0.0281],\n",
      "         [ 0.0088,  0.0127,  0.0256],\n",
      "         ...,\n",
      "         [ 0.0197, -0.0692, -0.0146],\n",
      "         [-0.0840,  0.0190, -0.0438],\n",
      "         [ 0.0329,  0.0172, -0.0407]],\n",
      "\n",
      "        [[ 0.0272,  0.0072, -0.0102],\n",
      "         [-0.0086, -0.0507, -0.0281],\n",
      "         [-0.0029,  0.0290, -0.0046],\n",
      "         ...,\n",
      "         [ 0.0489, -0.0047, -0.0161],\n",
      "         [-0.0247, -0.0193, -0.0251],\n",
      "         [-0.0290, -0.0179, -0.0108]],\n",
      "\n",
      "        [[ 0.0376,  0.0370, -0.0039],\n",
      "         [-0.0168, -0.0227, -0.0042],\n",
      "         [-0.0088,  0.0102, -0.0144],\n",
      "         ...,\n",
      "         [-0.0008, -0.0184, -0.0648],\n",
      "         [-0.0284,  0.0017, -0.0005],\n",
      "         [-0.0165,  0.0244, -0.0056]]], requires_grad=True)\n",
      "tensor([[[ 0.0791,  0.0189, -0.0134],\n",
      "         [ 0.0091,  0.0328, -0.0257],\n",
      "         [ 0.0315, -0.0146,  0.0308],\n",
      "         ...,\n",
      "         [-0.0314, -0.0398,  0.0499],\n",
      "         [ 0.0278,  0.0143,  0.0056],\n",
      "         [-0.0198,  0.0314,  0.0021]],\n",
      "\n",
      "        [[ 0.0403, -0.0182,  0.0096],\n",
      "         [ 0.0203, -0.0203, -0.0057],\n",
      "         [ 0.0059, -0.0325, -0.0251],\n",
      "         ...,\n",
      "         [-0.0441,  0.0083, -0.0406],\n",
      "         [ 0.0826, -0.0009,  0.0362],\n",
      "         [-0.0100,  0.0402,  0.0491]],\n",
      "\n",
      "        [[ 0.0133,  0.0142, -0.0165],\n",
      "         [ 0.0071,  0.0081,  0.0046],\n",
      "         [-0.0017, -0.0091, -0.0323],\n",
      "         ...,\n",
      "         [ 0.0088, -0.0059, -0.0058],\n",
      "         [-0.0394,  0.0006, -0.0140],\n",
      "         [-0.0259,  0.0174, -0.0264]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0039, -0.0228,  0.0357],\n",
      "         [ 0.0084,  0.0041, -0.0230],\n",
      "         [ 0.0025,  0.0626,  0.0357],\n",
      "         ...,\n",
      "         [ 0.0191, -0.0348,  0.0372],\n",
      "         [-0.0203, -0.0242, -0.0241],\n",
      "         [ 0.0038, -0.0239, -0.0130]],\n",
      "\n",
      "        [[-0.0072,  0.0025,  0.0003],\n",
      "         [ 0.0079, -0.0414, -0.0559],\n",
      "         [ 0.0230,  0.0797,  0.0494],\n",
      "         ...,\n",
      "         [-0.0308, -0.0192, -0.0745],\n",
      "         [-0.0187,  0.0186, -0.0109],\n",
      "         [ 0.0021,  0.0033,  0.0360]],\n",
      "\n",
      "        [[-0.0416, -0.0725, -0.0139],\n",
      "         [ 0.0129, -0.0362,  0.0492],\n",
      "         [ 0.0133, -0.0002,  0.0168],\n",
      "         ...,\n",
      "         [ 0.0016, -0.0213,  0.0036],\n",
      "         [-0.0023,  0.0278, -0.0208],\n",
      "         [-0.0069, -0.0342, -0.0353]]], requires_grad=True)\n",
      "tensor([[[-0.0018, -0.0117, -0.0397],\n",
      "         [-0.0018, -0.0135,  0.0016],\n",
      "         [ 0.0040, -0.0268,  0.0407],\n",
      "         ...,\n",
      "         [ 0.0333, -0.0540,  0.0848],\n",
      "         [-0.0824,  0.0188, -0.0423],\n",
      "         [-0.0828, -0.0087,  0.0153]],\n",
      "\n",
      "        [[ 0.0012, -0.0106,  0.0245],\n",
      "         [-0.0165, -0.0244,  0.0072],\n",
      "         [-0.0416,  0.0447,  0.0074],\n",
      "         ...,\n",
      "         [ 0.0383, -0.0186, -0.0361],\n",
      "         [ 0.0746, -0.0531, -0.0129],\n",
      "         [ 0.0213, -0.0266, -0.0468]],\n",
      "\n",
      "        [[-0.0248,  0.0052, -0.0415],\n",
      "         [-0.0017,  0.0003,  0.0091],\n",
      "         [ 0.0195,  0.0762,  0.1072],\n",
      "         ...,\n",
      "         [ 0.0273, -0.0190,  0.0348],\n",
      "         [-0.0382, -0.0669, -0.1085],\n",
      "         [ 0.0353,  0.0038, -0.0071]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0470, -0.0054, -0.0965],\n",
      "         [-0.0084,  0.0269, -0.0044],\n",
      "         [-0.0092, -0.0282, -0.0148],\n",
      "         ...,\n",
      "         [-0.0220,  0.0307,  0.0143],\n",
      "         [-0.0489,  0.0357,  0.0369],\n",
      "         [ 0.0642,  0.0034,  0.0434]],\n",
      "\n",
      "        [[ 0.0470, -0.0519, -0.0170],\n",
      "         [-0.0394, -0.0174, -0.0263],\n",
      "         [-0.0206,  0.0148,  0.0710],\n",
      "         ...,\n",
      "         [ 0.0089, -0.0354,  0.0424],\n",
      "         [-0.0176, -0.0670, -0.0198],\n",
      "         [-0.0333,  0.0064, -0.1003]],\n",
      "\n",
      "        [[ 0.0023, -0.0108,  0.0004],\n",
      "         [-0.0144,  0.0150,  0.0536],\n",
      "         [ 0.0298, -0.0031,  0.0003],\n",
      "         ...,\n",
      "         [-0.0231, -0.0170,  0.0273],\n",
      "         [ 0.0203, -0.0401, -0.0098],\n",
      "         [-0.0020, -0.0223,  0.0199]]], requires_grad=True)\n",
      "tensor([[[ 0.0101, -0.0686,  0.0681],\n",
      "         [ 0.0043, -0.0193, -0.0735],\n",
      "         [ 0.0372, -0.0097, -0.0387],\n",
      "         ...,\n",
      "         [-0.0901, -0.0329, -0.0363],\n",
      "         [-0.0363,  0.0298, -0.0002],\n",
      "         [ 0.0174,  0.0264,  0.0245]],\n",
      "\n",
      "        [[-0.0207,  0.0085,  0.0745],\n",
      "         [-0.0250,  0.0183,  0.0028],\n",
      "         [-0.0044,  0.0107,  0.0348],\n",
      "         ...,\n",
      "         [-0.0705,  0.0375,  0.0030],\n",
      "         [-0.0758,  0.0041,  0.0704],\n",
      "         [ 0.0097, -0.0036,  0.0428]],\n",
      "\n",
      "        [[ 0.0008,  0.0453,  0.0202],\n",
      "         [-0.0361, -0.0119,  0.0184],\n",
      "         [-0.0114,  0.0395,  0.0026],\n",
      "         ...,\n",
      "         [ 0.0034,  0.0112, -0.0070],\n",
      "         [ 0.0173,  0.0055, -0.0168],\n",
      "         [-0.0086, -0.0392, -0.0167]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0229, -0.0051, -0.0472],\n",
      "         [ 0.0112, -0.0101, -0.0019],\n",
      "         [ 0.0547, -0.0752, -0.0229],\n",
      "         ...,\n",
      "         [ 0.0005, -0.0032, -0.0591],\n",
      "         [ 0.0092, -0.0450,  0.0072],\n",
      "         [-0.0124,  0.0156, -0.0208]],\n",
      "\n",
      "        [[ 0.0035, -0.0473,  0.0761],\n",
      "         [-0.0034, -0.0606, -0.0363],\n",
      "         [ 0.0094,  0.0308, -0.0171],\n",
      "         ...,\n",
      "         [-0.0070,  0.0167,  0.0110],\n",
      "         [-0.0314, -0.0030, -0.0881],\n",
      "         [-0.0404, -0.0039, -0.0062]],\n",
      "\n",
      "        [[-0.0036,  0.0420,  0.0184],\n",
      "         [-0.0466, -0.0423,  0.0213],\n",
      "         [ 0.0229,  0.0054, -0.0076],\n",
      "         ...,\n",
      "         [-0.0573, -0.0298,  0.0207],\n",
      "         [ 0.0502,  0.0175, -0.0447],\n",
      "         [ 0.0115,  0.0299, -0.0164]]], requires_grad=True)\n",
      "tensor([[[ 1.8615e-02,  1.2600e-03, -1.8860e-02,  ..., -1.9534e-02,\n",
      "          -4.1145e-03,  6.1785e-03],\n",
      "         [ 1.9743e-02, -8.9467e-03, -1.3789e-02,  ..., -1.4614e-02,\n",
      "           1.1242e-03, -2.2354e-02],\n",
      "         [ 1.1047e-02, -1.0328e-02,  2.5852e-02,  ...,  5.2964e-03,\n",
      "           1.0965e-02, -4.6080e-03],\n",
      "         ...,\n",
      "         [ 7.5855e-03, -1.4378e-02,  2.5380e-05,  ..., -8.1122e-03,\n",
      "          -4.4708e-03,  5.8057e-03],\n",
      "         [ 1.6723e-02,  2.0350e-02,  3.3125e-02,  ...,  6.3217e-03,\n",
      "          -2.0453e-02,  2.8202e-03],\n",
      "         [-6.7103e-03,  1.5214e-02,  1.0478e-02,  ...,  2.8220e-03,\n",
      "           1.0638e-02, -1.7940e-02]],\n",
      "\n",
      "        [[ 2.0309e-02, -8.1899e-03,  1.2212e-02,  ..., -1.7369e-02,\n",
      "          -3.7902e-03, -4.6827e-03],\n",
      "         [-4.7724e-03,  4.1692e-03,  1.6495e-02,  ...,  7.9749e-03,\n",
      "          -3.5219e-03,  5.7216e-03],\n",
      "         [-5.1462e-03,  2.3410e-03, -6.6280e-03,  ...,  1.0940e-03,\n",
      "           5.0549e-03,  3.0747e-03],\n",
      "         ...,\n",
      "         [-6.4097e-03,  2.6271e-03, -5.6202e-03,  ..., -7.0633e-03,\n",
      "          -4.6979e-03,  6.3473e-03],\n",
      "         [-4.1124e-03, -8.0421e-03,  7.5732e-03,  ..., -1.8086e-02,\n",
      "          -8.5955e-03, -5.6319e-03],\n",
      "         [ 4.5471e-03, -3.5816e-03, -1.1597e-02,  ..., -4.3465e-03,\n",
      "           7.5635e-03, -1.6380e-02]],\n",
      "\n",
      "        [[ 5.6055e-02, -1.3498e-02, -3.7784e-03,  ...,  7.3066e-03,\n",
      "          -8.7605e-03, -8.3058e-05],\n",
      "         [-1.3628e-02,  3.3423e-02,  2.4683e-02,  ...,  5.0014e-02,\n",
      "           1.0678e-02,  1.4256e-02],\n",
      "         [-3.2640e-02,  4.8726e-03,  2.6919e-02,  ...,  1.6666e-02,\n",
      "          -7.3202e-03,  1.0400e-02],\n",
      "         ...,\n",
      "         [-3.3465e-03,  2.5332e-04,  1.3666e-02,  ..., -1.1905e-02,\n",
      "          -3.2445e-03, -2.2505e-02],\n",
      "         [-6.1642e-03,  1.9970e-02,  2.2463e-02,  ...,  5.6992e-04,\n",
      "           8.3204e-03, -3.2199e-03],\n",
      "         [ 1.0558e-02, -7.5355e-03,  1.1108e-02,  ..., -1.0524e-02,\n",
      "           1.5334e-02,  1.9952e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0087e-03, -2.8715e-02,  1.7550e-02,  ...,  5.4975e-03,\n",
      "           2.0669e-02, -4.3966e-03],\n",
      "         [-3.6811e-03, -3.1535e-03, -1.4435e-03,  ...,  8.8767e-03,\n",
      "           4.1605e-04,  1.2413e-02],\n",
      "         [-4.8038e-03,  3.4959e-03,  5.0327e-03,  ...,  1.5796e-02,\n",
      "           8.1800e-03,  3.3671e-04],\n",
      "         ...,\n",
      "         [ 9.4175e-03, -5.0770e-03,  4.2114e-03,  ...,  1.4553e-02,\n",
      "           1.3042e-02,  1.4624e-02],\n",
      "         [ 1.7419e-02, -1.5450e-03,  1.0099e-02,  ...,  2.9794e-03,\n",
      "          -2.1356e-03,  9.6464e-03],\n",
      "         [ 3.4437e-04,  7.9243e-03, -2.4557e-02,  ...,  2.7715e-03,\n",
      "          -4.4957e-03,  1.6468e-02]],\n",
      "\n",
      "        [[-1.3822e-02, -5.6900e-03, -3.5804e-03,  ...,  2.7798e-04,\n",
      "          -4.3720e-03, -4.2237e-03],\n",
      "         [ 2.4280e-03,  7.7308e-04, -1.6251e-03,  ...,  8.2753e-03,\n",
      "           9.0398e-04,  4.0731e-03],\n",
      "         [ 2.2053e-03, -2.8330e-03, -9.3240e-03,  ..., -7.5403e-03,\n",
      "          -5.0911e-03, -6.9810e-03],\n",
      "         ...,\n",
      "         [-5.8343e-03,  8.7719e-03, -2.7891e-03,  ...,  8.5309e-03,\n",
      "           2.2812e-03, -4.4045e-04],\n",
      "         [ 3.5244e-03,  1.0167e-03,  1.3598e-02,  ..., -1.8001e-03,\n",
      "          -1.8705e-03, -4.1206e-03],\n",
      "         [-4.2760e-03,  8.2143e-03, -5.4696e-03,  ..., -4.1438e-03,\n",
      "           3.5414e-03,  6.0542e-03]],\n",
      "\n",
      "        [[ 6.2380e-03,  4.3730e-03, -9.0077e-04,  ...,  3.8148e-03,\n",
      "          -5.6398e-03,  1.5407e-03],\n",
      "         [ 4.1202e-03, -3.5493e-03, -3.1023e-03,  ...,  4.3910e-03,\n",
      "           4.5911e-04,  1.3516e-02],\n",
      "         [-6.1404e-03, -1.7364e-03,  5.9200e-04,  ...,  2.9593e-03,\n",
      "           7.0858e-03, -5.5923e-03],\n",
      "         ...,\n",
      "         [-1.9667e-03, -9.4095e-03,  2.7899e-03,  ...,  2.5973e-03,\n",
      "          -3.2378e-04,  6.0301e-03],\n",
      "         [-9.5052e-04,  4.9855e-03, -5.4467e-04,  ...,  7.6324e-03,\n",
      "          -2.9808e-03,  4.5313e-03],\n",
      "         [ 1.5805e-03, -2.1811e-03, -1.3793e-03,  ..., -1.3646e-03,\n",
      "          -4.1570e-03, -6.5039e-04]]], requires_grad=True)\n",
      "tensor([[[ 0.0675,  0.0271, -0.0017],\n",
      "         [ 0.0212, -0.0375,  0.0422],\n",
      "         [-0.0961, -0.0449,  0.0022],\n",
      "         ...,\n",
      "         [ 0.0346, -0.0036, -0.0757],\n",
      "         [-0.0368, -0.0150,  0.0159],\n",
      "         [-0.0697, -0.0571, -0.0624]],\n",
      "\n",
      "        [[ 0.0090, -0.0122, -0.0327],\n",
      "         [ 0.0356,  0.0174,  0.0383],\n",
      "         [-0.0954, -0.0126,  0.0629],\n",
      "         ...,\n",
      "         [ 0.0434,  0.0116,  0.0112],\n",
      "         [-0.0418, -0.0160, -0.0283],\n",
      "         [-0.0332,  0.0240, -0.0293]],\n",
      "\n",
      "        [[-0.0148,  0.1058, -0.0496],\n",
      "         [ 0.0500, -0.0003, -0.0095],\n",
      "         [-0.0185,  0.0693, -0.0023],\n",
      "         ...,\n",
      "         [ 0.0279, -0.0587,  0.0223],\n",
      "         [-0.0225, -0.0756,  0.0329],\n",
      "         [ 0.0430, -0.0049,  0.0731]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0299, -0.1163, -0.0636],\n",
      "         [-0.0040, -0.0333, -0.0336],\n",
      "         [-0.0579,  0.0672,  0.0605],\n",
      "         ...,\n",
      "         [-0.0034,  0.0279, -0.0315],\n",
      "         [ 0.1079,  0.0681,  0.1439],\n",
      "         [ 0.0173,  0.0031, -0.0381]],\n",
      "\n",
      "        [[-0.0385, -0.0557,  0.0058],\n",
      "         [-0.0232,  0.0041,  0.0537],\n",
      "         [ 0.0705,  0.0471, -0.0055],\n",
      "         ...,\n",
      "         [-0.0187,  0.0017,  0.0307],\n",
      "         [-0.0481, -0.0165,  0.0101],\n",
      "         [-0.0511, -0.0494, -0.0832]],\n",
      "\n",
      "        [[-0.0155, -0.0052, -0.1057],\n",
      "         [-0.0265, -0.0291, -0.0569],\n",
      "         [ 0.0682, -0.0047, -0.0591],\n",
      "         ...,\n",
      "         [-0.0565,  0.0298, -0.0080],\n",
      "         [-0.0167, -0.0266,  0.0037],\n",
      "         [-0.0051, -0.0093,  0.0464]]], requires_grad=True)\n",
      "tensor([[[-0.0589,  0.0188,  0.0461],\n",
      "         [-0.0291, -0.0270,  0.0244],\n",
      "         [-0.0373,  0.0318, -0.0018],\n",
      "         ...,\n",
      "         [ 0.0185, -0.0047, -0.0519],\n",
      "         [-0.0015,  0.0748, -0.0250],\n",
      "         [-0.0244, -0.0096,  0.0033]],\n",
      "\n",
      "        [[ 0.0383,  0.0144,  0.0588],\n",
      "         [ 0.0337,  0.0423,  0.0220],\n",
      "         [-0.0451,  0.0394,  0.0226],\n",
      "         ...,\n",
      "         [-0.0208, -0.0593, -0.1150],\n",
      "         [-0.0340,  0.0261, -0.0093],\n",
      "         [-0.0039, -0.0437, -0.0071]],\n",
      "\n",
      "        [[ 0.0258, -0.0404, -0.0161],\n",
      "         [-0.0434,  0.0300, -0.0105],\n",
      "         [ 0.0128,  0.0696, -0.0004],\n",
      "         ...,\n",
      "         [-0.0308,  0.0212, -0.0917],\n",
      "         [ 0.0420, -0.0106, -0.0635],\n",
      "         [-0.0485, -0.0405,  0.0289]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0196, -0.0110, -0.0674],\n",
      "         [ 0.0247, -0.0156,  0.0006],\n",
      "         [-0.0388, -0.0114, -0.0636],\n",
      "         ...,\n",
      "         [-0.0177, -0.0014,  0.0113],\n",
      "         [ 0.0099,  0.0090, -0.0505],\n",
      "         [ 0.0121, -0.0020,  0.0142]],\n",
      "\n",
      "        [[-0.0547, -0.1120, -0.0379],\n",
      "         [-0.0495,  0.0181, -0.0650],\n",
      "         [ 0.0462, -0.0267, -0.0195],\n",
      "         ...,\n",
      "         [-0.0286,  0.0842,  0.0620],\n",
      "         [-0.0077, -0.0180,  0.0111],\n",
      "         [-0.0383,  0.0070, -0.0284]],\n",
      "\n",
      "        [[ 0.0164, -0.0504, -0.0741],\n",
      "         [ 0.0487,  0.0424,  0.0245],\n",
      "         [ 0.0432,  0.0547, -0.0292],\n",
      "         ...,\n",
      "         [ 0.0795,  0.0280,  0.0598],\n",
      "         [-0.0334, -0.0515,  0.0564],\n",
      "         [-0.0645,  0.0252,  0.0010]]], requires_grad=True)\n",
      "tensor([[[ 1.7321e-03, -5.2330e-02, -4.9788e-02],\n",
      "         [-9.0076e-02,  1.2646e-02, -4.4447e-03],\n",
      "         [ 2.6418e-02,  3.4134e-02, -4.4907e-02],\n",
      "         ...,\n",
      "         [ 2.1478e-02, -4.8379e-02,  5.3488e-03],\n",
      "         [-4.3433e-02,  2.8386e-02,  4.6140e-02],\n",
      "         [ 1.0504e-02, -5.0798e-02, -9.8503e-02]],\n",
      "\n",
      "        [[ 1.4563e-02, -3.4626e-02, -4.4989e-02],\n",
      "         [ 1.7130e-02, -9.8054e-03, -8.8235e-02],\n",
      "         [ 3.8711e-02, -6.7342e-02, -7.9060e-02],\n",
      "         ...,\n",
      "         [-4.7263e-04, -8.8901e-03, -2.0897e-02],\n",
      "         [-4.1802e-02, -1.4327e-02,  2.3334e-02],\n",
      "         [-3.6116e-02, -2.2697e-02, -4.9599e-02]],\n",
      "\n",
      "        [[-1.1428e-02,  7.5329e-02,  3.8447e-02],\n",
      "         [-9.8783e-02, -4.3224e-02,  1.6718e-02],\n",
      "         [-8.6981e-03,  1.5888e-02, -3.7192e-02],\n",
      "         ...,\n",
      "         [ 2.7621e-03,  5.7515e-02, -6.1839e-02],\n",
      "         [ 2.8851e-04,  2.5212e-02, -5.9290e-02],\n",
      "         [-6.8581e-02,  2.9171e-02, -1.5059e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0936e-01,  1.6086e-02,  2.1082e-02],\n",
      "         [-5.3313e-02,  6.0220e-02,  8.8753e-02],\n",
      "         [ 1.0775e-01, -6.2142e-02,  2.1005e-02],\n",
      "         ...,\n",
      "         [-3.5433e-02, -5.9972e-02, -4.0184e-02],\n",
      "         [ 5.3213e-02, -5.1993e-02, -1.2575e-03],\n",
      "         [-1.0406e-01, -6.4004e-02, -1.0381e-02]],\n",
      "\n",
      "        [[ 3.6101e-03, -4.6137e-02,  5.8548e-02],\n",
      "         [ 5.5949e-02, -1.0092e-01, -4.5372e-02],\n",
      "         [ 6.5234e-03, -7.9435e-03, -1.9216e-02],\n",
      "         ...,\n",
      "         [ 3.7420e-02, -2.3589e-03,  1.1924e-02],\n",
      "         [ 3.5744e-03, -3.9537e-02, -6.8583e-04],\n",
      "         [ 5.2189e-02, -3.7365e-02, -2.8939e-02]],\n",
      "\n",
      "        [[-6.3964e-02, -5.2771e-03, -3.2114e-02],\n",
      "         [ 9.7170e-02, -7.1032e-02, -2.9882e-02],\n",
      "         [ 7.1185e-03,  2.4829e-02, -2.6913e-02],\n",
      "         ...,\n",
      "         [-8.7394e-02, -9.9480e-02, -2.4108e-02],\n",
      "         [ 1.5204e-02,  3.8033e-02, -6.7519e-05],\n",
      "         [ 1.0299e-02,  1.7746e-02, -5.1560e-02]]], requires_grad=True)\n",
      "tensor([[[-0.0653, -0.0504, -0.1064],\n",
      "         [-0.0365, -0.0040,  0.0137],\n",
      "         [ 0.0120, -0.1010,  0.0089],\n",
      "         ...,\n",
      "         [-0.0103, -0.0450,  0.0376],\n",
      "         [ 0.0355, -0.0968, -0.0297],\n",
      "         [-0.0050, -0.0053, -0.1039]],\n",
      "\n",
      "        [[-0.0013, -0.0640,  0.0002],\n",
      "         [ 0.0552,  0.0134, -0.0801],\n",
      "         [-0.0327, -0.0045, -0.0280],\n",
      "         ...,\n",
      "         [ 0.0040, -0.0159, -0.0230],\n",
      "         [-0.0390,  0.0356, -0.0267],\n",
      "         [-0.0584, -0.0411,  0.0456]],\n",
      "\n",
      "        [[ 0.0226,  0.0147,  0.0611],\n",
      "         [-0.0413, -0.0102,  0.0466],\n",
      "         [-0.0033,  0.1204, -0.0439],\n",
      "         ...,\n",
      "         [-0.0256, -0.0354,  0.0986],\n",
      "         [-0.0115,  0.0232,  0.0292],\n",
      "         [ 0.0078, -0.0339,  0.0146]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0317, -0.0063, -0.0562],\n",
      "         [-0.0138,  0.0364,  0.0121],\n",
      "         [ 0.0107, -0.1033, -0.0661],\n",
      "         ...,\n",
      "         [ 0.0163,  0.0113,  0.0522],\n",
      "         [ 0.0204,  0.0081, -0.0047],\n",
      "         [-0.0123, -0.0077, -0.0230]],\n",
      "\n",
      "        [[-0.0275, -0.0063,  0.0437],\n",
      "         [-0.0113,  0.0355, -0.0781],\n",
      "         [-0.0140, -0.1252, -0.0240],\n",
      "         ...,\n",
      "         [-0.1183, -0.0481, -0.0202],\n",
      "         [-0.0091,  0.0921,  0.0770],\n",
      "         [-0.0006, -0.0271,  0.0306]],\n",
      "\n",
      "        [[ 0.0481,  0.0081,  0.0696],\n",
      "         [ 0.0594,  0.0190, -0.0362],\n",
      "         [ 0.0727,  0.0164,  0.1036],\n",
      "         ...,\n",
      "         [-0.0090, -0.0731, -0.0610],\n",
      "         [-0.0668,  0.0355, -0.1272],\n",
      "         [ 0.0642,  0.0336,  0.0499]]], requires_grad=True)\n",
      "tensor([[[ 8.8774e-04, -1.2186e-02, -9.8505e-03],\n",
      "         [ 2.4885e-03,  5.2070e-02, -1.2254e-02],\n",
      "         [-3.8267e-02, -1.8540e-02,  3.9405e-02],\n",
      "         ...,\n",
      "         [ 8.7326e-03, -2.2149e-02,  5.3099e-02],\n",
      "         [ 5.2095e-02, -6.0991e-02, -3.0997e-03],\n",
      "         [ 5.0086e-02, -2.6984e-02, -4.6377e-02]],\n",
      "\n",
      "        [[-9.2271e-02,  2.9795e-02, -1.5715e-02],\n",
      "         [ 3.1600e-03, -4.2560e-03,  8.3805e-03],\n",
      "         [ 2.0548e-02,  1.7969e-02,  1.7590e-02],\n",
      "         ...,\n",
      "         [-8.9691e-03, -5.2014e-02,  2.9543e-02],\n",
      "         [ 3.7475e-02, -3.0362e-02, -6.7282e-03],\n",
      "         [ 3.6233e-03, -6.2509e-03,  8.1641e-03]],\n",
      "\n",
      "        [[ 1.6824e-02, -6.8634e-03, -1.2852e-02],\n",
      "         [ 3.2568e-03,  1.1041e-02,  4.7951e-02],\n",
      "         [-1.5600e-02, -5.2452e-02,  3.2882e-02],\n",
      "         ...,\n",
      "         [ 1.5976e-02, -2.2011e-02,  2.2686e-02],\n",
      "         [ 2.5383e-02,  6.2332e-05, -1.7377e-03],\n",
      "         [ 6.7328e-03,  5.1812e-02,  6.0296e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.9679e-02,  4.1406e-02,  5.2320e-02],\n",
      "         [ 4.7057e-02, -1.1596e-02, -8.5379e-02],\n",
      "         [ 4.9856e-02,  4.5139e-02, -5.6754e-02],\n",
      "         ...,\n",
      "         [ 5.2012e-04, -4.5703e-02,  5.8364e-02],\n",
      "         [ 1.8791e-02, -7.2930e-03, -9.4933e-02],\n",
      "         [ 5.3168e-03, -2.4509e-02,  8.8550e-04]],\n",
      "\n",
      "        [[ 7.0095e-02, -5.4661e-02, -2.7169e-02],\n",
      "         [ 2.1377e-02, -3.3619e-02, -3.3043e-02],\n",
      "         [-9.2748e-02,  1.2966e-01, -1.2737e-02],\n",
      "         ...,\n",
      "         [-9.9321e-03,  1.0498e-04,  2.1041e-02],\n",
      "         [ 3.6400e-02, -5.2430e-02, -4.7870e-02],\n",
      "         [-3.2487e-02, -6.3622e-02, -8.6166e-02]],\n",
      "\n",
      "        [[ 1.9729e-02,  5.4250e-02, -1.0530e-01],\n",
      "         [-5.2714e-02,  4.0946e-02, -1.5608e-02],\n",
      "         [-4.0548e-02, -8.0880e-02, -1.7811e-02],\n",
      "         ...,\n",
      "         [-2.0279e-02, -2.7898e-02,  9.4605e-02],\n",
      "         [ 4.4847e-02,  5.0252e-03,  8.1109e-02],\n",
      "         [ 4.3066e-02, -2.8377e-02,  4.8282e-03]]], requires_grad=True)\n",
      "tensor([[[ 0.0285,  0.0584, -0.0073],\n",
      "         [ 0.0390,  0.0662,  0.0367],\n",
      "         [ 0.0247,  0.0131, -0.0479],\n",
      "         ...,\n",
      "         [ 0.1303,  0.0039,  0.0198],\n",
      "         [ 0.0066,  0.0169, -0.0620],\n",
      "         [ 0.0054,  0.0277, -0.0140]],\n",
      "\n",
      "        [[-0.0188,  0.0152,  0.0135],\n",
      "         [-0.0355,  0.0141, -0.0518],\n",
      "         [-0.0525, -0.0195, -0.0110],\n",
      "         ...,\n",
      "         [ 0.0047, -0.0152, -0.0088],\n",
      "         [ 0.0375, -0.0329,  0.0428],\n",
      "         [-0.0038,  0.0240,  0.0456]],\n",
      "\n",
      "        [[-0.0635,  0.0172,  0.0028],\n",
      "         [-0.0243, -0.0782,  0.0261],\n",
      "         [-0.0139,  0.0031,  0.0293],\n",
      "         ...,\n",
      "         [ 0.0815,  0.1012, -0.1423],\n",
      "         [ 0.0459, -0.0354, -0.1068],\n",
      "         [ 0.0485,  0.0308,  0.0629]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0521,  0.0186,  0.0120],\n",
      "         [ 0.0359,  0.0443, -0.0186],\n",
      "         [-0.0164, -0.0363, -0.0295],\n",
      "         ...,\n",
      "         [-0.0315,  0.0292, -0.0166],\n",
      "         [-0.0207, -0.0137, -0.0701],\n",
      "         [-0.0098, -0.0079, -0.0136]],\n",
      "\n",
      "        [[-0.0495,  0.0455,  0.0265],\n",
      "         [-0.1061, -0.0814,  0.0400],\n",
      "         [ 0.0224, -0.0397, -0.0481],\n",
      "         ...,\n",
      "         [-0.1107, -0.0098, -0.0556],\n",
      "         [-0.1557,  0.0999, -0.0336],\n",
      "         [ 0.0853, -0.0027,  0.0421]],\n",
      "\n",
      "        [[ 0.0230,  0.0044,  0.0412],\n",
      "         [-0.0034, -0.0726,  0.0528],\n",
      "         [-0.0069, -0.0610,  0.0080],\n",
      "         ...,\n",
      "         [-0.0146, -0.0333, -0.0658],\n",
      "         [-0.1091, -0.0862, -0.0183],\n",
      "         [-0.0072, -0.0064, -0.0752]]], requires_grad=True)\n",
      "tensor([[[ 6.6912e-03,  9.4862e-03,  5.1938e-03,  ...,  2.0012e-02,\n",
      "           6.4508e-03,  6.9127e-03],\n",
      "         [-2.2005e-02, -1.0335e-02,  3.9328e-03,  ..., -4.6120e-03,\n",
      "           2.1782e-03,  1.5934e-03],\n",
      "         [ 1.5358e-02,  1.5575e-02,  1.5323e-02,  ..., -5.8251e-04,\n",
      "           1.3859e-03,  1.8321e-03],\n",
      "         ...,\n",
      "         [ 1.4822e-02,  6.7946e-03,  1.0035e-02,  ...,  1.2312e-03,\n",
      "          -5.5629e-05,  3.8278e-03],\n",
      "         [ 3.2602e-02,  2.0979e-02,  1.3425e-03,  ..., -3.7158e-03,\n",
      "           2.9521e-02,  1.8590e-02],\n",
      "         [-1.2725e-02, -3.8836e-04,  9.5646e-03,  ...,  1.4464e-02,\n",
      "          -3.8380e-03, -1.0243e-02]],\n",
      "\n",
      "        [[ 1.2718e-02, -9.9171e-03,  2.5449e-03,  ...,  5.3129e-03,\n",
      "           1.5580e-02, -2.0263e-02],\n",
      "         [-2.3093e-02,  1.1138e-02, -3.6276e-03,  ..., -1.4177e-03,\n",
      "          -1.9051e-02, -2.3410e-02],\n",
      "         [-1.0625e-02, -2.8954e-03,  2.0141e-04,  ...,  1.0764e-02,\n",
      "           1.3608e-02, -8.9348e-03],\n",
      "         ...,\n",
      "         [-1.7277e-02, -2.1199e-03, -2.0525e-03,  ..., -2.1011e-02,\n",
      "           1.2484e-02, -8.0160e-03],\n",
      "         [ 3.1314e-03,  2.6512e-03, -1.2309e-02,  ...,  3.7476e-03,\n",
      "          -9.5387e-03, -9.4337e-03],\n",
      "         [ 2.2700e-03, -1.1739e-02, -3.5589e-03,  ...,  1.2538e-02,\n",
      "           1.7877e-03,  1.0624e-02]],\n",
      "\n",
      "        [[ 2.9773e-03, -7.2428e-03, -1.4581e-02,  ..., -7.0821e-03,\n",
      "          -3.3469e-03, -8.2478e-03],\n",
      "         [-1.9328e-03,  8.5453e-03, -1.5818e-03,  ...,  8.9596e-03,\n",
      "           1.0820e-02,  4.2351e-03],\n",
      "         [ 3.4340e-03, -8.1278e-03,  8.4902e-03,  ...,  6.0593e-03,\n",
      "          -2.1743e-02,  1.3517e-02],\n",
      "         ...,\n",
      "         [ 1.7484e-02, -7.6776e-03,  2.3085e-04,  ..., -3.2070e-03,\n",
      "           7.5997e-04,  9.2723e-03],\n",
      "         [ 1.0317e-02, -4.1453e-03,  3.3824e-03,  ..., -1.5970e-02,\n",
      "          -2.5940e-02,  1.8110e-02],\n",
      "         [ 3.3242e-03,  1.5548e-02,  1.9050e-03,  ...,  6.3070e-03,\n",
      "           1.6999e-02,  5.5154e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.0802e-03, -1.2096e-02, -7.1386e-03,  ...,  2.1653e-03,\n",
      "           1.9786e-03, -1.0272e-03],\n",
      "         [ 1.5837e-02,  1.5129e-02,  3.5914e-03,  ...,  1.4726e-02,\n",
      "          -7.0118e-03,  7.5908e-03],\n",
      "         [-5.9475e-04, -1.1151e-02,  7.4838e-03,  ..., -7.9188e-03,\n",
      "          -4.2881e-03,  1.8070e-02],\n",
      "         ...,\n",
      "         [-9.6370e-03,  4.7953e-03,  2.3047e-03,  ..., -1.0117e-03,\n",
      "           9.9448e-03, -1.2324e-02],\n",
      "         [ 4.6192e-03, -3.3262e-02, -2.5994e-02,  ...,  1.9260e-02,\n",
      "           6.8376e-03,  1.9749e-03],\n",
      "         [-5.0989e-03,  7.0952e-03, -7.5102e-03,  ..., -1.6529e-02,\n",
      "          -2.2807e-02, -3.4262e-03]],\n",
      "\n",
      "        [[-7.6943e-05, -1.8081e-03, -7.9956e-03,  ..., -2.2049e-03,\n",
      "          -8.4826e-03, -1.2933e-02],\n",
      "         [ 1.5092e-02, -3.0231e-03, -5.3876e-03,  ...,  6.5408e-03,\n",
      "           4.1270e-03,  1.4475e-02],\n",
      "         [ 1.2216e-02,  2.0930e-02,  3.4981e-02,  ...,  1.6481e-02,\n",
      "           1.9191e-02,  8.7104e-03],\n",
      "         ...,\n",
      "         [ 9.9811e-03,  1.0598e-02, -1.2014e-02,  ...,  2.9568e-02,\n",
      "           1.4773e-03,  1.0784e-03],\n",
      "         [ 1.6801e-02,  1.5880e-02, -1.9894e-02,  ..., -1.2575e-02,\n",
      "           9.4885e-03, -2.3467e-02],\n",
      "         [ 9.6999e-03, -2.7268e-04, -5.1496e-03,  ...,  4.4388e-03,\n",
      "           1.0541e-02,  1.6444e-02]],\n",
      "\n",
      "        [[-9.1329e-03,  3.1118e-02, -1.2593e-02,  ..., -7.1385e-03,\n",
      "          -1.4499e-02,  2.9942e-02],\n",
      "         [ 9.7036e-03, -3.5655e-02,  3.3417e-02,  ...,  2.3502e-02,\n",
      "           5.4679e-02, -4.6493e-02],\n",
      "         [-7.2593e-03,  3.4860e-02, -2.3217e-02,  ...,  7.5821e-03,\n",
      "          -4.2961e-03,  2.0399e-02],\n",
      "         ...,\n",
      "         [ 1.9667e-02, -1.2814e-02,  5.2340e-03,  ...,  1.1013e-02,\n",
      "           2.1505e-02, -7.5797e-03],\n",
      "         [ 2.2095e-03, -2.8221e-03, -1.2292e-02,  ...,  1.9595e-02,\n",
      "           3.7447e-03, -1.6341e-02],\n",
      "         [ 1.3283e-02, -1.9314e-02, -1.1080e-03,  ...,  9.8499e-03,\n",
      "          -2.2667e-03,  1.8502e-02]]], requires_grad=True)\n",
      "tensor([[[-0.0174,  0.0296,  0.0092],\n",
      "         [-0.0288,  0.0400,  0.0987],\n",
      "         [-0.0359,  0.0593, -0.1035],\n",
      "         ...,\n",
      "         [ 0.1023, -0.0893,  0.0187],\n",
      "         [ 0.0067,  0.0149,  0.0064],\n",
      "         [-0.0385, -0.0373, -0.0169]],\n",
      "\n",
      "        [[ 0.0250, -0.0132,  0.0131],\n",
      "         [-0.0256,  0.0385, -0.0731],\n",
      "         [ 0.0344,  0.0106,  0.0403],\n",
      "         ...,\n",
      "         [-0.0062,  0.0087, -0.0464],\n",
      "         [ 0.0252, -0.0001,  0.0495],\n",
      "         [ 0.0021, -0.0364, -0.0258]],\n",
      "\n",
      "        [[ 0.0133,  0.0079,  0.0047],\n",
      "         [ 0.0133,  0.0745,  0.0882],\n",
      "         [-0.0067,  0.0307, -0.0202],\n",
      "         ...,\n",
      "         [-0.0104,  0.0166,  0.0194],\n",
      "         [ 0.0343,  0.0270,  0.0070],\n",
      "         [ 0.0302,  0.0002, -0.0177]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0106,  0.0277,  0.0548],\n",
      "         [-0.0689,  0.0123,  0.0811],\n",
      "         [-0.0309,  0.0193, -0.0427],\n",
      "         ...,\n",
      "         [ 0.0271,  0.0361, -0.0416],\n",
      "         [-0.0898, -0.0616, -0.0443],\n",
      "         [-0.0481, -0.0190,  0.0341]],\n",
      "\n",
      "        [[-0.0264, -0.0468, -0.0098],\n",
      "         [-0.0170, -0.0290, -0.0394],\n",
      "         [ 0.0304, -0.0161,  0.0452],\n",
      "         ...,\n",
      "         [-0.0499, -0.0035, -0.0773],\n",
      "         [-0.0787,  0.0213,  0.0419],\n",
      "         [-0.0141,  0.0059,  0.0475]],\n",
      "\n",
      "        [[-0.0451,  0.0438, -0.0146],\n",
      "         [ 0.0212,  0.0249,  0.0086],\n",
      "         [ 0.0385,  0.0151,  0.0120],\n",
      "         ...,\n",
      "         [ 0.0906,  0.0913, -0.0631],\n",
      "         [-0.0364,  0.0134, -0.0607],\n",
      "         [-0.0361, -0.0637,  0.0253]]], requires_grad=True)\n",
      "tensor([[[-0.0216,  0.0014, -0.0765],\n",
      "         [-0.0627, -0.0359, -0.0549],\n",
      "         [ 0.0052,  0.0122, -0.0260],\n",
      "         ...,\n",
      "         [ 0.0655, -0.0108, -0.0856],\n",
      "         [ 0.1044, -0.0090, -0.0019],\n",
      "         [-0.0445,  0.0574,  0.0979]],\n",
      "\n",
      "        [[ 0.1536, -0.1381,  0.1388],\n",
      "         [ 0.0701, -0.0593,  0.0415],\n",
      "         [ 0.0189, -0.0093,  0.0109],\n",
      "         ...,\n",
      "         [ 0.0063, -0.0835, -0.0634],\n",
      "         [-0.0187,  0.0398, -0.0018],\n",
      "         [ 0.0262,  0.0438,  0.0091]],\n",
      "\n",
      "        [[-0.0168, -0.0332, -0.0463],\n",
      "         [ 0.0006,  0.0077,  0.0396],\n",
      "         [ 0.0419,  0.0492,  0.0560],\n",
      "         ...,\n",
      "         [-0.0340, -0.0377,  0.1074],\n",
      "         [ 0.0689,  0.0636, -0.0431],\n",
      "         [ 0.0093, -0.0392,  0.0966]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1830,  0.1389, -0.0276],\n",
      "         [ 0.0516,  0.0132,  0.0298],\n",
      "         [-0.0253,  0.0381,  0.0237],\n",
      "         ...,\n",
      "         [-0.0260, -0.0208,  0.1354],\n",
      "         [-0.0922, -0.0362, -0.0203],\n",
      "         [-0.0188,  0.0449,  0.1030]],\n",
      "\n",
      "        [[-0.1011, -0.0378, -0.0799],\n",
      "         [ 0.0141,  0.0142, -0.0484],\n",
      "         [ 0.0219,  0.0477,  0.0469],\n",
      "         ...,\n",
      "         [ 0.0013,  0.0332, -0.0744],\n",
      "         [ 0.0368,  0.0293, -0.0190],\n",
      "         [-0.0020,  0.0664, -0.0199]],\n",
      "\n",
      "        [[-0.0216,  0.0245, -0.0041],\n",
      "         [ 0.0027,  0.0294, -0.0030],\n",
      "         [-0.0401, -0.0353,  0.0414],\n",
      "         ...,\n",
      "         [-0.0387,  0.0124,  0.0880],\n",
      "         [ 0.0034,  0.0635, -0.0403],\n",
      "         [ 0.0360,  0.0250,  0.1053]]], requires_grad=True)\n",
      "tensor([[[-2.4989e-02,  3.5903e-02,  6.4427e-04],\n",
      "         [ 7.7854e-02,  3.6567e-03,  1.0766e-02],\n",
      "         [-6.4767e-02,  7.1655e-02,  5.0834e-02],\n",
      "         ...,\n",
      "         [-9.8090e-03,  5.8137e-02,  6.0732e-02],\n",
      "         [ 3.3052e-02,  1.3235e-02, -7.1083e-02],\n",
      "         [ 1.6016e-01, -5.4093e-02, -6.0025e-02]],\n",
      "\n",
      "        [[-6.5760e-02, -4.0411e-02,  6.4486e-02],\n",
      "         [-8.0433e-02, -7.2821e-02, -1.2230e-01],\n",
      "         [-4.0983e-02, -6.6474e-02,  6.1077e-03],\n",
      "         ...,\n",
      "         [-1.5818e-02,  5.6549e-02,  4.5038e-02],\n",
      "         [-6.1994e-02, -7.7640e-02, -6.1995e-02],\n",
      "         [ 4.1955e-03,  7.0112e-02,  5.8180e-03]],\n",
      "\n",
      "        [[ 7.4290e-02, -2.9888e-03, -7.1484e-02],\n",
      "         [-5.6638e-03, -2.2838e-02, -1.1461e-02],\n",
      "         [-4.9803e-02, -1.0514e-02, -7.0614e-02],\n",
      "         ...,\n",
      "         [ 1.0934e-02, -6.4980e-02, -1.8896e-02],\n",
      "         [ 2.2342e-02, -8.0941e-02, -1.3696e-02],\n",
      "         [-7.3655e-02, -8.4960e-02, -1.2278e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9679e-02,  8.7297e-02,  1.9497e-02],\n",
      "         [ 1.5822e-01, -6.4195e-02,  2.9020e-02],\n",
      "         [-1.2304e-01,  1.5990e-03,  4.2157e-03],\n",
      "         ...,\n",
      "         [ 5.1415e-02,  7.2194e-02, -7.8020e-02],\n",
      "         [ 7.7518e-02,  1.6755e-02, -3.2132e-02],\n",
      "         [-4.2393e-02,  2.8081e-02,  2.5472e-02]],\n",
      "\n",
      "        [[ 5.1371e-02,  1.7430e-02, -9.6470e-03],\n",
      "         [-1.6518e-02,  1.5434e-02,  4.6686e-03],\n",
      "         [-3.9152e-02,  1.7814e-02,  8.2072e-02],\n",
      "         ...,\n",
      "         [-2.8240e-02,  4.0646e-03, -2.2437e-03],\n",
      "         [-5.8242e-02, -8.0234e-03,  6.0661e-02],\n",
      "         [ 3.7500e-03, -1.3725e-02,  2.7048e-03]],\n",
      "\n",
      "        [[-1.8546e-01,  2.9806e-03,  5.3922e-02],\n",
      "         [ 1.1526e-02, -3.7099e-02,  1.7369e-01],\n",
      "         [-1.2251e-02, -6.5436e-03,  1.9698e-02],\n",
      "         ...,\n",
      "         [-2.5117e-02,  6.5269e-02, -6.5312e-03],\n",
      "         [-2.2095e-02, -5.1824e-05, -5.4359e-02],\n",
      "         [-4.2521e-02,  4.0661e-02, -5.1415e-02]]], requires_grad=True)\n",
      "tensor([[[-0.0150,  0.0087,  0.1234],\n",
      "         [ 0.0146,  0.0538, -0.0956],\n",
      "         [ 0.0295,  0.1306,  0.0111],\n",
      "         ...,\n",
      "         [ 0.0888, -0.0370,  0.0669],\n",
      "         [-0.0406,  0.0634, -0.0017],\n",
      "         [ 0.0025, -0.0452,  0.1359]],\n",
      "\n",
      "        [[-0.1039,  0.0505, -0.0441],\n",
      "         [-0.0103,  0.0622, -0.0425],\n",
      "         [-0.0482,  0.0244, -0.0390],\n",
      "         ...,\n",
      "         [ 0.0027,  0.0915, -0.0247],\n",
      "         [ 0.0625, -0.0088,  0.1406],\n",
      "         [-0.0749,  0.0406, -0.1812]],\n",
      "\n",
      "        [[-0.0223, -0.0046,  0.0651],\n",
      "         [ 0.0563,  0.0197,  0.0247],\n",
      "         [ 0.0134,  0.0870,  0.0537],\n",
      "         ...,\n",
      "         [ 0.0753, -0.0257,  0.0161],\n",
      "         [-0.0576,  0.0229, -0.0760],\n",
      "         [-0.0435, -0.0620,  0.0126]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0012,  0.0198, -0.0877],\n",
      "         [ 0.0762,  0.0233,  0.0048],\n",
      "         [ 0.0934,  0.0350, -0.0481],\n",
      "         ...,\n",
      "         [-0.0219, -0.0844, -0.0423],\n",
      "         [-0.0124, -0.0393,  0.0800],\n",
      "         [ 0.1330, -0.0060, -0.1258]],\n",
      "\n",
      "        [[-0.1406,  0.0405,  0.0285],\n",
      "         [-0.1000, -0.0010, -0.0176],\n",
      "         [-0.0548, -0.0456,  0.0492],\n",
      "         ...,\n",
      "         [-0.0667,  0.0345,  0.1112],\n",
      "         [ 0.0247,  0.0914,  0.0394],\n",
      "         [-0.0273, -0.0094,  0.0068]],\n",
      "\n",
      "        [[ 0.0015,  0.0974,  0.0516],\n",
      "         [ 0.0086,  0.0065, -0.0587],\n",
      "         [-0.0614, -0.0368,  0.0126],\n",
      "         ...,\n",
      "         [-0.0106, -0.0917, -0.0098],\n",
      "         [ 0.0166, -0.0472,  0.0414],\n",
      "         [-0.0411,  0.0275, -0.0254]]], requires_grad=True)\n",
      "tensor([[[-0.1098, -0.0231, -0.1645],\n",
      "         [-0.0348,  0.0523, -0.0262],\n",
      "         [ 0.0652,  0.0381, -0.0104],\n",
      "         ...,\n",
      "         [ 0.0404,  0.0592, -0.0043],\n",
      "         [ 0.0386, -0.0362, -0.0138],\n",
      "         [ 0.0858, -0.0487,  0.0456]],\n",
      "\n",
      "        [[-0.1028, -0.0071, -0.0190],\n",
      "         [-0.0206, -0.0149,  0.0203],\n",
      "         [-0.0158, -0.0457, -0.0457],\n",
      "         ...,\n",
      "         [ 0.0041, -0.0541, -0.0745],\n",
      "         [-0.0222, -0.0219,  0.0270],\n",
      "         [-0.0548, -0.0294, -0.0325]],\n",
      "\n",
      "        [[ 0.0488, -0.0394, -0.0820],\n",
      "         [ 0.0381,  0.0524, -0.0211],\n",
      "         [-0.0072,  0.0041,  0.2504],\n",
      "         ...,\n",
      "         [ 0.0017, -0.0969, -0.0400],\n",
      "         [-0.0349,  0.0333,  0.0059],\n",
      "         [-0.0746,  0.0879, -0.0089]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0965,  0.0488,  0.0513],\n",
      "         [-0.0179,  0.0135, -0.0628],\n",
      "         [-0.0145, -0.0666,  0.0082],\n",
      "         ...,\n",
      "         [ 0.0888, -0.0901,  0.0354],\n",
      "         [-0.1242, -0.0170, -0.0384],\n",
      "         [ 0.0046,  0.0221, -0.0513]],\n",
      "\n",
      "        [[ 0.0109, -0.0061, -0.0687],\n",
      "         [ 0.0257, -0.0127, -0.0092],\n",
      "         [-0.0332,  0.0215,  0.0385],\n",
      "         ...,\n",
      "         [ 0.0157,  0.0145,  0.0161],\n",
      "         [-0.0097, -0.0055, -0.0378],\n",
      "         [ 0.0026,  0.0196, -0.0354]],\n",
      "\n",
      "        [[-0.0282, -0.0126, -0.0421],\n",
      "         [ 0.0445,  0.0354, -0.0779],\n",
      "         [-0.0222, -0.1056,  0.0476],\n",
      "         ...,\n",
      "         [ 0.0404, -0.0723,  0.0209],\n",
      "         [ 0.0488,  0.0724,  0.0018],\n",
      "         [-0.0392,  0.0934, -0.0617]]], requires_grad=True)\n",
      "tensor([[[-0.0039,  0.0013,  0.0485],\n",
      "         [ 0.0025, -0.0113, -0.0844],\n",
      "         [-0.0043,  0.1233, -0.0419],\n",
      "         ...,\n",
      "         [ 0.0099, -0.0200,  0.0293],\n",
      "         [ 0.0221, -0.0993, -0.0259],\n",
      "         [ 0.0071, -0.0860, -0.0139]],\n",
      "\n",
      "        [[ 0.0613,  0.0485,  0.0270],\n",
      "         [-0.0022, -0.0516,  0.0406],\n",
      "         [ 0.0821,  0.0922,  0.0123],\n",
      "         ...,\n",
      "         [ 0.1025, -0.0508, -0.0097],\n",
      "         [-0.0340,  0.0699,  0.0062],\n",
      "         [ 0.0085, -0.0202,  0.0262]],\n",
      "\n",
      "        [[-0.0088, -0.0379,  0.0762],\n",
      "         [-0.0395, -0.0103,  0.0180],\n",
      "         [-0.0896, -0.1039,  0.0168],\n",
      "         ...,\n",
      "         [-0.0501, -0.0138, -0.0104],\n",
      "         [-0.0082,  0.0122, -0.0432],\n",
      "         [ 0.0484, -0.0008,  0.0158]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0823, -0.0092,  0.0676],\n",
      "         [ 0.0626,  0.0112,  0.0186],\n",
      "         [-0.1455,  0.1606,  0.0948],\n",
      "         ...,\n",
      "         [ 0.0225,  0.0711,  0.0267],\n",
      "         [ 0.1200,  0.0982,  0.0506],\n",
      "         [ 0.0025, -0.0138,  0.0648]],\n",
      "\n",
      "        [[ 0.0006,  0.0270, -0.0126],\n",
      "         [ 0.0240,  0.0258, -0.0576],\n",
      "         [ 0.0596,  0.0310, -0.0312],\n",
      "         ...,\n",
      "         [-0.0686,  0.0430, -0.0165],\n",
      "         [-0.0274, -0.0428,  0.0622],\n",
      "         [-0.0121, -0.0445, -0.0587]],\n",
      "\n",
      "        [[ 0.0388, -0.0025, -0.0668],\n",
      "         [-0.0110, -0.0101, -0.0618],\n",
      "         [-0.0063, -0.0461,  0.1258],\n",
      "         ...,\n",
      "         [ 0.0982,  0.0394, -0.0036],\n",
      "         [-0.0209, -0.0178, -0.0513],\n",
      "         [ 0.0572,  0.0381,  0.0104]]], requires_grad=True)\n",
      "tensor([[[ 0.0185,  0.0215, -0.0221,  0.0308],\n",
      "         [-0.0064,  0.0148,  0.0090, -0.0579],\n",
      "         [ 0.0041,  0.0055,  0.0440, -0.0187],\n",
      "         ...,\n",
      "         [-0.0060,  0.0482, -0.0111,  0.0350],\n",
      "         [ 0.0109, -0.0114, -0.0248,  0.0233],\n",
      "         [ 0.0612, -0.0435, -0.0069, -0.0001]],\n",
      "\n",
      "        [[-0.0128,  0.0029,  0.0248,  0.0081],\n",
      "         [ 0.0131,  0.0051,  0.0182,  0.0010],\n",
      "         [ 0.0150,  0.0005, -0.0271, -0.0069],\n",
      "         ...,\n",
      "         [ 0.0101,  0.0632,  0.0065, -0.0084],\n",
      "         [ 0.0093, -0.0202, -0.0051, -0.0078],\n",
      "         [-0.0200,  0.0013,  0.0465,  0.0366]],\n",
      "\n",
      "        [[ 0.0203,  0.0152, -0.0071,  0.0021],\n",
      "         [ 0.0170,  0.0095,  0.0258,  0.0156],\n",
      "         [ 0.0163,  0.0013,  0.0229,  0.0147],\n",
      "         ...,\n",
      "         [-0.0185, -0.0034,  0.0058,  0.0088],\n",
      "         [-0.0360, -0.0121,  0.0217,  0.0030],\n",
      "         [ 0.0296,  0.0237, -0.0109,  0.0027]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0369,  0.0212, -0.0201, -0.0507],\n",
      "         [-0.0011, -0.0500, -0.0316,  0.0050],\n",
      "         [ 0.0202,  0.0122, -0.0167, -0.0153],\n",
      "         ...,\n",
      "         [-0.0005, -0.0363, -0.0096, -0.0007],\n",
      "         [ 0.0227,  0.0282,  0.0221, -0.0166],\n",
      "         [-0.0216,  0.0085,  0.0516, -0.0020]],\n",
      "\n",
      "        [[ 0.0411,  0.0054, -0.1179,  0.0111],\n",
      "         [ 0.0229, -0.0334, -0.0400, -0.0109],\n",
      "         [ 0.0296,  0.0275, -0.0038,  0.0231],\n",
      "         ...,\n",
      "         [ 0.0060, -0.0139, -0.0048,  0.0332],\n",
      "         [-0.0033, -0.0063,  0.0014,  0.0006],\n",
      "         [ 0.0415,  0.0245, -0.0059, -0.0021]],\n",
      "\n",
      "        [[ 0.0067, -0.0003,  0.0403,  0.0208],\n",
      "         [-0.0272, -0.0150, -0.0382, -0.0052],\n",
      "         [ 0.0041,  0.0270,  0.0055, -0.0048],\n",
      "         ...,\n",
      "         [ 0.0021,  0.0165,  0.0337,  0.0091],\n",
      "         [ 0.0127,  0.0120,  0.0083,  0.0072],\n",
      "         [ 0.0276, -0.0215, -0.0190,  0.0028]]], requires_grad=True)\n",
      "tensor([[[-0.0137, -0.0294, -0.0212],\n",
      "         [ 0.0503,  0.0544,  0.0407],\n",
      "         [-0.0056, -0.0130,  0.0017],\n",
      "         ...,\n",
      "         [-0.0186, -0.0602, -0.0081],\n",
      "         [-0.0160,  0.0798,  0.0571],\n",
      "         [-0.0361,  0.0181,  0.0190]],\n",
      "\n",
      "        [[ 0.0335,  0.0057,  0.0336],\n",
      "         [-0.1163,  0.1021,  0.1015],\n",
      "         [-0.0396, -0.1613, -0.0206],\n",
      "         ...,\n",
      "         [-0.0914, -0.0054,  0.1612],\n",
      "         [ 0.0080,  0.0228, -0.0792],\n",
      "         [-0.0067,  0.0706, -0.0023]],\n",
      "\n",
      "        [[-0.0053,  0.0991,  0.1221],\n",
      "         [ 0.0319, -0.0786, -0.0163],\n",
      "         [-0.0116,  0.0075,  0.0180],\n",
      "         ...,\n",
      "         [-0.0218, -0.0378, -0.0169],\n",
      "         [ 0.0568,  0.0030,  0.0375],\n",
      "         [ 0.0650, -0.0425, -0.0005]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0204, -0.0484, -0.0594],\n",
      "         [-0.0065,  0.0266,  0.0682],\n",
      "         [ 0.0076,  0.0065, -0.0183],\n",
      "         ...,\n",
      "         [ 0.0155, -0.0084,  0.0293],\n",
      "         [-0.0443, -0.0390, -0.0563],\n",
      "         [-0.0235, -0.0354, -0.0090]],\n",
      "\n",
      "        [[ 0.0419, -0.0318, -0.0158],\n",
      "         [-0.0323, -0.0154, -0.0036],\n",
      "         [ 0.0372, -0.1595, -0.0603],\n",
      "         ...,\n",
      "         [-0.0333, -0.0043, -0.1163],\n",
      "         [-0.0192, -0.0346,  0.0513],\n",
      "         [-0.1212, -0.0984,  0.0238]],\n",
      "\n",
      "        [[-0.0639,  0.0483, -0.2061],\n",
      "         [-0.0826, -0.0333,  0.1675],\n",
      "         [-0.0543, -0.0967, -0.0353],\n",
      "         ...,\n",
      "         [ 0.0131, -0.0139,  0.1210],\n",
      "         [-0.1294,  0.0862, -0.0558],\n",
      "         [ 0.0276, -0.0176, -0.0100]]], requires_grad=True)\n",
      "tensor([[[ 4.3325e-02,  9.4524e-03, -3.3362e-02],\n",
      "         [ 1.2817e-01, -2.4288e-01, -8.6305e-02],\n",
      "         [-4.7273e-02,  9.3614e-02, -3.2183e-02],\n",
      "         ...,\n",
      "         [ 1.6325e-02, -4.4696e-02, -1.5363e-02],\n",
      "         [-1.5859e-01,  5.5864e-02, -8.2463e-03],\n",
      "         [-8.0024e-02,  5.8243e-02, -1.0320e-01]],\n",
      "\n",
      "        [[-7.7098e-02, -6.3221e-02,  5.6733e-02],\n",
      "         [-1.1913e-01, -2.1156e-01,  8.1672e-02],\n",
      "         [-2.4511e-03, -9.1665e-02, -7.2340e-02],\n",
      "         ...,\n",
      "         [-3.2431e-02, -1.2181e-04, -1.0121e-01],\n",
      "         [-5.0604e-03,  1.2233e-01, -6.7015e-02],\n",
      "         [ 7.7638e-03,  4.2022e-02,  1.1621e-01]],\n",
      "\n",
      "        [[ 3.3410e-02, -3.9622e-03, -1.8569e-02],\n",
      "         [ 1.0954e-02, -6.0011e-03,  1.0204e-01],\n",
      "         [-1.3311e-02, -6.5039e-03, -4.6447e-03],\n",
      "         ...,\n",
      "         [-6.5840e-02, -1.9273e-02,  2.7502e-02],\n",
      "         [ 3.1059e-02, -6.7296e-02, -2.2400e-02],\n",
      "         [ 4.2328e-02, -3.9004e-04, -5.4479e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.5653e-02,  2.8928e-02, -6.5672e-02],\n",
      "         [-1.3215e-02, -1.0425e-01,  5.8087e-02],\n",
      "         [-6.8386e-02, -1.5471e-02,  7.3672e-02],\n",
      "         ...,\n",
      "         [ 4.2102e-02,  2.6635e-02,  4.7635e-02],\n",
      "         [ 2.1728e-02,  1.1102e-02, -4.1664e-02],\n",
      "         [-7.3963e-02, -1.2145e-03,  4.5197e-02]],\n",
      "\n",
      "        [[-6.8244e-02, -7.1038e-02, -2.3356e-02],\n",
      "         [-2.5041e-02,  3.5519e-02,  1.1783e-03],\n",
      "         [ 3.4981e-02,  1.0089e-01, -2.7923e-02],\n",
      "         ...,\n",
      "         [ 8.3664e-03,  4.5048e-02,  1.9696e-02],\n",
      "         [-3.3736e-02,  2.0575e-02,  2.5555e-02],\n",
      "         [ 4.0918e-03,  5.5216e-02, -1.7862e-02]],\n",
      "\n",
      "        [[ 1.4151e-02, -4.9403e-02, -5.0626e-03],\n",
      "         [ 5.9970e-02,  3.5444e-02,  1.8206e-02],\n",
      "         [-1.3708e-03,  9.6975e-03, -9.3397e-02],\n",
      "         ...,\n",
      "         [ 1.0338e-02,  5.3046e-02, -4.9340e-02],\n",
      "         [-3.9578e-02,  1.6340e-02, -1.8070e-02],\n",
      "         [ 3.5935e-02,  7.5866e-02, -8.4103e-02]]], requires_grad=True)\n",
      "tensor([[[-3.2636e-02, -2.0217e-02, -1.3968e-02],\n",
      "         [ 1.4906e-04, -8.4138e-03, -2.8347e-02],\n",
      "         [-4.9157e-02, -4.7714e-02,  2.8189e-02],\n",
      "         ...,\n",
      "         [-3.7339e-02, -5.6761e-02,  3.2034e-02],\n",
      "         [-4.3689e-02,  1.8287e-02,  5.8460e-03],\n",
      "         [ 9.9596e-03,  3.4741e-02, -5.1493e-02]],\n",
      "\n",
      "        [[ 2.7817e-02,  1.0334e-01,  1.4862e-02],\n",
      "         [-4.7701e-02, -7.7921e-02, -4.2485e-02],\n",
      "         [-3.0524e-02, -8.9004e-03, -5.3472e-04],\n",
      "         ...,\n",
      "         [ 2.3545e-02,  1.4653e-02, -4.1820e-02],\n",
      "         [-9.8382e-02, -2.0035e-02,  1.6664e-02],\n",
      "         [-2.2637e-02, -3.8085e-02, -2.3676e-02]],\n",
      "\n",
      "        [[ 1.1557e-02,  2.8891e-02,  1.9671e-02],\n",
      "         [-1.0859e-02, -1.2597e-02, -8.5051e-02],\n",
      "         [-6.7790e-02,  3.9617e-02,  1.9335e-01],\n",
      "         ...,\n",
      "         [-3.3480e-02,  6.0207e-02, -2.9370e-02],\n",
      "         [-3.0075e-02, -2.0247e-02, -9.1005e-02],\n",
      "         [-8.1139e-02, -9.6305e-02,  8.2019e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7450e-03, -9.5021e-02, -3.2654e-02],\n",
      "         [ 4.0825e-03, -1.5179e-02, -7.1893e-03],\n",
      "         [-5.9850e-02, -9.4783e-02,  9.4387e-02],\n",
      "         ...,\n",
      "         [ 5.7619e-03, -3.9606e-02, -5.9507e-03],\n",
      "         [ 1.7790e-02, -2.0632e-02, -8.3306e-02],\n",
      "         [ 5.3559e-02,  1.4897e-01,  1.7167e-01]],\n",
      "\n",
      "        [[ 7.2592e-02,  1.9820e-03,  3.9243e-03],\n",
      "         [-4.0850e-02, -7.1272e-03,  4.4443e-03],\n",
      "         [-8.4566e-03, -3.1704e-02, -4.1785e-02],\n",
      "         ...,\n",
      "         [-3.2481e-02, -1.6457e-02,  7.7463e-03],\n",
      "         [ 3.3346e-02, -3.6237e-02, -1.2073e-01],\n",
      "         [-2.0206e-02,  3.0289e-03, -1.2124e-02]],\n",
      "\n",
      "        [[-2.6225e-02, -2.9567e-03, -5.8877e-02],\n",
      "         [-6.7842e-02, -5.1997e-02,  8.1598e-03],\n",
      "         [ 1.7682e-02, -2.9627e-02, -1.3288e-03],\n",
      "         ...,\n",
      "         [-4.6701e-03, -3.8279e-02,  1.7593e-02],\n",
      "         [ 2.5732e-02,  8.4005e-03,  6.5744e-02],\n",
      "         [-9.2425e-03,  4.8081e-03, -2.8709e-02]]], requires_grad=True)\n",
      "tensor([[[ 0.0171, -0.0155,  0.0007],\n",
      "         [-0.0163, -0.0469,  0.0263],\n",
      "         [-0.0149, -0.0439,  0.0006],\n",
      "         ...,\n",
      "         [-0.0565, -0.0323,  0.0756],\n",
      "         [-0.0206, -0.0029, -0.0121],\n",
      "         [-0.0211,  0.0137,  0.0193]],\n",
      "\n",
      "        [[ 0.0633,  0.0156,  0.0045],\n",
      "         [ 0.0388,  0.0426, -0.0067],\n",
      "         [-0.0525, -0.0840, -0.1139],\n",
      "         ...,\n",
      "         [ 0.0592, -0.0177, -0.0713],\n",
      "         [ 0.0660, -0.0106, -0.0572],\n",
      "         [ 0.0227,  0.0060, -0.0307]],\n",
      "\n",
      "        [[ 0.0283, -0.0040, -0.0060],\n",
      "         [ 0.0019, -0.0143, -0.0127],\n",
      "         [ 0.0317,  0.0168,  0.0830],\n",
      "         ...,\n",
      "         [-0.1002, -0.0152,  0.0373],\n",
      "         [-0.0262,  0.0181, -0.0073],\n",
      "         [-0.0079, -0.0667,  0.0015]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0089,  0.0791,  0.0309],\n",
      "         [-0.0193, -0.0138, -0.0021],\n",
      "         [ 0.0133,  0.0338,  0.0291],\n",
      "         ...,\n",
      "         [-0.0193, -0.0312,  0.0010],\n",
      "         [-0.0052,  0.0059,  0.0023],\n",
      "         [ 0.0857, -0.0057, -0.0126]],\n",
      "\n",
      "        [[ 0.0324,  0.0352, -0.0076],\n",
      "         [ 0.0719,  0.1252,  0.1592],\n",
      "         [-0.0050,  0.0165,  0.0047],\n",
      "         ...,\n",
      "         [-0.0196, -0.0278,  0.0410],\n",
      "         [-0.0264, -0.0308, -0.0222],\n",
      "         [-0.0176, -0.0302, -0.0222]],\n",
      "\n",
      "        [[ 0.0103,  0.0012,  0.0137],\n",
      "         [-0.0224, -0.0053, -0.0332],\n",
      "         [ 0.0638,  0.0310, -0.0180],\n",
      "         ...,\n",
      "         [ 0.1027, -0.1565,  0.1306],\n",
      "         [-0.0251,  0.0207, -0.0056],\n",
      "         [-0.0120,  0.0241,  0.0251]]], requires_grad=True)\n",
      "tensor([[[ 0.0673,  0.0076,  0.0403],\n",
      "         [-0.0680, -0.0885, -0.1075],\n",
      "         [-0.0525, -0.0961, -0.0680],\n",
      "         ...,\n",
      "         [-0.0337, -0.0170,  0.0448],\n",
      "         [ 0.0068, -0.0621,  0.0470],\n",
      "         [ 0.0075, -0.0433,  0.0255]],\n",
      "\n",
      "        [[ 0.1016,  0.0702,  0.0804],\n",
      "         [ 0.0030, -0.0015,  0.0822],\n",
      "         [-0.0258,  0.0062, -0.0301],\n",
      "         ...,\n",
      "         [ 0.0365,  0.0764, -0.0565],\n",
      "         [-0.0310,  0.0117, -0.0407],\n",
      "         [ 0.0208, -0.0116, -0.0117]],\n",
      "\n",
      "        [[-0.0081,  0.0440, -0.0633],\n",
      "         [-0.0041, -0.0303,  0.0779],\n",
      "         [ 0.0069, -0.0173, -0.0036],\n",
      "         ...,\n",
      "         [ 0.0390,  0.0148,  0.0836],\n",
      "         [-0.0252, -0.0151, -0.1578],\n",
      "         [ 0.0083,  0.0071,  0.0540]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0855,  0.0377,  0.0603],\n",
      "         [-0.0002,  0.0136,  0.0342],\n",
      "         [ 0.0150, -0.0101, -0.1172],\n",
      "         ...,\n",
      "         [ 0.0715,  0.0808,  0.0982],\n",
      "         [ 0.0496,  0.0280,  0.0382],\n",
      "         [-0.0359,  0.0286,  0.0585]],\n",
      "\n",
      "        [[ 0.0209,  0.0424, -0.0120],\n",
      "         [-0.0213, -0.0129, -0.0178],\n",
      "         [ 0.0347, -0.0219, -0.0310],\n",
      "         ...,\n",
      "         [-0.0055, -0.0029, -0.0152],\n",
      "         [-0.0613, -0.0452, -0.1100],\n",
      "         [ 0.0222,  0.0143, -0.0143]],\n",
      "\n",
      "        [[ 0.0321, -0.0489,  0.0544],\n",
      "         [ 0.0346, -0.0007, -0.0992],\n",
      "         [-0.1015,  0.0407,  0.0064],\n",
      "         ...,\n",
      "         [ 0.0386, -0.0211,  0.0036],\n",
      "         [ 0.0429,  0.0367, -0.0141],\n",
      "         [-0.0013,  0.0403,  0.0297]]], requires_grad=True)\n",
      "tensor([[[ 3.1012e-02, -2.4656e-02, -2.0092e-02],\n",
      "         [ 1.5735e-03,  6.4804e-03, -2.0688e-02],\n",
      "         [-1.6054e-02, -5.1180e-02,  8.9254e-03],\n",
      "         ...,\n",
      "         [-5.5782e-02,  1.0501e-01,  6.7943e-02],\n",
      "         [ 8.9544e-04,  1.5420e-03, -7.0600e-03],\n",
      "         [-3.3085e-02, -6.7873e-03, -4.0253e-02]],\n",
      "\n",
      "        [[-1.8760e-02,  4.5231e-02, -4.1002e-02],\n",
      "         [ 7.5072e-04, -1.1096e-02, -4.6942e-03],\n",
      "         [ 9.7261e-03,  8.7720e-03, -1.8117e-02],\n",
      "         ...,\n",
      "         [-2.0491e-02,  8.3262e-02,  3.3583e-02],\n",
      "         [ 2.6959e-02, -3.5947e-03, -1.6505e-02],\n",
      "         [ 7.4739e-03, -5.8651e-03,  2.5145e-02]],\n",
      "\n",
      "        [[-4.9067e-03,  3.3079e-02, -2.1346e-02],\n",
      "         [-8.9392e-02, -3.1533e-02, -7.5984e-04],\n",
      "         [ 4.5594e-02,  2.1424e-02, -5.3764e-03],\n",
      "         ...,\n",
      "         [ 3.1163e-02, -1.0068e-04, -4.2750e-02],\n",
      "         [ 3.6256e-02,  1.8239e-02, -4.3901e-03],\n",
      "         [ 1.3489e-02,  4.9177e-03, -1.4212e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.5385e-02, -7.8222e-02, -7.3953e-02],\n",
      "         [-2.5789e-02,  3.4346e-02, -5.9720e-02],\n",
      "         [-5.2242e-02, -2.7587e-02,  2.2983e-02],\n",
      "         ...,\n",
      "         [-2.7361e-02, -2.0069e-02,  6.2601e-02],\n",
      "         [-1.4368e-02, -2.7883e-02, -4.1649e-02],\n",
      "         [ 5.4532e-02, -3.1741e-02, -5.8214e-02]],\n",
      "\n",
      "        [[-1.1433e-02, -6.8911e-02,  1.1823e-01],\n",
      "         [ 1.2817e-02, -5.9638e-02,  6.5643e-03],\n",
      "         [ 1.4322e-02,  8.3627e-02, -3.6626e-02],\n",
      "         ...,\n",
      "         [ 1.4292e-02,  1.4207e-02,  1.3416e-02],\n",
      "         [-1.4570e-01, -2.1497e-01, -1.5415e-01],\n",
      "         [-8.9806e-03, -2.0540e-02, -1.1899e-02]],\n",
      "\n",
      "        [[-1.2198e-02,  8.0532e-03, -3.1812e-03],\n",
      "         [-6.1474e-03,  3.3781e-02,  2.5145e-02],\n",
      "         [ 2.4537e-02,  2.3625e-02, -5.7279e-03],\n",
      "         ...,\n",
      "         [ 1.0584e-01, -3.3122e-02,  9.2634e-02],\n",
      "         [-3.7149e-03,  1.1300e-02,  1.2182e-02],\n",
      "         [-2.3380e-02,  1.1836e-02,  6.9958e-03]]], requires_grad=True)\n",
      "tensor([-6.2863e-04,  4.3727e-04,  1.1240e-04,  1.3638e-04,  9.5821e-06,\n",
      "         1.1177e-04, -2.2506e-04,  4.7010e-05,  1.7747e-04,  7.5881e-05,\n",
      "        -5.9374e-06, -2.2682e-05, -6.8594e-05, -7.1871e-05, -8.7666e-05,\n",
      "        -4.1382e-05], requires_grad=True)\n",
      "tensor([[[-1.8838e-02, -5.0765e-03, -1.5503e-02,  ...,  1.0485e-02,\n",
      "          -7.3018e-03,  3.4230e-02],\n",
      "         [ 1.0065e-02, -7.6521e-03, -1.9118e-02,  ..., -4.7667e-03,\n",
      "           1.6525e-02,  1.0504e-02],\n",
      "         [-3.8500e-03,  1.3530e-02,  1.5269e-02,  ..., -2.6443e-02,\n",
      "          -2.3769e-03,  7.7301e-03],\n",
      "         ...,\n",
      "         [-2.2183e-02, -4.2886e-03,  8.7224e-03,  ..., -4.5534e-02,\n",
      "          -1.1973e-02,  7.1759e-02],\n",
      "         [-6.4803e-02, -5.4324e-02, -3.2428e-02,  ...,  1.7217e-01,\n",
      "           5.6473e-02, -7.3966e-02],\n",
      "         [ 6.6917e-03,  9.6977e-04,  1.0420e-02,  ...,  7.9343e-03,\n",
      "           5.2507e-03, -2.1269e-02]],\n",
      "\n",
      "        [[-1.6455e-03, -3.5383e-05, -4.7115e-03,  ..., -3.4591e-03,\n",
      "           1.5605e-03,  8.6506e-03],\n",
      "         [ 2.9087e-03, -2.8344e-03, -1.6098e-02,  ...,  1.7745e-02,\n",
      "          -1.8713e-02, -9.0905e-03],\n",
      "         [ 5.6579e-03, -5.0828e-03,  5.2148e-03,  ..., -7.9363e-03,\n",
      "           2.7140e-02, -1.0246e-02],\n",
      "         ...,\n",
      "         [-1.4696e-02, -1.0711e-04, -5.7223e-04,  ...,  7.7916e-03,\n",
      "           9.5764e-03,  1.4533e-02],\n",
      "         [-2.3795e-02,  5.3880e-03, -8.9999e-03,  ...,  1.0130e-02,\n",
      "           5.6180e-03,  2.4181e-02],\n",
      "         [-5.6341e-03, -5.3490e-03,  1.0061e-03,  ..., -1.0415e-02,\n",
      "           9.4304e-03,  7.1364e-03]],\n",
      "\n",
      "        [[-6.4641e-03,  2.5646e-03, -2.1942e-03,  ...,  2.1052e-02,\n",
      "          -8.8015e-03, -1.1472e-02],\n",
      "         [ 9.3335e-03, -3.1824e-03,  4.6522e-03,  ..., -2.5932e-02,\n",
      "           1.2365e-02,  5.4797e-03],\n",
      "         [ 8.7570e-03,  1.2112e-02, -1.3054e-02,  ...,  8.9532e-03,\n",
      "          -1.1214e-02, -3.8287e-03],\n",
      "         ...,\n",
      "         [-5.3740e-04, -5.4363e-03, -4.8387e-03,  ...,  1.4122e-02,\n",
      "           3.6300e-03,  2.1972e-03],\n",
      "         [ 2.5578e-03, -1.3658e-02,  1.3329e-02,  ..., -5.4952e-03,\n",
      "           6.2595e-03, -1.1317e-02],\n",
      "         [-1.3963e-03, -8.2177e-04,  6.4351e-03,  ...,  4.3525e-03,\n",
      "          -9.7733e-03, -2.3111e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.9705e-05,  1.5562e-04,  3.9327e-04,  ..., -1.5046e-04,\n",
      "          -5.0001e-04, -6.1005e-04],\n",
      "         [-6.7578e-06,  1.5638e-05, -6.2853e-05,  ..., -1.7577e-04,\n",
      "           1.9302e-04, -4.7519e-05],\n",
      "         [ 2.7709e-04,  2.0264e-05,  1.3843e-04,  ...,  8.1969e-05,\n",
      "          -8.8840e-05, -3.0004e-04],\n",
      "         ...,\n",
      "         [ 2.3719e-05, -2.3868e-06,  4.1171e-06,  ...,  5.6511e-04,\n",
      "          -1.2158e-04,  7.7301e-05],\n",
      "         [ 6.8713e-05, -2.2492e-07,  7.9670e-05,  ...,  2.3302e-04,\n",
      "          -3.4835e-04,  1.0569e-04],\n",
      "         [ 2.1488e-04,  4.2781e-04, -3.9948e-04,  ..., -3.7146e-04,\n",
      "           3.4674e-04,  2.0463e-04]],\n",
      "\n",
      "        [[-3.9354e-05, -5.8692e-05,  1.4555e-04,  ...,  1.4293e-05,\n",
      "          -2.8489e-04, -1.8473e-04],\n",
      "         [-8.7926e-06,  3.2739e-05,  4.1334e-05,  ...,  9.6323e-05,\n",
      "          -3.2683e-04, -1.0820e-04],\n",
      "         [-1.3233e-05, -5.5494e-05,  5.7904e-05,  ...,  1.9089e-04,\n",
      "          -5.3150e-05,  5.0805e-05],\n",
      "         ...,\n",
      "         [ 3.5623e-05,  5.2553e-05, -2.5346e-04,  ...,  2.8259e-04,\n",
      "           1.8178e-04,  3.2798e-05],\n",
      "         [-2.2348e-05, -1.3519e-04, -1.4571e-04,  ...,  3.4295e-05,\n",
      "           1.3816e-04,  2.2155e-04],\n",
      "         [ 2.1039e-05,  3.6751e-05, -3.5976e-05,  ..., -4.7523e-05,\n",
      "           2.9547e-05,  4.1243e-05]],\n",
      "\n",
      "        [[ 3.0623e-05, -4.9787e-05,  8.6906e-06,  ...,  1.2214e-06,\n",
      "           6.2589e-05, -6.1683e-05],\n",
      "         [ 8.0378e-05, -6.1229e-05, -2.1355e-05,  ..., -8.1777e-06,\n",
      "           5.1206e-06, -3.2125e-05],\n",
      "         [-2.6043e-05,  1.3722e-04,  2.2218e-06,  ...,  9.6285e-05,\n",
      "          -7.5099e-05,  1.9944e-05],\n",
      "         ...,\n",
      "         [ 2.1626e-04, -8.8592e-05, -1.2529e-05,  ..., -1.6538e-05,\n",
      "           3.6862e-05, -1.2791e-04],\n",
      "         [-6.3255e-05,  4.3015e-05,  1.2302e-04,  ...,  8.8273e-06,\n",
      "           4.0734e-05, -1.4455e-05],\n",
      "         [ 7.9708e-05,  2.4042e-05, -1.5976e-04,  ...,  3.5568e-05,\n",
      "          -3.0800e-05, -1.2787e-05]]], requires_grad=True)\n",
      "tensor([-0.3143], requires_grad=True)\n",
      "tensor([[[-8.6899e-03, -7.3282e-03, -1.7061e-02],\n",
      "         [-1.0209e-02,  4.3171e-03, -1.1125e-03],\n",
      "         [ 1.2925e-02,  1.3685e-02, -3.3545e-02],\n",
      "         [ 1.5289e-02,  2.4725e-02, -4.0924e-02],\n",
      "         [ 1.1206e-02, -7.3447e-03,  1.0994e-02],\n",
      "         [ 2.1416e-02,  3.5771e-02, -3.7888e-03],\n",
      "         [-7.2777e-03, -4.4832e-03,  3.3374e-02],\n",
      "         [ 2.9120e-02, -4.4158e-03, -1.5225e-02],\n",
      "         [-1.0178e-02, -4.4887e-02, -2.0203e-02],\n",
      "         [-1.5175e-02, -2.5821e-02,  7.3272e-02],\n",
      "         [ 4.9896e-02,  5.1637e-02, -7.6607e-02],\n",
      "         [ 2.4524e-02,  2.2724e-02, -7.3307e-02],\n",
      "         [-4.7262e-03,  2.5660e-02,  4.5972e-02],\n",
      "         [-7.9706e-03, -3.1647e-03,  2.3638e-02],\n",
      "         [-2.2780e-05,  5.4120e-03,  1.3958e-02],\n",
      "         [-2.3627e-01, -4.9241e-01, -6.5642e-01],\n",
      "         [-1.8978e-01, -3.0435e-01, -4.3648e-01],\n",
      "         [ 4.9979e-03,  7.5081e-03, -6.3679e-02],\n",
      "         [-7.0012e-03,  3.0375e-02, -5.8274e-03],\n",
      "         [-3.2207e-02,  2.6249e-02, -3.0012e-02],\n",
      "         [-7.3604e-03, -2.2791e-02,  4.5857e-02],\n",
      "         [ 2.5879e-03, -3.7984e-02,  1.5217e-02],\n",
      "         [ 2.3808e-02, -8.2795e-03, -7.7782e-04],\n",
      "         [-9.4037e-03,  3.2236e-03,  3.2494e-02],\n",
      "         [-1.8720e-02,  1.3770e-02,  1.2046e-02],\n",
      "         [ 2.5048e-03,  6.3272e-03, -4.2997e-02],\n",
      "         [-3.6976e-02,  4.1985e-02,  4.6004e-02],\n",
      "         [ 2.9769e-02,  1.5379e-02, -2.7230e-02],\n",
      "         [ 1.4672e-02, -5.0915e-03, -3.2971e-02],\n",
      "         [ 6.1295e-03,  7.3942e-02, -7.7665e-02],\n",
      "         [-1.0735e-02,  4.4570e-02, -1.1287e-02],\n",
      "         [ 6.8009e-03, -5.6662e-03,  4.6967e-02],\n",
      "         [-8.1418e-03, -4.4121e-02,  3.7445e-02],\n",
      "         [-1.9385e-02, -1.5363e-02,  4.6126e-02],\n",
      "         [ 1.4234e-02, -3.6584e-02, -3.6920e-02],\n",
      "         [ 1.9413e-02,  1.0757e-02,  4.3550e-02],\n",
      "         [-3.6290e-03, -2.2988e-02,  1.4421e-02],\n",
      "         [-3.9988e-02, -4.7380e-02,  8.3994e-02],\n",
      "         [ 1.7374e-03,  3.7297e-04,  1.2658e-03],\n",
      "         [-4.6644e-02,  4.1093e-02,  3.8099e-02],\n",
      "         [-4.0967e-02, -1.0312e-02,  7.3745e-02],\n",
      "         [-3.3472e-02,  7.4062e-03,  3.3058e-03],\n",
      "         [-7.5828e-03,  3.3537e-02, -9.4380e-03],\n",
      "         [ 4.4339e-02,  4.3365e-02, -7.8992e-02],\n",
      "         [ 1.8001e-02,  3.4681e-02, -5.7267e-02],\n",
      "         [ 1.2147e-02, -1.0824e-02, -1.3687e-02],\n",
      "         [-2.6766e-02, -7.9990e-03, -1.3822e-02],\n",
      "         [-6.3128e-03, -2.5738e-02,  6.9363e-02],\n",
      "         [-1.2139e-02, -2.1638e-02,  2.6084e-02],\n",
      "         [-4.7307e-02,  5.6525e-04, -9.5348e-03],\n",
      "         [-1.1497e-02, -9.5298e-03, -1.1235e-02],\n",
      "         [-5.8216e-03,  5.9943e-02,  1.4314e-02],\n",
      "         [ 3.7675e-03,  1.6678e-02,  5.9802e-02],\n",
      "         [ 2.1090e-02,  2.2783e-02, -4.8645e-02],\n",
      "         [ 4.0967e-04,  1.4665e-04, -2.9236e-02],\n",
      "         [-1.1060e-02,  1.9168e-03, -1.4632e-02],\n",
      "         [ 1.3532e-02,  5.8754e-03,  3.3007e-02],\n",
      "         [-1.5515e-02,  1.0230e-02, -2.8126e-02],\n",
      "         [ 3.9995e-03,  2.9216e-03, -2.2600e-02],\n",
      "         [ 2.7992e-02,  1.2232e-02, -3.0918e-03],\n",
      "         [-3.2659e-02, -3.8119e-02,  5.2655e-02],\n",
      "         [ 6.3068e-03,  4.4802e-02, -1.8381e-02],\n",
      "         [ 1.4367e-03, -8.6468e-03,  1.9718e-02],\n",
      "         [-1.3218e-02, -1.9879e-03, -2.7423e-02]]], requires_grad=True)\n",
      "tensor([[[ 1.6000e-02,  2.4319e-02,  1.4648e-01],\n",
      "         [ 4.4742e-02, -4.5935e-02, -3.1067e-02],\n",
      "         [-1.7826e-02,  1.1428e-01,  3.3828e-02],\n",
      "         ...,\n",
      "         [-6.8156e-03, -5.1398e-03, -2.0223e-01],\n",
      "         [-2.3109e-02, -1.3546e-01, -1.0524e-01],\n",
      "         [ 6.1662e-02, -1.1809e-02, -4.4724e-02]],\n",
      "\n",
      "        [[-9.0957e-02, -5.4095e-02,  1.0705e-01],\n",
      "         [ 3.1153e-02,  7.0331e-03, -3.9668e-02],\n",
      "         [-2.0356e-02, -1.1920e-02,  1.4810e-01],\n",
      "         ...,\n",
      "         [ 1.1123e-02, -7.4673e-02, -1.6510e-01],\n",
      "         [ 1.6701e-01,  7.2753e-02,  1.5099e-01],\n",
      "         [-5.6347e-03, -6.0797e-02, -2.0415e-02]],\n",
      "\n",
      "        [[-1.5810e-02,  5.4871e-02,  1.8004e-01],\n",
      "         [-2.0094e-02, -6.6498e-02, -1.3439e-01],\n",
      "         [ 9.0765e-02,  9.3496e-02,  1.6687e-01],\n",
      "         ...,\n",
      "         [-4.2809e-02,  6.1871e-03, -2.1992e-01],\n",
      "         [ 6.7464e-02,  3.0185e-02,  8.7892e-02],\n",
      "         [ 7.3096e-02,  6.4871e-02, -4.3837e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.5213e-02,  3.9069e-02, -5.4485e-02],\n",
      "         [ 2.1335e-03, -4.4567e-02,  2.9135e-02],\n",
      "         [-4.1278e-05, -3.2679e-02, -3.9402e-02],\n",
      "         ...,\n",
      "         [-6.8434e-03,  5.0636e-02,  1.2313e-01],\n",
      "         [ 2.0810e-02, -3.0416e-03,  2.3197e-02],\n",
      "         [ 2.3248e-02,  7.8695e-02,  8.9928e-03]],\n",
      "\n",
      "        [[-5.2578e-02, -5.2824e-02,  1.2526e-01],\n",
      "         [ 4.7908e-02,  7.4465e-02, -3.9583e-02],\n",
      "         [ 7.9744e-02,  8.5069e-03,  1.0731e-01],\n",
      "         ...,\n",
      "         [-1.0367e-01, -6.9159e-02, -2.1074e-01],\n",
      "         [-1.0122e-01, -1.4078e-01, -1.2417e-01],\n",
      "         [ 7.0894e-02, -5.4972e-02, -1.4064e-01]],\n",
      "\n",
      "        [[ 5.7642e-02, -3.0537e-02,  1.5287e-01],\n",
      "         [-3.9895e-03,  3.8687e-02, -1.3090e-02],\n",
      "         [-3.6466e-02,  6.5742e-02,  7.7160e-02],\n",
      "         ...,\n",
      "         [-1.2402e-01, -2.3703e-02, -1.9679e-01],\n",
      "         [ 1.3166e-01,  7.3293e-02,  1.4456e-01],\n",
      "         [ 6.7908e-02, -1.2687e-02, -4.4691e-02]]], requires_grad=True)\n",
      "tensor([ 0.0090,  0.1141,  0.0913,  0.1090, -0.1162,  0.0248,  0.1023, -0.1175,\n",
      "        -0.0154, -0.0038,  0.1133,  0.0345, -0.0096,  0.0890,  0.0465,  0.0984,\n",
      "        -0.0054,  0.0487,  0.0882,  0.0587,  0.0240,  0.0333, -0.0086,  0.0545,\n",
      "        -0.1342,  0.0455,  0.0076, -0.1064,  0.0064,  0.0496, -0.1118,  0.0379,\n",
      "        -0.0860,  0.0923,  0.0072, -0.1136, -0.0878,  0.0504, -0.0964,  0.0569,\n",
      "         0.0731,  0.0867,  0.0246,  0.1149,  0.0825,  0.0677,  0.0993, -0.0029,\n",
      "         0.0535, -0.1277,  0.0885, -0.0090,  0.1083, -0.0934,  0.0250,  0.0553,\n",
      "        -0.1290,  0.0414, -0.0041, -0.1283, -0.1012, -0.1450,  0.0318,  0.0231],\n",
      "       requires_grad=True)\n",
      "tensor([[[ 0.0572, -0.0153,  0.0462],\n",
      "         [ 0.0372,  0.1412,  0.0670],\n",
      "         [ 0.0032,  0.0245,  0.0605],\n",
      "         ...,\n",
      "         [-0.0025, -0.0933,  0.0206],\n",
      "         [ 0.1114,  0.0701,  0.1148],\n",
      "         [ 0.0804, -0.0098, -0.0058]],\n",
      "\n",
      "        [[ 0.1086,  0.1189,  0.1123],\n",
      "         [ 0.0896,  0.0198,  0.0897],\n",
      "         [ 0.0586,  0.0028,  0.0661],\n",
      "         ...,\n",
      "         [-0.1085, -0.1301, -0.0457],\n",
      "         [ 0.1029,  0.1212,  0.0454],\n",
      "         [ 0.0371, -0.0093,  0.0367]],\n",
      "\n",
      "        [[-0.0376,  0.0193,  0.0921],\n",
      "         [ 0.0877,  0.0462, -0.0459],\n",
      "         [ 0.0142,  0.0853,  0.0806],\n",
      "         ...,\n",
      "         [-0.0024, -0.0877, -0.0450],\n",
      "         [ 0.0180,  0.1044,  0.0019],\n",
      "         [-0.0058,  0.0935, -0.0090]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0303, -0.0829, -0.1108],\n",
      "         [-0.0917, -0.0830, -0.0543],\n",
      "         [ 0.0215, -0.1099,  0.0436],\n",
      "         ...,\n",
      "         [ 0.0074,  0.0942, -0.0299],\n",
      "         [-0.0327, -0.1053, -0.0452],\n",
      "         [-0.0960, -0.0954,  0.0083]],\n",
      "\n",
      "        [[ 0.0232,  0.0228, -0.0950],\n",
      "         [ 0.0039, -0.0418,  0.0038],\n",
      "         [-0.0641, -0.1016,  0.0040],\n",
      "         ...,\n",
      "         [ 0.0026,  0.0929, -0.0054],\n",
      "         [-0.0557, -0.0878, -0.0703],\n",
      "         [-0.0128, -0.0015, -0.0315]],\n",
      "\n",
      "        [[ 0.0182,  0.0154, -0.1383],\n",
      "         [-0.0109, -0.0272,  0.0179],\n",
      "         [-0.0569, -0.0644, -0.0882],\n",
      "         ...,\n",
      "         [ 0.1162,  0.0806,  0.0166],\n",
      "         [-0.0719, -0.0778, -0.1005],\n",
      "         [-0.0299, -0.0671, -0.1080]]], requires_grad=True)\n",
      "tensor([ 0.0079, -0.0073,  0.0542, -0.0864,  0.0246,  0.0546,  0.0796,  0.0145,\n",
      "        -0.0695,  0.0637, -0.0365,  0.0706,  0.0443,  0.1259,  0.0290, -0.1073,\n",
      "        -0.0487,  0.0306,  0.1374, -0.1245,  0.0413, -0.1050, -0.0221, -0.0725,\n",
      "        -0.0188, -0.0534,  0.0198,  0.0554, -0.0431,  0.0608, -0.0980, -0.0247,\n",
      "         0.0376,  0.0558,  0.0620, -0.1150, -0.0507,  0.0637,  0.0569, -0.0839,\n",
      "         0.0636, -0.0482,  0.0090,  0.0489,  0.0933,  0.0378, -0.0125,  0.0160,\n",
      "         0.0356, -0.0932, -0.0910,  0.0767,  0.0308, -0.0175,  0.0365, -0.0911,\n",
      "         0.0899, -0.0259,  0.0657,  0.0554, -0.0403, -0.0583, -0.0778, -0.1113],\n",
      "       requires_grad=True)\n",
      "tensor([[[-3.6659e-02,  3.9282e-02, -3.5379e-02],\n",
      "         [ 7.2070e-03, -9.0041e-03, -2.1516e-04],\n",
      "         [-1.0597e-01,  2.9389e-02, -7.5101e-02],\n",
      "         ...,\n",
      "         [-2.0868e-02,  3.3643e-02, -5.2239e-02],\n",
      "         [ 2.6418e-03, -2.0174e-02,  1.4046e-02],\n",
      "         [ 2.7625e-02,  4.6248e-02,  7.2753e-02]],\n",
      "\n",
      "        [[-6.9268e-02,  1.4467e-02, -6.7607e-02],\n",
      "         [-7.1720e-02, -8.4373e-02, -8.1341e-02],\n",
      "         [-1.0449e-01, -6.2679e-02, -5.5049e-02],\n",
      "         ...,\n",
      "         [-1.4482e-02,  9.8368e-02,  1.3752e-02],\n",
      "         [ 7.5379e-02,  2.3033e-02,  4.4093e-02],\n",
      "         [ 5.3911e-02,  5.0959e-02, -1.4857e-02]],\n",
      "\n",
      "        [[ 3.2023e-03,  8.2249e-03,  3.4689e-02],\n",
      "         [-6.5256e-03, -5.5567e-02, -8.6545e-02],\n",
      "         [-1.0700e-01, -1.1624e-01, -8.3765e-02],\n",
      "         ...,\n",
      "         [ 5.3445e-02,  1.1210e-01,  1.9366e-02],\n",
      "         [ 9.8786e-02,  1.3549e-02,  1.6708e-02],\n",
      "         [ 5.1906e-02,  9.1293e-02,  5.0217e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.2089e-02, -1.3087e-01, -1.3900e-01],\n",
      "         [-1.2438e-01, -9.0673e-02, -1.4393e-01],\n",
      "         [-8.9313e-02, -1.0267e-01, -1.1287e-01],\n",
      "         ...,\n",
      "         [ 9.2956e-02,  7.6574e-02,  1.0424e-01],\n",
      "         [ 5.3955e-02,  1.2666e-01,  1.3270e-01],\n",
      "         [ 7.5941e-02,  7.3922e-02,  2.1536e-01]],\n",
      "\n",
      "        [[-9.7534e-02, -8.5601e-02, -2.2146e-01],\n",
      "         [-1.0471e-01, -9.3375e-02, -2.1718e-01],\n",
      "         [-5.4997e-02, -3.7971e-02, -1.4840e-01],\n",
      "         ...,\n",
      "         [ 1.2131e-01,  1.0949e-01,  1.7046e-01],\n",
      "         [ 9.4923e-02,  6.2826e-02,  1.6263e-01],\n",
      "         [ 1.3827e-01,  1.3122e-01,  1.1312e-01]],\n",
      "\n",
      "        [[-6.4598e-02, -5.1846e-02, -1.4507e-01],\n",
      "         [-6.7936e-03, -1.1809e-01, -4.7973e-02],\n",
      "         [-1.4726e-01, -7.7199e-02, -1.2840e-01],\n",
      "         ...,\n",
      "         [ 4.1306e-02,  2.1989e-02,  5.8847e-02],\n",
      "         [ 1.2306e-01,  7.3323e-03,  7.1464e-02],\n",
      "         [ 1.1198e-01,  1.7407e-02,  1.1637e-01]]], requires_grad=True)\n",
      "tensor([ 0.0522, -0.0558, -0.0325,  0.0375,  0.0231, -0.0319,  0.0118, -0.0128,\n",
      "        -0.0547,  0.0111, -0.0893,  0.0113, -0.0770, -0.0139, -0.1171, -0.0007,\n",
      "        -0.0669, -0.0095, -0.0394, -0.0251, -0.0284, -0.0775, -0.0156, -0.0895,\n",
      "        -0.0895, -0.0415, -0.0237, -0.0205, -0.0825,  0.0014, -0.0662, -0.0264,\n",
      "        -0.0791, -0.0103, -0.1573, -0.1539, -0.1229, -0.1386, -0.1757, -0.0661,\n",
      "        -0.0198, -0.0482, -0.1191, -0.1284, -0.0762, -0.0163, -0.0314, -0.0784,\n",
      "        -0.0254, -0.0693, -0.0739, -0.0767, -0.0592, -0.1602, -0.1133, -0.0909,\n",
      "         0.0341, -0.0780, -0.0651, -0.1033, -0.1266,  0.0008, -0.0017, -0.0505,\n",
      "        -0.0110, -0.0257, -0.0568, -0.0947, -0.0423, -0.0635, -0.1439, -0.0520,\n",
      "        -0.0623,  0.0401,  0.0549,  0.0450, -0.0366, -0.1015, -0.1514, -0.1863],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([123, 123, 123, 123,  44,   0, 104,  34,  18,  44])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10, 1024)\n",
    "q = qmodel.quantize(x)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([125,  82, 125, 125, 125,  63,  34, 125,   8,   8])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vr \u001b[39m=\u001b[39m VectorQuantization(\u001b[39m1024\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m10\u001b[39m, \u001b[39m1024\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vr(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/nikny/RAVE/test.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m     device \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_in(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nikny/RAVE/test.ipynb#W2sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m     quantize, embed_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codebook(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "vr = VectorQuantization(1024, 128)\n",
    "x = torch.rand(10, 1024)\n",
    "vr(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __gin__ import dynamic_registration\n",
    "import cached_conv as cc\n",
    "from cached_conv import convs\n",
    "import rave\n",
    "from rave import blocks\n",
    "from rave import core\n",
    "from rave import dataset\n",
    "from rave import discriminator\n",
    "from rave import model\n",
    "from rave import pqmf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Macros:\n",
    "# ==============================================================================\n",
    "CAPACITY = 128\n",
    "DILATIONS = [[1, 3, 9], [1, 3, 9], [1, 3, 9], [1, 3]]\n",
    "KERNEL_SIZE = 3\n",
    "LATENT_SIZE = 16\n",
    "N_BAND = 16\n",
    "NOISE_AUGMENTATION = 128\n",
    "PHASE_1_DURATION = 2000000\n",
    "RATIOS = [4, 4, 4, 2]\n",
    "SAMPLING_RATE = 44100\n",
    "\n",
    "# Parameters for core.AudioDistanceV1:\n",
    "# ==============================================================================\n",
    "core.AudioDistanceV1.log_epsilon = 1e-07\n",
    "core.AudioDistanceV1.multiscale_stft = @core.MultiScaleSTFT\n",
    "\n",
    "# Parameters for model.BetaWarmupCallback:\n",
    "# ==============================================================================\n",
    "model.BetaWarmupCallback.initial_value = 100\n",
    "model.BetaWarmupCallback.target_value = 100\n",
    "model.BetaWarmupCallback.warmup_len = 1\n",
    "\n",
    "# Parameters for pqmf.CachedPQMF:\n",
    "# ==============================================================================\n",
    "pqmf.CachedPQMF.attenuation = 100\n",
    "pqmf.CachedPQMF.n_band = %N_BAND\n",
    "\n",
    "# Parameters for discriminator.CombineDiscriminators:\n",
    "# ==============================================================================\n",
    "discriminator.CombineDiscriminators.discriminators = \\\n",
    "    [@discriminator.MultiPeriodDiscriminator,\n",
    "     @discriminator.MultiScaleDiscriminator]\n",
    "\n",
    "# Parameters for cc.Conv1d:\n",
    "# ==============================================================================\n",
    "cc.Conv1d.bias = False\n",
    "\n",
    "# Parameters for scales/torch.nn.Conv1d:\n",
    "# ==============================================================================\n",
    "scales/torch.nn.Conv1d.bias = True\n",
    "scales/torch.nn.Conv1d.device = None\n",
    "scales/torch.nn.Conv1d.dilation = 1\n",
    "scales/torch.nn.Conv1d.dtype = None\n",
    "scales/torch.nn.Conv1d.groups = 1\n",
    "scales/torch.nn.Conv1d.padding = 0\n",
    "scales/torch.nn.Conv1d.padding_mode = 'zeros'\n",
    "scales/torch.nn.Conv1d.stride = 1\n",
    "\n",
    "# Parameters for periods/nn.Conv2d:\n",
    "# ==============================================================================\n",
    "periods/nn.Conv2d.bias = True\n",
    "periods/nn.Conv2d.device = None\n",
    "periods/nn.Conv2d.dilation = 1\n",
    "periods/nn.Conv2d.dtype = None\n",
    "periods/nn.Conv2d.groups = 1\n",
    "periods/nn.Conv2d.padding = 0\n",
    "periods/nn.Conv2d.padding_mode = 'zeros'\n",
    "periods/nn.Conv2d.stride = 1\n",
    "\n",
    "# Parameters for periods/discriminator.ConvNet:\n",
    "# ==============================================================================\n",
    "periods/discriminator.ConvNet.capacity = %CAPACITY\n",
    "periods/discriminator.ConvNet.conv = @nn.Conv2d\n",
    "periods/discriminator.ConvNet.in_size = 1\n",
    "periods/discriminator.ConvNet.kernel_size = (5, 1)\n",
    "periods/discriminator.ConvNet.n_layers = 4\n",
    "periods/discriminator.ConvNet.out_size = 1\n",
    "periods/discriminator.ConvNet.stride = 4\n",
    "\n",
    "# Parameters for scales/discriminator.ConvNet:\n",
    "# ==============================================================================\n",
    "scales/discriminator.ConvNet.capacity = %CAPACITY\n",
    "scales/discriminator.ConvNet.conv = @torch.nn.Conv1d\n",
    "scales/discriminator.ConvNet.in_size = 1\n",
    "scales/discriminator.ConvNet.kernel_size = 15\n",
    "scales/discriminator.ConvNet.n_layers = 4\n",
    "scales/discriminator.ConvNet.out_size = 1\n",
    "scales/discriminator.ConvNet.stride = 4\n",
    "\n",
    "# Parameters for cc.ConvTranspose1d:\n",
    "# ==============================================================================\n",
    "cc.ConvTranspose1d.bias = False\n",
    "\n",
    "# Parameters for blocks.EncoderV2:\n",
    "# ==============================================================================\n",
    "blocks.EncoderV2.adain = None\n",
    "blocks.EncoderV2.capacity = %CAPACITY\n",
    "blocks.EncoderV2.data_size = %N_BAND\n",
    "blocks.EncoderV2.dilations = %DILATIONS\n",
    "blocks.EncoderV2.keep_dim = False\n",
    "blocks.EncoderV2.kernel_size = %KERNEL_SIZE\n",
    "blocks.EncoderV2.latent_size = %LATENT_SIZE\n",
    "blocks.EncoderV2.n_out = 1\n",
    "blocks.EncoderV2.ratios = %RATIOS\n",
    "blocks.EncoderV2.recurrent_layer = None\n",
    "blocks.EncoderV2.spectrogram = None\n",
    "\n",
    "# Parameters for blocks.GeneratorV2:\n",
    "# ==============================================================================\n",
    "blocks.GeneratorV2.adain = None\n",
    "blocks.GeneratorV2.amplitude_modulation = True\n",
    "blocks.GeneratorV2.capacity = %CAPACITY\n",
    "blocks.GeneratorV2.data_size = %N_BAND\n",
    "blocks.GeneratorV2.dilations = %DILATIONS\n",
    "blocks.GeneratorV2.keep_dim = False\n",
    "blocks.GeneratorV2.kernel_size = %KERNEL_SIZE\n",
    "blocks.GeneratorV2.latent_size = @core.get_augmented_latent_size()\n",
    "blocks.GeneratorV2.noise_module = None\n",
    "blocks.GeneratorV2.ratios = %RATIOS\n",
    "blocks.GeneratorV2.recurrent_layer = None\n",
    "\n",
    "# Parameters for core.get_augmented_latent_size:\n",
    "# ==============================================================================\n",
    "core.get_augmented_latent_size.latent_size = %LATENT_SIZE\n",
    "core.get_augmented_latent_size.noise_augmentation = %NOISE_AUGMENTATION\n",
    "\n",
    "# Parameters for convs.get_padding:\n",
    "# ==============================================================================\n",
    "convs.get_padding.dilation = 1\n",
    "convs.get_padding.mode = 'centered'\n",
    "convs.get_padding.stride = 1\n",
    "\n",
    "# Parameters for periods/convs.get_padding:\n",
    "# ==============================================================================\n",
    "periods/convs.get_padding.dilation = 1\n",
    "\n",
    "# Parameters for scales/convs.get_padding:\n",
    "# ==============================================================================\n",
    "scales/convs.get_padding.dilation = 1\n",
    "\n",
    "# Parameters for discriminator.MultiPeriodDiscriminator:\n",
    "# ==============================================================================\n",
    "discriminator.MultiPeriodDiscriminator.convnet = @periods/discriminator.ConvNet\n",
    "discriminator.MultiPeriodDiscriminator.periods = [2, 3, 5, 7, 11]\n",
    "\n",
    "# Parameters for discriminator.MultiScaleDiscriminator:\n",
    "# ==============================================================================\n",
    "discriminator.MultiScaleDiscriminator.convnet = @scales/discriminator.ConvNet\n",
    "discriminator.MultiScaleDiscriminator.n_discriminators = 3\n",
    "\n",
    "# Parameters for core.MultiScaleSTFT:\n",
    "# ==============================================================================\n",
    "core.MultiScaleSTFT.magnitude = True\n",
    "core.MultiScaleSTFT.normalized = False\n",
    "core.MultiScaleSTFT.num_mels = None\n",
    "core.MultiScaleSTFT.sample_rate = %SAMPLING_RATE\n",
    "core.MultiScaleSTFT.scales = [2048, 1024, 512, 256, 128]\n",
    "\n",
    "# Parameters for blocks.normalization:\n",
    "# ==============================================================================\n",
    "blocks.normalization.mode = 'weight_norm'\n",
    "\n",
    "# Parameters for periods/blocks.normalization:\n",
    "# ==============================================================================\n",
    "periods/blocks.normalization.mode = 'weight_norm'\n",
    "\n",
    "# Parameters for scales/blocks.normalization:\n",
    "# ==============================================================================\n",
    "scales/blocks.normalization.mode = 'weight_norm'\n",
    "\n",
    "# Parameters for model.RAVE:\n",
    "# ==============================================================================\n",
    "model.RAVE.audio_distance = @core.AudioDistanceV1\n",
    "model.RAVE.decoder = @blocks.GeneratorV2\n",
    "model.RAVE.discriminator = @discriminator.CombineDiscriminators\n",
    "model.RAVE.enable_pqmf_decode = True\n",
    "model.RAVE.enable_pqmf_encode = True\n",
    "model.RAVE.encoder = @blocks.WasserteinEncoder\n",
    "model.RAVE.feature_matching_fun = @feature_matching/core.mean_difference\n",
    "model.RAVE.gan_loss = @core.hinge_gan\n",
    "model.RAVE.latent_size = %LATENT_SIZE\n",
    "model.RAVE.multiband_audio_distance = @core.AudioDistanceV1\n",
    "model.RAVE.num_skipped_features = 1\n",
    "model.RAVE.phase_1_duration = %PHASE_1_DURATION\n",
    "model.RAVE.pqmf = @pqmf.CachedPQMF\n",
    "model.RAVE.sampling_rate = %SAMPLING_RATE\n",
    "model.RAVE.update_discriminator_every = 4\n",
    "model.RAVE.valid_signal_crop = True\n",
    "model.RAVE.warmup_quantize = None\n",
    "model.RAVE.weights = \\\n",
    "    {'adversarial': 2,\n",
    "     'fullband_spectral_distance': 2,\n",
    "     'multiband_spectral_distance': 2}\n",
    "\n",
    "# Parameters for dataset.split_dataset:\n",
    "# ==============================================================================\n",
    "dataset.split_dataset.max_residual = 1000\n",
    "\n",
    "# Parameters for blocks.WasserteinEncoder:\n",
    "# ==============================================================================\n",
    "blocks.WasserteinEncoder.encoder_cls = @blocks.EncoderV2\n",
    "blocks.WasserteinEncoder.noise_augmentation = %NOISE_AUGMENTATION\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
